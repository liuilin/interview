[TOC]

阳哥、JavaGuide、中华石杉、马士兵、左神、廖雪峰、阮一峰

> 临阵磨枪：线程池、AQS、分布式事务 Seata 原理、Spring IOC、Spring AOP、Bean 流程、Spring 循环依赖、Redis 原理、反射，jvm 调优（线上调优参数），NIO、IO 多路复用（Redis）

思想层面：如果让你来设计一个 Spring 框架你会怎么做？如果让你来设计一个 Dubbo 框架你会怎么做？如果让你来设计一个 MyBatis 框架你会怎么做？如果让你来设计一个消息中间件你会怎么做？

> `/**` 的注释好像会导致本文档错乱，不要在 `> 引用` 中使用`1. 或 - `，目前暂时记录一下
>
> 学东西要先抓脉络，不要先扣细节。体会知识成体系的感觉
>
> 一个开发技巧提示：在需要局部变量（辅助变量）时候再创建，参考 HashMap 的 `add()`
>
> 学知识点，学到自己够用就行了，不需要死磕到底。毕竟你不是 JDK 的设计者，里面太过于复杂，抓取对自己有用的就行了
>
> 书籍：design pattern

[Java 教程 - 廖雪峰的官方网站](https://www.liaoxuefeng.com/wiki/1252599548343744)

[尚硅谷 Java 大厂面试题第二季 (Java 面试必学，周阳主讲)_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV18b411M7xz)

[尚硅谷 2021 逆袭版 Java 面试题第三季（Java 大厂面试题，周阳主讲）_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1Hy4y1B78T?p=70&spm_id_from=pageDriver)

[【中华石杉】互联网 Java 工程师面试突击（第一季）_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1gE411M7cs/?vd_source=4844de7cb051be29fbaf4555af0bbd8b)

【中华石杉 | ES | 面试突击】百度网盘

> CU底缺原
>
> 无多并内原
>
> mm同高性
>
> 原使等加绑

### Java基础

| 运算符号 | 运算描述 | 运算规则                                                     |
| -------- | -------- | ------------------------------------------------------------ |
| &        | 与       | 两位都为 1 时，结果才为 1                                    |
| \|       | 或       | 两位只要有一个为 1 时，结果就为 1                            |
| ^        | 异或     | 两位不同时，结果才为 1                                       |
| ~        | 取反     | 0 变 1，1 变 0                                               |
| <<       | 左移     | 各二进位全部左移若干位，高位丢弃，低位补 0                   |
| >>       | 右移     | 各二进位全部右移若干位，对无符号数，高位补 0，有符号数，各编译器处理方法不一样，有的补符号位（算术右移），有的补 0（逻辑右移） |

1. **面向对象的特征**

   封装、继承、多态

   封装：就是把属性和方法封装到一个类中，调用方不需要知道里面具体的实现细节只管调用方法就行了，同时增加了安全性

   继承：子类可以使用父类的所有非 private 的功能，并且对这些功能进行扩展

   多态：父类引用指向子类对象（接口的多种不同实现方式）。它的前提是要子类继承父类并重写对应方法

   > 彩色打印机和黑白打印机都继承于打印机，并重写了 `print()` 方法，如果 new 出来的实例是彩色打印机打印出来就是彩色的，黑白打印机打印出来就是黑白的
   >
   > <img src="https://www.runoob.com/wp-content/uploads/2013/12/java-polymorphism-111.png" alt="java-polymorphism-111.png (701×561)" style="zoom: 50%;" />
   >
   > 面向对象 vs 面向过程
   >
   > 面向对象： 狗.吃(肉)
   >
   > 面向过程： 吃.(狗,肉)

2. **重载和重写的区别**

   重载：发生在同一个类中，方法名必须相同，而参数类型、个数、顺序都不同，还有就是方法返回值及访问修饰符也可以不同

   > 比如在分布式系统里面用 Redis 进行加锁，调用 `setAbsent()` 方法，而想要加过期时间的话正常我们可以再调用一个让 key 过期的方法。但如果加锁和添加过期时间分两行写的话，就会有线程安全问题。所以用带过期参数的重载方法让代码放同一行执行来保证原子性，从而让线程安全。

   重写：发生在子父类中，方法名、参数列表必须相同，如果父类方法访问修饰符为 private 则子类就不能重写该方法

3. **接口和抽象类有什么区别？**

   接口不能够被实例化，它对行为的抽象。利用接口可以达到 API 定义与实现分离的目的；不能包含任何非常量成员，所以里面的属性都隐含着 public static final 的含义；而方法呢要么是抽象方法，要么是静态方法

   > 如 JDK 库中的 List 接口。API 分离 - 字段 - 方法

   抽象类是不能实例化的类，抽象类大多用于抽取共用成员变量或方法，然后通过继承的方式达到代码复用的目的

   > 在标准库中，比如集合框架中，很多通用部分就被抽取成为抽象类，例如 Java.util.AbstractList

   **总结：**

   - 抽象类也是类，也就说明它只能单继承。但接口可以多实现
   - 如果实现了一个接口，那么必须要实现接口的所有抽象方法；而抽象类中可以有抽象方法，也可以有普通方法从而避免必须在子类中重复实现，更加灵活

   > Java 类实现 interface 使用 implements 关键词，继承 abstract class 则是使用 extends 关键词，我们可以参考 Java 标准库中的 ArrayList.
   >
   > ```java
   > public class ArrayList<E> extends AbstractList<E>
   >   implements List<E>， RandomAccess， Cloneable， Java.io.Serializable
   > {}
   > ```
   >
   > ...Servlet extends HttpServlet extends GenericServlet implement Servlet
   >
   > 实现接口就需要复写所有方法；而继承接口就只需要复写 abstract 方法

4. **String 为什么是不可变的？String 和 StringBuilder、StringBuffer 的区别是什么？**

   String 类中是使用 final 关键字的字符数组来保存字符串的（`private final char value []`） ，所以 String 对象是不可变的，每次对 String 类改变的时候相当于生成了一个新的 String 对象然后将指针指向它，所以内容经常改变的字符串最好不要用 String，因为每次生成对象都会对系统性能产生影响，特别是当内存中无引用对象多了以后，JVM 的 GC 就会开始工作，那时速度就会很慢

   StringBuilder 和 StringBuffer 类就不一样了，每次结果都是对对象本身进行操作，而不是生成新的对象。然而 StringBuffer 是对方法加了锁是线程安全的。而 StringBuilder 是非线程安全的

   > 加锁了效率会受到一定影响

   总结：

   1. 操作少量数据时用 String
   2. 单线程下操作大量数据用 StringBuilder
   3. 多线程下操作大量数据用 StringBuffer 

5. **一个 ArrayList 在循环过程中删除，会不会出问题，为什么**

   [【原理探究】ArrayList遍历时删除元素的正确姿势是什么？ - 掘金](https://juejin.cn/post/6844904038442467336)

   有问题。例如 [1 2 2 3]。解决方案：1. 倒序 for 循环 2. 用 Iterator 遍历和 Iterator 的 `remove()`

   - 如果是用 for 循环正序遍历的话会删不干净。首先找到第一个元素 2 之后，后面的数组元素会往前移，i 等于 2 时此时该位置上的元素已经是 3 了，所以这种连续重复的值会删不干净

   - 如果使用的是增强 for 循环，调用 `remove()` 方法之后再遍历就会出现 ConcurrentModificationException。是因为增强 for 底层实现用的是 Iterator 的 `hasNext()` 和 `next()` 方法，在 `next()` 里面会去判断 expectedModCount 与 modCount 值是否一致，如果不一致就抛错

   - 那么要保证不抛异常的话的关键点在于保证 expectedModCount 与 modCount 一致，解决方案就是调用 Iterator 的 `remove()` 方法，里面在调用 ArrayList 的 `remove()` 方法进行 modCount++ 之后赋值给了 expectedModCount，因此两个值是一致的就不会抛错了

     > 注意 Iterator 循环遍历里面用 ArrayList 的 `remove()` 也会抛异常，因为 modCount++ 了

6. **Java 容器有哪些？哪些是普通容器？哪些是并发容器？（Java 集合之间的区别 | 介绍一下集合容器）**

   ![Java 集合容器](http://rjnbyrj6g.hn-bkt.clouddn.com/interview/Java%20%E9%9B%86%E5%90%88%E5%AE%B9%E5%99%A8.jpg)

   Java 容器主要分两大类，一类是 Collection，一类是 Map，它们有共同父接口 Iterator

   - Collection
     - List 与 Set

       List 特点：数据对象有序且可以重复，可以加入多个 null 值

       Set 特点：数据对象无序且不可以重复，只能放一个 null 值

     - ArrayList 和 LinkedList

       **ArrayList 原理**

       ArrayList 是个数组，可以加入多个 null 值，特点是查改快（根据数组下标直接获取值）、增删慢（因为要移动元素）

       > 数组删除慢，删了之后要把后面的数据往前移动。

       **扩容原理源码：**
       
       i. ArrayList 中维护了一个 Object 类型的数组 elementData，`transient Object[] elementData`
       
       ii. 创建对象时是使用的一个无参构造，初始容量 elementData 为 0（JDK 7 为 10）
       
       iii. 当添加元素时会判断是否需要扩容，如果 minCapacity 需要的最小容量数组值大于当前数组大小就会调用 grow(minCapacity) 方法来扩容，并把旧数组数据拷贝过去。否则就直接正常添加元素就行了
       
       ```java
       private void ensureExplicitCapacity(int minCapacity) {
           modCount++;
           // overflow-conscious code
           if (minCapacity - elementData.length > 0)
               grow(minCapacity); // 执行扩容
       }
       
       private void grow(int minCapacity) {
           // overflow-conscious code
           int oldCapacity = elementData.length;
           // 右移一位相当于除以 2，这里 oldCapacity + (oldCapacity >> 1) 相当于是 1.5 倍
           int newCapacity = oldCapacity + (oldCapacity >> 1);
           if (newCapacity - minCapacity < 0)
               newCapacity = minCapacity;
           if (newCapacity - MAX_ARRAY_SIZE > 0)
               newCapacity = hugeCapacity(minCapacity);
           // minCapacity is usually close to size, so this is a win:
           elementData = Arrays.copyOf(elementData, newCapacity);
       }
       ```
       
       iv. 如果调用的是无参构造，那么第一次添加元素会判断 elementData 为空数组对象，就会把数组扩容为 10。当数组满了容量超过 10 需要再次扩容就为旧数组容量加旧数组无符号右移 1 位（其实就相当于旧 elementData 的 1.5 倍）；
       
       而使用的是指定 capacity 容量的构造器方法，那么 elementData 容量就为 capacity 的值，扩容的时候大小为传入值的 1.5 倍
       
       **LinkedList 原理**
       
       LinkedList 本质上是个双向链表，因为它实现了 Deque 双向队列接口，而 Deque 又实现了 Queue 队列接口，它的特点是查改慢（要进行依次遍历）、增删快（因为只用改变指针指向）
       
       ![链表](http://rjnbyrj6g.hn-bkt.clouddn.com/interview/%E9%93%BE%E8%A1%A8.jpg)
       
       **源码解析：**
       
       i. LinkedList 底层维护了一个双向链表，里面有两个属性值 first 和 last 分别指向了首节点 Node 对象和尾结点 Node 对象
       
       ii. 每个节点（Node 对象）又维护了 prev、next、item 这三个属性，通过 prev 指向前一个节点，next 指向后一个节点，最终就实现了双向链表。所以 LinkedList 的添加和删除元素效率特别高，只用移动指针并且不需要进行数组扩容
       
       **总结：**所以要是查改多就用 ArrayList，增删多就用 LinkedList，一般来说项目里面 80-90% 都是查询，大部分情况下是使用 ArrayList。但是 ArrayList 是线程不安全的，可以使用 `Vector` 或 `Collections.synchronizedList(new ArrayList<>());` 或者 `CopyOnWriteArrayList` 来保证线程安全
       
     - ArrayList 和 Vector
     
       Vector 加了锁是线程安全的，数据一致性保证了，但是性能急剧下级。因为它在每个方法里面都加了重锁(synchronized)
     
       ArrayList 不是同步的，所以在不需要保证线程安全时建议使用 ArrayList，多线程情况下呢可以使用  `Collections.synchronizedList(new ArrayList<>());` 或者 `CopyOnWriteArrayList` 来保证线程安全
     
     - 我们知道 ArrayList 是线程不安全的，请编写一个不安全的案例并给出解决方案（方案：CopyOnWriteArrayList）
     
       ```java
       // 1. 故障现象
       // java.util.ConcurrentModificationException
       // 2. 导致原因
       // 并发争抢修改导致的，有点像签名的情况。一个人正在写入时，另外一个人过来抢夺导致数据不一致异常。报出并发修改异常。
       // 3. 解决方案：同步容器
       // 3.1 new Vector<>（）；
       // 3.2 Collections. synchronizedList（new ArrayList<>（））；
       // 3.3 new CopyOnWriteArrayList（）；
       // 4. 优化建议（同样的错误不犯第2次）
       public static void main(String[] args) {
           // List<String> list = new ArrayList<>();
           // List<String> list = new Vector<>();
           // List<String> list = Collections.synchronizedList(new ArrayList<>());
           List<String> list = new CopyOnWriteArrayList<>();
           for (int i = 1; i <= 30; i++) {
               new Thread(() -> {
                   list.add(UUID.randomUUID().toString().substring(0, 8));
                   System.out.println(list);
               }, String.valueOf(i)).start();
           }
       }
       ```
     
       ![CopyOnWrite](http://rjnbyrj6g.hn-bkt.clouddn.com/interview/CopyOnWrite.jpg)
     
       ArrayList 线程不安全，可以使用 CopyOnWriteArrayList 替代
     
       比如我在加班名单上签自己名字时，突然有人上来抢了我的笔把我写的名字搞乱了。那我希望我写操作时能够加锁来保证原子性，一次只有一个人操作；但同时我又希望大家读的时候不加锁来提高并发性，让所有人都能同时看到这份名单，而不是一个人一个人的看
     
       **原理：**CopyOnWrite 利用的是读写分离的思想。它往一个容器添加元素的时（add 写操作），是先将当前容器 Object[] 进行拷贝，复制出一个新的容器 Object[] newElements 里，然后往新的容器里添加元素，添加完元素之后，再将原容器的引用指向新的容器（ setArray 步骤）；这样做的好处是并发读旧数组时不加锁保证性能；而写的时候调用 `add()` 加入重入锁保证线程安全
     
       源码：
     
       ```java
       public boolean add(E e) {
           final ReentrantLock lock = this.lock;
           lock.lock();
           try {
               Object[] elements = getArray();
               int len = elements.length;
               Object[] newElements = Arrays.copyOf(elements， len + 1);
               newElements[len] = e;
               setArray(newElements);
               return true;
           } finally {
               lock.unlock();
           }
       }
       ```
     
     - HashSet 实现了 Set 接口
     
       HashSet 底层空参构造器就是 HashMap（原理、扩容原理参考下面的 HashMap 部分）
     
       ```java
       public HashSet() {
           map = new HashMap<>();
       }
       ```
     
       > hash 值计算：调用 `hash()` ，计算 (h = key.hashCode()) ^ (h >>> 16) 
       >
       > 索引值计算：tab[i = (n - 1) & hash]，hash 为上面计算的 hash 值
       
     - LinkedHashSet（TODO：后期补一个“韩顺平” 老师的图）
     
       LinkedHashSet 是 HashSet 的子类，底层是一个 LinkedHashMap，底层维护了一个数组 + 双向链表
     
       LinkedHashSet 根据元素的 hashCode 值来决定元素的存储位置，同时使用链表维护元素的次序，这使得元素看起来是以插入顺序保存的。
     
       LinkedHashSet 不允许添重复元素
     
       源码解读：
     
       在 LinkedHastSet 中维护了一个 hash 表和双向链表（ LinkedHashSet 有 head 和 tail）
     
       每一个节点有 before 和 after 属性，这样可以形成双向链表
     
       在添加一个元素时，先求 hash 值，在求索引，确定该元素 table 的位置，然后将添加的元素加入到双向链表（如果已存在，不添加，原理和 HashSet 一致）
     
       ```java
       tail.next= newElement; //示意代码
       newElement.pre=tail;
       tail newEelment;
       ```
     
       这样的话遍历 LinkedHashSet 也能确保插入顺序和遍历顺序一致
     
   - Map
   
     > HashMap 部分见后面
   
     Map 用于保存 key、value 键值对，它们可以是任何引用类型的数据，会封装到 Node 对象中
   
     Map 的 key 不允许重复，并且只能有一个 null 值；value 可以重复并可以有多个 null 值
   
     key 和 value 之间存在单向一对一关系，即通过指定的 key 总能找到对应的 value，时间复杂度是 O(1) 的

7. **HashTable、HashMap、和 TreeMap 有什么不同？**

   Hashtable 的 key、value 都不能为 null。它是线程安全的，但读和写都加了锁导致效率比较低，所以很少被使用

   HashMap 的 key、value 都可以为 null。它没有加锁是线程不安全的，所以单线程情况下用它效率很高

     > HashMap 进行 put 或者 get 操作，可以达到常数时间的性能（就像电脑里的快捷访问文件夹，它作为 Key，点击之后立马跳转到对应的 Value 目录，就不用再一层层去翻了。）

8. **HashMap 底层数据结构？实现原理？源码？**

   [Java 8系列之重新认识HashMap - 美团技术团队](https://tech.meituan.com/2016/06/24/java-hashmap.html)

   [【源码解析】用 Java 手写（Spring、Tomcat、Hashmap、Mybatis、SpringBoot...）框架源码，挑战年薪 70W_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1S64y1o7S6?p=40)

   [【韩顺平讲 Java】Java 集合专题 -ArrayList HashMap HashSet List Map TreeMap TreeSet 等_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1YA411T76k?p=28&spm_id_from=pageDriver)

   [【咕泡学院】HashMap1.8 源码及线程非安全分析_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1ZJ411G7Ss?vd_source=4844de7cb051be29fbaf4555af0bbd8b)

   [【小傅哥】HashMap 原理 - 知乎](https://www.zhihu.com/question/422840340/answer/1494603694)

   - 1.8 做了什么优化？

     1.7 是数组 + 链表

     1.8 是数组 + 链表（单向链表，只有 next 指针） + 红黑树

     数组的特点是查改快、增删慢；而链表是增删快、查改慢。所以 HaspMap 就是综合了它两的各自的优点巧妙设计

     ![HashMap data structure jdk8](http://rjnbyrj6g.hn-bkt.clouddn.com/interview/HashMap%20data%20structure%20jdk8.jpg)

     ```java
     // Node 对象节点数据：数组可以放基本数据类型，也可以放引用数据类型。所以数组里至少应该放置的是个对象，key value 肯定得有，还需要 hash 通过 key 计算一个哈希值并存储。下面要是有相同 hash 的数据时，需要用链表来连接起来，那就需要一个 next 指针
     static class Node<K,V> implements Map.Entry<K,V> {
       final int hash;
         final K key;
       V value;
         Node<K,V> next;
     }
     ```
     
     > key 不能重复，重复了就会把节点插入尾部。但值是可以重复，同时允许 null 键和 null 值
     > HashMap 如果添加相同的 key，那么 value 会覆盖原来的值
     > 里面的键值对是无序的，因为是用的 hash 算法

     **HashMap 底层原理**
     
     - HashMap 底层维护了 Node 类型的数组 table，默认都是为 null
     
     - 当我们调用空参构造方法来创建对象时，会初始化 table 容量为 16，阈值（threshold）初始化为 16*0.75（加载因子，经过大量工业验证的一个固定值）=12，后面再扩容，则 table 扩容为原来的两倍（容量 <<1），阈值（threshold）变为旧阈值的两倍（oldThr << 1）
     
     - Put 过程：当添加一个元素时，会先得到 `hashCode()` 值然后转为索引值。然后找到存储数据表 table（Node[]） 具体要存储的索引位置，看这个位置是否已经存放的有元素。如果没有就直接放入；如果有就调用 `equals()` 比较，如果相同就把 value 值覆盖，如果不相同则添加到末尾节点
     
     - 树化过程：如果一条链表的元素个数 > TREEIFY THRESHOLD 树化的阈值 8（默认是 8），并且 table 数组的大小（不包含链表） >=MIN TREEIFY CAPACITY 最小树化容量 64（默认 64），就会进行树化成红黑树。否则仍然会采用数组扩容机制
     
       > HashMap 数组扩容机制：==如果从第一次开始一直往同一个链表加元素 > 8 个，加第 9 个元素时数组就会立马扩容为 32，再加第 10 个元素会扩容为 64，再加一个元素就会把 > 8 个元素的链表转为红黑树==
     
     ```java
     /**
      * The default initial capacity - MUST be a power of two. 必须是 2 的 n 次
      */
     static final int DEFAULT_INITIAL_CAPACITY = 1 << 4; // aka 16
     /**
      * 加载因子，类似于一般水桶要满的时候就需要换桶。要满了就是指超过加载因子了
      * The load factor used when none specified in constructor.
      */
     static final float DEFAULT_LOAD_FACTOR = 0.75f; // 这是经过大量统计与计算得出来的一个值
     /**
      * The bin count threshold for using a tree rather than list for a
      * bin.  Bins are converted to trees when adding an element to a
      * bin with at least this many nodes. The value must be greater
      * than 2 and should be at least 8 to mesh with assumptions in
      * tree removal about conversion back to plain bins upon
      * shrinkage.
      */
     static final int TREEIFY_THRESHOLD = 8; // 树化，链表转红黑树的阈值
      /**
      * The bin count threshold for untreeifying a (split) bin during a
        * resize operation. Should be less than TREEIFY_THRESHOLD, and at
      * most 6 to mesh with shrinkage detection under removal.
      */
     static final int UNTREEIFY_THRESHOLD = 6; // 反树化，红黑树转链表的阈值
      /**
       * The smallest table capacity for which bins may be treeified.
     * (Otherwise the table is resized if too many nodes in a bin.)
       * Should be at least 4 * TREEIFY_THRESHOLD to avoid conflicts
     * between resizing and treeification thresholds.
       */
     static final int MIN_TREEIFY_CAPACITY = 64; // 树化的其中一个前提
     ```

     > 直接用位运算性能更好，省去了 10 进制转 2 进制的过程
     
     **HashMap put 过程**
     
     ```java
     final V putVal(int hash, K key, V value, boolean onlyIfAbsent,
                    boolean evict) {
      Node<K,V>[] tab; Node<K,V> p; int n, i; // 辅助变量
         // 如果 table 数组为空或者 length = 0; 就扩容到 16
      if ((tab = table) == null || (n = tab.length) == 0)
           n = (tab = resize()).length;
      // 插入根据 hash 计算出数组下标，判断数组下标上的第一个 Node 节点是否为空，空则新建 Node 加入该位置
       if ((p = tab[i = (n - 1) & hash]) == null)
          tab[i] = newNode(hash, key, value, null);
         // 否则执行下面把计算出来 hash 值相同的 Node 节点插入尾部
      else {
             Node<K,V> e; K k;
          // 如果 table 索引位置的 key 对应的 hash 值相同，并且 table 索引位置上的节点的 key 和准备添加的节点的 key 是同一个对象 || equals 也相等，就会走下面的替换操作
             如果传入 Node 节点与当前链表中的某一个 Node 节点，他们的 key 对应的 hash 值和 equals 都相等，那么旧的节点对应的 value 就会被新的 value 覆盖
          if (p.hash == hash &&
                 ((k = p.key) == key || (key != null && key.equals(k))))
            e = p;
             // 红黑树情况
          else if (p instanceof TreeNode)
                 e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
        else {
                 // 找到的节点后面链表，会把想插入的元素去和链表里的元素挨个进行比较
               for (int binCount = 0; ; ++binCount) {
                     // 如果 key 都不同就把节点插入到末尾
                 if ((e = p.next) == null) {
                         p.next = newNode(hash, key, value, null);
                         // 如果当前链表个数已经到达 8 个就调用 treeifyBin 方法。
                      // 但是 treeifyBin 方法里面还有个条件就是 table 数组大小大于等于 64 时
                         if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st
                          treeifyBin(tab, hash);
                         break;
                     }
                   // hash 值相同，value 值也相同，就直接退出死循环
                     if (e.hash == hash &&
                       ((k = e.key) == key || (key != null && key.equals(k))))
                         break;
                     p = e;
                 }
          }
             // key 相同，新 value 会替换旧 value
             if (e != null) { // existing mapping for key
                 V oldValue = e.value;
              if (!onlyIfAbsent || oldValue == null)
                     e.value = value;
              afterNodeAccess(e);
                 return oldValue;
          }
         }
         ++modCount;
         // 数组里所有链表里每增加一个 Node 就 size++，如果 size 大于了临界值就去扩容
         if (++size > threshold)
             resize();
         afterNodeInsertion(evict);
         return null;
     }
     ```

     **链表用的好好的，为什么要从链表转为红黑树呢？**

     因为链表搜索的时候时间复杂度是 O(N)，每次整个链表遍历一遍，而红黑树是个二叉树，它的时间复杂度是 O(logN) 的，它就可以使用这种二分的查找，这样效率就快一半

     **HaspMap 是怎样扩容的？**

     ```java
     if ((tab = table) == null || (n = tab.length) == 0)
         n = (tab = resize()).length; // n 是初始化默认容量 16
     // ...
     ++modCount;
     if (++size > threshold)
         resize();
     ```

     第一次添加元素时 table 数组大小会初始化为 16。阈值初始化为（threshold） =  16 * 0.75（加载因子LoadFactor）=12。如果 table 的数组元素大于了阈值 12 ，数组就会扩容到 16 * 2=32，此时新的阈值就是 32 * 0.75=24，依次类推

     > 不是等于 12，是大于 12 添加第 13 个元素时才会扩容
     >
     > 如果链表的元素到达 8 个并且数组大小是 > 64 的，就会调用 `resize()` 进行数组扩容，为什么要到达加载因子这个阈值就开始扩容：因为作者设计的很巧妙，他怕容量快用完的时候又大量的进来数据卡在这里，所以提前扩容以备无患
     >
     > `afterNodeInsertion(evict)` 默认没做任何处理，是为了给 LinkedHashMap 做双向链表去扩展实现的
     >
     > `equals()` 需要特别注意，如果是 String 类型，那边比较的是值，是因为 String 里面重写了 `equals()` 。但并不是所有的都是这样，比如 Person 对象，我们可以定义它的年龄+身高相同就相等，也可以年龄+薪水相同就相等。如果不改动不重写的话比较的还是对象地址是否相等
     >
     > ++size 放的是 map 中 K,V 键值对的个数；就是我们每次加入的一个节点 Node(k,v,h,next)，不管是数组上的还是链表上的节点都算。只要 size > threshold 就会进行 resize() 扩容

9. **Hash 冲突**

   > 源码：
   >
   > hash 值计算：调用 `hash()` ，计算 (h = key.hashCode()) ^ (h >>> 16)
   >
   > 索引值计算：tab[i = (n - 1) & hash]，hash 为上面计算的 hash 值
   >
   > 为什么会有 hash 冲突算法 `(n-1)&hash` 呢：我们想要让它均匀的落在每个节点上，避免链表过长，就能同时避免提前扩容，扩容耗费性能（我个人猜想）

   **扰动函数**

   [JDK 源码中 HashMap 的 hash 方法原理是什么？ - 知乎](https://www.zhihu.com/question/20733617)

   ```java
   //Java 8中的散列值优化函数
   static final int hash(Object key) {
       int h;
       return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16); // key.hashCode()为哈希算法，返回初始哈希值
   }
   ```

   hash 值计算 `(h = key.hashCode()) ^ (h >>> 16)`：首先是根据 key 取一个 `hashCode()` 值，然后把它自己与 `hashCOde()` 值右移 16 位后取得的高 16 位进行异或 ^ 运算（相异才得 1）。取异或的原因是想要异或运算时 0、1 均衡，让所有位都参与运算，避免出现全是 0 的情况，全都是 0 会导致全都落在同一条链上。所以这算法能让值均匀的落在数组上，不然会浪费很多空间并提前扩容（扩容比较消耗性能，因为要拷贝旧数组到新数组）

   ```java
   // 固定容量为 16 大小的 HashMap，取不同的 hash 值模拟计算
   List<Integer> hashList = Arrays.asList(65536, 196608, 458752, 983040);
   for (Integer hash : hashList) {
       // 异或运算后结果为 1 3 7 15，比较均匀的分布；不进行异或运算，结果在同一条链上 0 0 0 0
       // hash = hash ^ (hash >>> 16);
       System.out.println((16 - 1) & hash);
   }
   ```

   ![HashMap 容量为 2 的 n 次幂](http://rjnbyrj6g.hn-bkt.clouddn.com/interview/HashMap%20%E5%AE%B9%E9%87%8F%E4%B8%BA%202%20%E7%9A%84%20n%20%E6%AC%A1%E5%B9%82.jpg)

   假如低 16 位全是 0，不进行扰动函数就与 (n-1=15) 进行 & 操作，那么全会冲突并落在 0 位置；而相反高十六位与低十六位进行异或操作的话，有一定几率产生 1，再 & 操作时就能落在不同位置的节点上

   **数组长度 n 为什么必须是 2 的 n 次幂（相当于无符号左移一位）？**

   数组下标索引具体算法 `tab[i = (n - 1) & hash]`：因为 2 的 n 次幂是一个偶数，偶数减 1 是一个奇数。奇数二进制最低位一定是一个 1，这时和 hash 值进行一个与 & 运算时，算法计算 key 落在数组的索引值时是奇数位还是偶数位，取决于的数组长度二进制值==所对应的 hash 值二进制的那一位==是 0 还是 1

   但如果数组长度是一个奇数，减 1 得到一个偶数后，偶数的最低位一定是 0，无论你的 hash 值二进制最低位是几，与运算之后只是偶数，那么意味了索引数组的奇数位就用不了浪费了

   ```java
   & 运算：都为 1 才为 1，否则为 0
   010101000001111110 hashCode()
   			 01111 在老数组16中key落在节点上的索引
   			011111 在新数组32中key落在节点上的索引
   ```

   > 为什么要用 & 运算代替 % 运算？
   >
   > 28745%16=(也可以得到 0~15 之间的值)
   >
   > `& 与运算（位运算）` 是要比 `% 取模运算` 效率高很多，测试有 16 倍。达到的目的相同，都是把结果控制在 0-15 之间

10. **HashMap 是线程安全的吗？不安全会导致哪些问题？如何解决？（HashMap 在高并发下如果没有处理线程安全会有怎样的安全隐患，具体表现是什么？）**

   [HashMap? ConcurrentHashMap? 相信看完这篇没人能难住你！ - 知乎](https://zhuanlan.zhihu.com/p/50675786)

   [HashMap 与 ConcurrentHashMap 的区别_XF 的专栏 - CSDN 博客_hashmap 和 concurrenthashmap 的区别](https://blog.csdn.net/xuefeng0707/article/details/40834595)

   HashMap 概述及可能导致的问题

   ```java
   1). hash(key) 计算
   2). 数组初始化
   3). 检查数组位置是否为 null，如果为 null 则插入
   4). 不为 null，分三种情况：替换、红黑树、链表
   5). 扩容 new Table[double]，并移动数据
   ```

   1) 安全：每个线程来计算出值都是一样的

   2) 不安全：每个线程想初始化容量大小可能不同，会出现线程安全问题。ConcurrentHashMap 是在 `initTable()` 中，把成员变量 `sizeCtl` 用 volatile 修饰来保证内存的可见性，同时用 `U.compareAndSwapInt(this, SIZECTL, sc, -1)` CAS 自旋来更改值，性能也大大提高

   3) 不安全：进行数组判空检查时，会出现线程安全问题。ConcurrentHashMap 还是使用 `U.getObjectVolatile()` 先获取内存中的最新值，然后在进行判断。判断通过后用 CAS 来替换插入值

   4) 不安全：还是在进行数组判空检查时，会出现线程安全问题。而且里面有分了复杂的替换、红黑树、链表三种情况，所以 ConcurrentHashMap 采用了分段锁方案，用 synchronized(f) 代码块来锁住当前数组下标那条链，在不影响其它数组链路情况下，把锁的粒度降低了很多

   5) 不安全：t1 线程进行扩容时，其它线程也可能在进行扩容，线程不安全。而且扩容方法 `addCount(1L, binCount);` 还不能简单的直接用 synchronized 加到方法处，因为这会非常影响性能，t1 线程在扩容时，别的线程就没办法 put 元素了，而且还还必须等扩容完成后插入新的数组才行，不然插入旧数组也没有意义。==所以必须要满足线程进行扩容时，其他线程都不要进行插入数据==

9. **有没有线程安全的并发容器？为什么要 ConcurrentHashMap？ConcurrentHashMap 是如何实现的？**

   > ConcurrentHashMap 性能比 HashTable 好得多，HashTable 所有方法都加了 synchronized 重锁，性能大打折扣。而前者是细化到哪里需要哪里才进行加锁

   i. 可以使用 `Collections.synchronizedMap()`

   ii. 使用 ConcurrentHashMap 实现的是分段锁技术，本质上是一个 segments 数组，把 map 分成了 n 个 segment，put 和 get 的时候，都是现根据 `key.hashCode()` 算出放到哪个 segment 中，每个 segment 通过继承 ReentrantLock 来进行分段加锁，然后给每一段数据分配一把锁，当一个线程占用锁访问其中一个段的数据时，其他段的数据也能被其他线程访问。 ConcurrentHashMap 不仅保证了多线程运行环境下的数据访问安全性，而且性能上有长足的提升。

   而它的 get() 方法非常的高效，因为整个过程不需要加锁。

   iii. 采用分段锁实现，但是同一段的写和读也是互斥的，所以性能稍微低。所以可以采用读写分离的 CopyOnWriteMap 实现，写操作加锁保证数据一致性，读操作不加锁共享读保证并发性
   
    > 当数据一致性至关重要时，我们应该选择 Collections.synchronizedMap()，对于写入操作远多于读取操作的性能关键型应用程序，我们应该选择 ConcurrentHashMap。 这是因为 Collections.synchronizedMap() 要求每个线程都为读/写操作获取整个对象的锁相比之下，ConcurrentHashMap 允许线程在集合的不同段上获取锁
    >
    > 人很聪明，真的很聪明。既然不能全锁（HashTable）又不能不锁（HashMap），所以就搞个部分锁，只锁部分，用到哪部分就锁哪部分。一个大仓库，里面有若干个隔间，每个隔间都有锁，同时只允许一个人进隔间存取东西。但是，在存取东西之前，需要有一个全局索引，告诉你要操作的资源在哪个隔间里，然后当你看到隔间空闲时，就可以进去存取，如果隔间正在占用，那你就得等着。聪明
   
11. **TransferValue 值传递问题？**

    ```java
    public class TestTransferValue {
        public static void main(String[] args) {
            TestTransferValue test = new TestTransferValue();
            int age = 20;
            test.changeValue1(age);
            System.out.println("age = " + age); // 打印的是 main 方法数据
    
            Person person = new Person("lin");
            test.changeValue2(person);
            System.out.println("person.getName() = " + person.getName()); // 打印的是 main 方法数据
    
            String str = "Daniel";
            test.changeValue3(str);
            System.out.println("str = " + str); // 打印的是 main 方法数据
        }
        // ================================ print out ================================
        // age = 20
        // person.getName() = liu
        // str = Daniel
    
        private void changeValue3(String str) {
            str = "xxx";
        }
    
        private void changeValue2(Person person) {
            person.setName("xxx");
        }
    
        private void changeValue1(int age) {
            age = 30;
        }
    
    }
    
    @Getter
    @Setter
    @AllArgsConstructor
    class Person {
    
        private String name;
    
    }
    ```

    ![值传递问题](http://rjnbyrj6g.hn-bkt.clouddn.com/interview/%E5%80%BC%E4%BC%A0%E9%80%92%E9%97%AE%E9%A2%98.jpg)

    **传值和传引用的区别，Java是怎么样的，有没有传值引用？**

    这个直接画对象的内存分布图就好了

    值传递：传递的是真实内容的一个副本，对副本的操作不影响原内容，也就是形参怎么变化，不会影响实参对应的内容

    引用传递：传递变量的引用地址，若地址指向的变量改变，指向同一内存空间的变量同步改变

11. **Object 类中的方法**

    - clone

    - finalize()：虚拟机的垃圾回收，每个对象都可以调用这个方法来判断对象是否可以被销毁

    - hashCode()、equals()：==**你重写过 hashcode 和 equals 么，为什么重写 equals 时必须重写 hashCode 方法？**==

      ```java
      for (int i = 0; i < 10; i++) {
          // System.out.println(Integer.valueOf(1).hashCode()); // 10 次都相同
          // System.out.println("1".hashCode()); // 10 次都相同
          System.out.println(new Test().hashCode()); // 10 次都不同
      }
      ```

      原因是 Integer、String 都重写了 `hashCode()` 方法，所以每次得到的值都相同；而自己写的类没有重写 `hashCode()` 方法，所以调用的是父类 Object 的 `public native int hashCode();` 方法，以至于每次 hashCode 值不同

      > 假如在 HashMap 里，我们想让所有 Node 节点都挂在同一条链表上，由于放在哪个位置是根据 `hashCode()` 去计算的，所以我们可以在对象里面重写 `hashCode()` 改为固定值

      hashCode() 根据一个 key 获取一个 hash 值，HashMap 用到的就是 `(h = key.hashCode()) ^ (h >>> 16)` 来计算数组下标；

      而 equal() 是判断对象是否相等，也就是比较两个对象的内存地址。同时我们可以重写 equals() 方法来自定义比较对象某些属性值都相同则两个对象相等。比如我们判断两个 Person 对象是否相等时，可以重写 equals 方法，自定义 name 和 age 都相等时才是同一个对象。

      像 HashSet 里面的 key 是不能重复的，就是因为底层调用的是 HashMap 的 hashCode() 与 equals() 判断，如果 hash 值和 equals 都相等的话也就不会产生重复数据。因为新创建两个对象的话肯定不是同一个对象，如果想自定义让 HashSet 不添加重复的，就要把对应的属性（比如 name 和 age 属性，自定义这两个字段都相同就不重复添加对象）重写 hashCode 和 equals 方法

      ```java
      // HashMap
      if (p.hash == hash &&
          ((k = p.key) == key || (key != null && key.equals(k))))
          e = p;
      ```

      > 而 String 比较的是两个值是否相等，因为 String 类重写了 equals()，同时重写了 hashCode() 方法

      ```java
      // 自定义重写 equals 和 hashCode 方法，判断两个对象是否相同的时候用
      class User {
          private String name;
          private Integer age;
          @Override
          public boolean equals(Object o) {
              if (this == o) return true;
              if (o == null || getClass() != o.getClass()) return false;
              User user = (User) o;
              return name.equals(user.name) && age.equals(user.age);
          }
          @Override
          public int hashCode() {
              return Objects.hash(name, age);
          }
      }
      ```

    - getClass()：获取当前类的全类名，`.getClassLoader()` 可以获取类加载器等等。还可以在反射的时候使用

    - toString()：重写 toString() 可以打印对象里面的信息

    - wait()、notify()、notifyAll()：多线程相关，操作线程的等待和唤醒

13. **<span id='newObject'>一个对象占用多少个字节？</span>对象的创建过程？**

    使用 OpenJDK 的 JOL（Java Object Layout） 工具来查看对象的布局，Object 对象默认是 8 个字节

    > klass point 默认（压缩）是 4 个字节，不压缩的话是 8 个字节

    ```java
    public class HelloJOL {
        public static void main(String[] args) {
            Object o = new Object();
            System.out.println(ClassLayout.parseInstance(o).toPrintable());
        }
        // ================================ print out ================================
        // java.lang.Object object internals:
        //  OFFSET  SIZE   TYPE DESCRIPTION                               VALUE
        //       0     4        (object header)                           01 00 00 00 (00000001 00000000 00000000 00000000) (1)
        //       4     4        (object header)                           00 00 00 00 (00000000 00000000 00000000 00000000) (0)
        //       8     4        (object header)                           e5 01 00 f8 (11100101 00000001 00000000 11111000) (-134217243)
        //      12     4        (loss due to the next object alignment)
        // Instance size: 16 bytes
        // Space losses: 0 bytes internal + 4 bytes external = 4 bytes total
    }
    ```

    前两个 `(object header)` 是 markword，第三个 `(object header)` 是 klass pointer，`e5 01 00 f8` 值是 `Object.class` 。第四行是 padding 补齐

    ![Java 对象头信息](http://rjnbyrj6g.hn-bkt.clouddn.com/interview/Java%20%E5%AF%B9%E8%B1%A1%E5%A4%B4%E4%BF%A1%E6%81%AF.jpg)

    一个对象包含哪些内容

    - mark word：作用是记录了锁状态信息、 GC 标记信息、hashCode 信息
    - klass pointer：指向当前对象
    - instance data：

14. **String::intern() 题目**

    ```java
    public static void main(String[] args) {
        String str1 = new StringBuilder("计算机").append("软件").toString();
        System.out.println(str1.intern() == str1);
        String str2 = new StringBuilder("ja").append("va").toString();
        System.out.println(str2.intern() == str2);
    }
    ```

    这段代码在 JDK 6 中运行，会得到两个 false， 而在 JDK 7 中运行，会得到一个 true 和一个 false。原因是在 JDK 6 中，intern () 方法会把首次遇到的字符串实例复制到永久代的字符串常量池中存储， 返回的也是永久代里面这个字符串实例的引用， 而由 StringBuilder 创建的字符串对象实例在 Java 堆上， 所以必然不可能是同一个引用， 结果将返回 false。

    而 JDK 7（以及部分其他虚拟机， 例如 JRockit） 的 intern () 方法实现就不需要再拷贝字符串的实例
    到永久代了， 既然字符串常量池已经移到 Java 堆中， 那只需要在常量池里记录一下首次出现的实例引
    用即可， 因此 intern () 返回的引用和由 StringBuilder 创建的那个字符串实例就是同一个。 而对 str2 比较返
    回 false， 这是因为 “java”[2] 这个字符串在执行 String-Builder.toString () 之前就已经出现过了， 字符串常量
    池中已经有它的引用， 不符合 intern () 方法要求 “首次遇到” 的原则， “计算机软件” 这个字符串则是首次
    出现的， 因此结果返回 true

###  数据结构和算法

[黑马程序员 - 2020版数据结构与算法_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1Uk4y197ob?p=124)

[LeetCode刷题特训营：带你10天从算法零基础到精通，左程云/马士兵详解各大厂高频算法面试题（搭上offer的直通车）_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV18i4y1K7JK?p=31)

> 写在前面：至少掌握三种排序冒泡排序、快排、堆排。常见算法两数求和

数据结构有哪些：数组、栈、队列、链表、树、堆、（图）、哈希表

1. **树**

   [30张图带你彻底理解红黑树 - 简书](https://www.jianshu.com/p/e136ec79235c)

   [数据结构与算法 (八)—— 二叉树（Binary Tree） | xeh 的学习笔记](https://xeh1430.github.io/text/dataStructure8)

   ![tree](http://rjnbyrj6g.hn-bkt.clouddn.com/interview/tree.png)

   - 二叉树：左叶子节点一定是小于根节点，而右叶子结点一定是大于根节点的

     [左神 - 二叉树](https://www.bilibili.com/video/BV18i4y1K7JK?p=31)

     二叉树遍历的值：1234567

     ```java
     public class Code01_BinaryTreeWithRecursive {
         public static void main(String[] args) {
             BinaryTreeNode node7 = new BinaryTreeNode(7, null, null);
             BinaryTreeNode node6 = new BinaryTreeNode(6, null, null);
             BinaryTreeNode node5 = new BinaryTreeNode(5, null, null);
             BinaryTreeNode node4 = new BinaryTreeNode(4, null, null);
             BinaryTreeNode node3 = new BinaryTreeNode(3, node6, node7);
             BinaryTreeNode node2 = new BinaryTreeNode(2, node4, node5);
             BinaryTreeNode node1 = new BinaryTreeNode(1, node2, node3);
     
             System.out.println("==================== 前序 ====================");
             pre(node1); System.out.println();
             System.out.println("==================== 中序 ====================");
             middle(node1); System.out.println();
             System.out.println("==================== 后序 ====================");
             post(node1);
         }
     	// 先序
         private static void pre(BinaryTreeNode head) {
             if (head == null) {
                 return;
             }
             System.out.print(head.value + "\t");
             pre(head.left);
             pre(head.right);
         }
     	// 中序
         private static void middle(BinaryTreeNode head) {
             if (head == null) {
                 return;
             }
             middle(head.left);
             System.out.print(head.value + "\t");
             middle(head.right);
         }
     	// 后序
         private static void post(BinaryTreeNode head) {
             if (head == null) {
                 return;
             }
             post(head.left);
             post(head.right);
             System.out.print(head.value + "\t");
         }
     }
     ```

     递归序（递归遍历的本质）：递归序结果：1244425552 - 13666377731（先从左树执行，然后再去右树执行，每一个节点都会到达三次）。二叉树会先去左树执行，如果返回 null，就记录一次接着去右树执行，如果又返回 null，就再记录一次然后返回

     > 先序、中序、后序都可以在递归序的基础上加工出来
     >
     > 统计递归序里第一次到达一个节点就打印就是先序
     >
     > 统计递归序里第二次到达一个节点就打印打印即中序
     >
     > 统计递归序里第三次到达一个节点就打印即后序

     - 先序（根 - 左 - 右）：1245367

       先序就是打印所有第一次出现的值

     - 中序（左 - 根 - 右）：4251637

     - 后序（左 - 右 - 根）：4526731

     非递归方式实现二叉树的先序、中序、后序遍历（自己设计栈来实现）：构建一个栈 i. 每弹栈一次就打印 ii. 如有右，压入右 iii. 如有左，压入左（栈是先进后出，所以就会先处理左边的树。但这是先序）

     非递归压栈式前序：

     ```java
     private static void pre(Node head) {
         System.out.print("pre-order: ");
         if (head != null) {
             Stack<Node> stack = new Stack<>();
             stack.add(head);
             while (!stack.isEmpty()) {
                 head = stack.pop();
                 System.out.print(head.value + " "); // 弹出就打印
                 if (head.right != null) {
                     stack.push(head.right); // 先压右
                 }
                 if (head.left != null) {
                     stack.push(head.left); // 再压左，这样永远先处理左边的树，这就是先序
                 }
             }
         }
     }
     ```

     非递归压栈式中序：i. 整条左边界节点先依次压入栈 ii. 左边界节点无法压栈之后弹栈并打印并到右树重复执行前面的所有操作（比如 4 节点左右都没值，会弹栈两次）

     非递归压栈式后序：把 `头左右` 变为 `头右左` ，然后倒过来就是后序（也就是 `左右头` ，实现方案就是再准备一个栈来收集 `头右左` 的东西，最后弹栈就是后序）

   - [红黑树](https://www.bilibili.com/video/BV1Uk4y197ob?p=124)（二叉查找树 | 平衡的二叉树）

     i. 红黑树是一颗平衡的二叉树：尽量保证左子树和右子树层数是差不多的

     > AVL 树：最高子树和最低子树高度差不能 1

     ii. 最高子树和最低子树高度差不能超过两倍

     iii. 搜索路径里面必须要保证有相同的黑色节点

     iv. 搜索路径里面不能存在两个连续的红色节点

   - B 树

     > B 树特点：
     >
     > i. 所有键值分布在整颗树中
     >
     > ii. 不管节点还有没有子节点，在中间查找到值就结束了。不再去它的子节点进行查找。性能接近二分查找
     >
     > iii. 每个节点最多拥有 m 个子树，根节点至少有 2 个子树
     >
     > v. 分支节点至少拥有 m/2 颗子树（除根节点和叶子节点外都是分支节点）
     >
     > vi. 所有叶子节点都在同一层、每个节点最多可以有 m-1 个 key，并且以升序排列

     B 树实例图说明：

     B-Tree 由一个个节点组成，每个节点占用一个磁盘块（相当于是 ”页“，每一页理解为 4k 大小或 4k 的整数倍），一个节点上有两个升序排序的关键字、三个指向子树根节点的指针和数据 data，指针存储的是子节点所在磁盘块的地址。两个关键词划分成的三个范围域对应三个指针指向的子树的数据的范围域。以根节点为例，关键字为 16 和 34，P1 指针指向的子树的数据范围为小于 16，P2 指针指向的子树的数据范围为16~34，P3 指针指向的子树的数据范围为大于 34

     查找关键字过程：

     i. 根据根节点找到磁盘块 1，读入内存。【磁盘 IO 操作第 1 次】

     ii. 比较关键字 28 在区间（16,34），找到磁盘块 1 的指针 p2.

     iii. 根据 p2 指针找到磁盘块 3，读入内存。【磁盘 IO 操作第 2 次】

     iv. 比较关键字 28 在区间（27,29），找到磁盘块 3 的指针 P2

     v. 根据 p2 指针找到磁盘块 8，读入内存。【磁盘 IO 操作第 3 次】

     vi. 在磁盘块 8 中的关键字列表中找到关键字 28

     缺点：

     i. 每个节点都有 key，同时也包含 data，而每个页存储空间是有限的，如果 data 比较大的话会导致每个节点存储的 key 数量就会变少

     ii. 当存储的数据量很大的时候会导致深度变大，同时查询时磁盘 IO 次数就会变多，进而影响查询性能，于是就有了 B+ tree（比如我每个 data 数据大小为 1.9k，那就只有 0.2k 留给指针和值使用，在单个磁盘块上就只能存很少的数据，只能存两条的话，按照树的方式那么节点就会加深，树越深就意味着 IO 次数会越多）

   - B+ 树

     ![B+tree](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/B+%20tree.jpg)

     B+ tree 是在 B-Tree 上做了优化，变化如下：

     i. B+ tree 每个节点可以包含更多的节点，这样做的目的是为了降低树的高度，还有就是将数据范围变为多个区间，区间越多，数据检索越快

     ii. 非叶子节点存储 key，叶子节点存储 key 和数据

     iii. 叶子节点指针相互连接（符合磁盘的预读特性），顺序查询性能更高

     > Innodb 每次预读读取的是数据是 16k（需要是 4k 的整数倍），如果一个指针和数值占 10 字节，那么 16k 的一个磁盘块可以存储 1600 条数据。这样的话三层的 B+ 树每次只需要 3 次 IO 就可以搞定百万级（或千万级，取决于存储数据的大小）数据

     非叶子节点的磁盘块上存储的是指针和数据值，一个磁盘块是 4k，如果指针和数据是分别占 1 字节，那存储的数据就多了（数据的范围变得更大）。这就可以让我们在每次进行 IO 操作的时候查找尽可能多的数据。那三层的 B+ tree 呢每次只需要 3 次 IO 就可以搞定百万级（或千万级的，取决于存储数据的大小）数据

     > 注意：在 B+ 树上有两个头指针，一个指向根节点，另一个指向关键字最小的叶子节点，而且所有叶子节点（即数据节点）之间是一种链式环结构。因此可以对 B+Tree 进行两种查找运算：一种是对于主键的范围查找和分页查找，另一种是从根节点开始，进行随机查找。

     - InnoDB 对应的 B+ tree

       ![InnoDB 存储数据结构](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/InnoDB%20%E5%AD%98%E5%82%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.jpg)

       i. InnoDB 是通过 B+ 树结构对主键创建索引，然后叶子节点中存储记录值，如果没有主键，那么会选择唯一键，如果唯一键也没有，那么会生成一个 6 位的 row id 来作为主键

       ii. 如果创建索引的键是其他字段，那么在叶子节点中存储的是该记录的主键，然后再通过主键索引找到对应的记录
     
       > 如果是其他字段创建索引的话（比如 name 字段），会走两次 B+ 树。会先在 name 字段维护的 B+ 树索引找到记录值对应的当前记录值的主键，再通过主键索引去找到对应的值（这也叫做**回表**）
     
     - MyISAM 对应的 B+ 树
     
       ![MyISAM 数据结构](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/MyISAM%20%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.jpg)
       
       MyISAM 对应的 B+ 树的叶子节点不再存储像 InnoDB 那样具体的数据，而是文件的地址值，因为 MyISAM 在系统文件夹中是分两个文件来存储的，然后通过地址值去对应的文件中查找记录值

2. **==冒泡排序==、选择排序、插入排序、归并排序、==快速排序、堆排序==（堆和排序准备模板，比如对数器模板）**

   [左神直通 BAT 算法笔记（基础篇）- 上](https://juejin.cn/post/6844903779289006094#heading-24)

   [左神直通 BAT 算法笔记（基础篇）- 下](https://juejin.cn/post/6844903779289022478)

   [可视化算法动图：VisuAlgo - visualising data structures and algorithms through animation](https://visualgo.net/en)

   想要绕开原本数据状况方式：1. 随机选一个数 2. hash

   [二分查找 - 力扣（LeetCode）](https://leetcode-cn.com/problems/binary-search/solution/er-fen-cha-zhao-by-leetcode/)：先取中间值，要找的值如果比中间值大就查找右边，否则查左边。再重复遍历。时间复杂度为 O(M) * O(logN) 比选择排序和插入排序要好。

   ```java
   public static int search(int[] nums, int target) {
       int pivot, left = 0, right = nums.length - 1;
       while (left <= right) {
           // 不要写成 pivot = (right + left) / 2
           // 防止 integer 溢出。Integer.MAX_VALUE 最大值为 2147483647
           pivot = left + (right - left) / 2;
           if (nums[pivot] == target) return pivot;
           if (target < nums[pivot]) right = pivot - 1;
           else left = pivot + 1;
       }
       return -1;
   }
   ```

   - 冒泡排序：时间复杂度 O(N[^2])， 额外空间复杂度 O(1)  ，工程组已经少见

     如果前一个比后一个大就交换，直到排好最后一位，重复操作。从 N...N-1...1，结果是个等差数列，类似于 aN [^2]+bN+1，不要低阶项 bN+1，不要高阶项系数 a，结果就是 O (N [^2])

     ```java
     public class BubbleSort {
         public static void main(String[] args) {
             int[] arr = {2, 5, 8, 4, 6, 1};
             bubbleSort(arr);
             System.out.print(Arrays.toString(arr));
         }
     
         private static void bubbleSort(int[] arr) {
             if (arr == null || arr.length < 2) {
                 return;
             }
             for (int i = 0; i < arr.length; i++) {
                 for (int j = i + 1; j < arr.length; j++) {
                     if (arr[i] > arr[j]) {
                         swap(arr, i, j);
                     }
                 }
             }
         }
     
         private static void swap(int[] arr, int i, int j) {
             int temp = arr[i];
             arr[i] = arr[j];
             arr[j] = temp;
         }
     }
     ```

   - 选择排序（时间复杂度O(N^2^)， 额外空间复杂度O(1)  ）

   - 插入排序（时间复杂度O(N^2^)， 额外空间复杂度O(1)  ）

     斗地主整理牌的过程，选定第一个位置为最小值，遍历后面的值比第一个位置小就交换。然后从第二个位置、第三个位置依次开始循环

   - 归并排序（时间复杂度O(N*log^N^)，额外空间复杂度O(1) )

     先左侧排序，然后右侧排序。准备一个辅助数组，然后用外排序的方式小的填，依次动到末尾。另外一部分把没动到末尾的部分copy进辅助数组，再整体的copy回原数组

     ```java
      public static void sortProcess(int[] arr, int L, int R) {
         if (L == R) {
             return;
         }
         int mid = L + ((R - L) >> 1);//L和R中点位置，防止溢出，除以2等于右移一位，等同于（L+R)/2
         sortProcess(arr, L, mid);//T(N/2)
         sortProcess(arr, mid + 1, R);//T(N/2)
         merge(arr, L, mid, R);//big O(N)
         //T(N)=2T(N/2)+O(N)，用mater公式【T(N)=2T(N/2)+O(N)】、log(b,a) = d -> 复杂度为O(N^d * logN)
         //求解时间复杂度为：O(N*logN)，额外空间为借用的数组为：O(N)
     }
     private static void merge(int[] arr, int L, int mid, int R) {
         int[] auxiliary = new int[R - L + 1];
         int i = 0;
         int p1 = L;
         int p2 = mid + 1;
         while (p1 <= mid && p2 <= R) {
             auxiliary[i++] = arr[p1] < arr[p2] ? arr[p1++] : arr[p2++];
         }
         //两个必有且只有一个越界
         while (p1 <= mid) {
             auxiliary[i++] = arr[p1++];
         }
         while (p2 <= R) {
             auxiliary[i++] = arr[p2++];
         }
         for (i = 0; i < auxiliary.length; i++) {
             arr[L + i] = auxiliary[i];//[L+i]表示可能在坐边P1位置，可能在右边P2（mid+1）位置
         }
     }
     ```

     > 引出来分治的思想非常重要
     >
     > 小和问题：
     >
     > ```java
     > 只改变了这两处内容
     > return sortProcess(arr, L, mid)+sortProcess(arr, mid+1, R)+merge(arr,L,mid,R);
     > 
     > while (p1 <= mid && p2 <= R) {
     > //ture:R-p2+1是递归后右边的个数 * 比p1小的值
     > res += arr[p1] < arr[p2] ? (R - p2 + 1) * arr[p1] : 0;
     > auxiliary[i++] = arr[p1] < arr[p2] ? arr[p1++] : arr[p2++];
     > }
     > ```
     >
     > 
     
   - ==随机快排：==【时间复杂度O(N*log^N^)，额外空间复杂度O(log^N^)就是数组需要2分多少次且每次都在中间位置，最差情况O(N)】

     [快排](https://www.bilibili.com/video/BV1qb411g7fK?from=search&seid=15785115392021315340)：长期期望的复杂度是O(N*log^N^)，如果是顺序的数组则每次只排好一位数，时间复杂度为O(N^2^)

     优势：代码简洁，可以说明常数项很低；各种算法时间复杂度一样时，开始拼常数项；归并排序输在需要准备数组和拷贝数组过程，而且会遍历两边，快排只用while一遍

     随机快排：数组中随机选一个数和末尾交换，然后进行快排。好处是变成了一个概率事件

     实现：选最后一个数x，根据Netherlands荷兰国旗问题，选定最后一位为num值，遍历数组把小于x的和小于区下一位交换，然后小于区扩大一位，大于x的放大于区前一位，然后大于区缩小一位。然后递归前面操作

     - 数组的第一位是一个基准数

     - 从右开始解锁一个比基准数小的数，如果找到了，停下来

     - 再从左开始解锁一个比基准数大的数，如果找到了，停下来，如果1，2两者停下来的位置，未相遇，将停下来的元素交换位置

     - 直到两者相遇后，称之为基准数归位，将基准数和停下来的元素交换位置，再按照之前的逻辑反复操作

       > i. 方法是3个参数
       >
       > ii. 解锁操作实际上是一个while
       >
       > iii. 条件：当前这个数 >= baseNum  j--
       >
       > iv. 左边也是一个解锁，实际上是一个while条件，当前这个数 <= baseNum  i++
     
     ```java
     public static void quickSort(int[] arr) {
         if (arr == null || arr.length < 2) {
             return;
         }
         quickSort(arr, 0, arr.length - 1);
     }
     private static void quickSort(int[] arr, int l, int r) {
         if (l < r) {
             //加上这行则为随机快排
             swap(arr, l + (int) (Math.random() * (r - l + 1)), r);
             int[] p = partition(arr, l, r);
             quickSort(arr, l, p[0] - 1);
             quickSort(arr, p[1] + 1, r);
         }
     }
     private static int[] partition(int[] arr, int l, int r) {
         int less = l - 1;
         int more = r + 1;
         int cur = l;
         while (cur < more) {
             if (arr[cur] < arr[r]) {
                 swap(arr, ++less, cur++);
             } else if (arr[cur] > arr[r]) {
                 swap(arr, cur, --more);
             } else {
                 cur++;
             }
         }
         return new int[]{less + 1, more - 1};
     }
     ```
     
     > 在工程上是不允许递归函数出现的，因为准备递归函数的代价比较高，函数的指针、函数的变量域、code中的哪一行。系统压栈会将和业务有关或无关的信息都记下来，准备函数它常数时间比较大。而且系统栈递归了多少层之后会报错 不安全，所以工程上一定是改为的非递归版本

   - ==堆排（堆，就算被火车撞了也不能忘）==

     [图解排序算法(三)之堆排序 - dreamcatcher-cx - 博客园](https://www.cnblogs.com/chengxiao/p/6129630.html)：两步，1. 构建大顶堆 2. 调整堆结构 + 交换堆顶元素与末尾元素

     堆分为大根堆（父节点比左右叶子节点大）和小根堆（父节点比左右叶子节点小）。左叶子节点：`2*i+1`、右叶子节点：`2*i+2`、非叶子节点（父节点）：`(i-1)/2`
     
     优点：形成了结构大小为 N，每次新进来一个数调整代价为 logN（非常逆天，想想 40 多亿的数），也就是二叉树的高度。（因为是满二叉树，所以高度和节点是 O (log[N) 的关系，15 个节点高度就是四层）
     
     建立大根堆 heapinsert：0 到 i-1 位置已经是大根堆，进加入的 i 调整代价为 logi-1，i+1 位置调整代价为 logi，则 N 个节点代价为 log1+...log^N-1^ = O (N)
     
     heapify 过程：
     
     将整数数组（ 7-6-3-5-4-1-2 ）按照堆排序的方式进行升序排列，请问在第一轮排序结束之后，数组的顺序是（）
     
     堆排序首先将堆顶元素与最后一个元素互换，然后对未排序的部分维护堆的性质，从堆顶元素开始互换，2 的左右分别是 6 和 3, 由于 6 比 3 大，因此 2 与 6 互换，然后 2 的左右分别是 5 和 4，由于 5 比 4 大，因此 2 与 5 互换形成最终的堆。顺序为 6532417
     
     ```java
     public static void main(String[] args) {
         int[] arr = {2, 5, 8, 4, 6, 1, 7};
         heapSort(arr);
         printArray(arr);
     }
     public static void heapSort(int[] arr) {
         if (arr == null || arr.length < 2) {
             return;
         }
         for (int i = 0; i < arr.length; i++) {
             // 建立大根堆
             heapInsert(arr, i);
         }
         int size = arr.length;
         swap(arr, 0, --size);
         while (size > 0) {
             heapify(arr, 0, size);
             swap(arr, 0, --size);
         }
     }
     // 某个数现在处在index位置，往上继续移动
     private static void heapInsert(int[] arr, int index) {
         // 判断当前索引位置（叶子节点）是否比父节点（非叶子节点）大，大的话就交换。重置 index 为父节点后继续与父节点比较知道堆顶
         while (arr[index] > arr[(index - 1) / 2]) {
             swap(arr, index, (index - 1) / 2);
             index = (index - 1) / 2;
         }
     }
     private static void heapify(int[] arr, int index, int size) {
         // 左叶子节点
         int left = index * 2 + 1;
       // 左叶子大于 --size（数组长度）就不再继续；left+1<size 是判断 --size 时右叶子越界与否
         while (left < size) {
             // 比较父节点（非叶子节点）与两个叶子节点谁大，大的和父节点比较并交换，交换后把 index 改为交换后父节点索引位置，然后重复比较操作
             int right = left + 1;
     		int largestIndex = right < size && arr[right] > arr[left] ? right : left;
     		largestIndex = arr[index] > arr[largestIndex] ? index : largestIndex;
             
             // 如果最大值是自己本身，那么跳出循环。（while 里一定要有边界退出条件，不然一直跑下去）
             if (largest == index) {
                 break;
             }
             swap(arr, largest, index);
             index = largest;
             left = index * 2 + 1;
         }
     }
     private static void swap(int[] arr, int i, int j) {
         int tmp = arr[i];
         arr[i] = arr[j];
         arr[j] = tmp;
     }
     ```
     
     > 样本量不估计常数，只估计规模
     >
     > 剖析递归行为和递归行为时间复杂度的估算
     > master公式的使用
     > T(N) = a*T(N/b) + O(N^d)
     >
     > i. log(b,a) > d -> 复杂度为O(N^log(b,a))
     >
     > ii. log(b,a) = d -> 复杂度为O(N^d * logN)
     >
     > iii. log(b,a) < d -> 复杂度为O(N^d)  

3. **给定一个字符串类型的数组 arr，求其中出现次数最多的前 n 个**

   方案用大根堆、小根堆都可以。但是小根堆更优，可以固定门槛容量

   arr [a,a,a,b,b,c,c,c,d,f,f,f,f,f,f]，遍历计数得到 a:3,b:2,c:3,d:1,f:6，建立门槛为 3 的小根堆，进来的值比小根堆最顶的值大的话，顶部值就弹出，这样来得到 TOP3
   
4. **综合排序算法**

   首先和判断数组中的数据类型，如果是基本数据类型则用快排，因为基础类型不用考虑稳定性（也就是像两个3，谁在前 谁在后），自己定义的类型如student用归并排序，一个班的同学先按分数排序，再按班级排序，此时相同班级的个体可能不一样，样本量极少（小于60的情况下）用插入排序

   > 综合排序在样本量很小的情况下，为什么会选择复杂度很高的排序。因为常数项很低
   >
   > 为什么基础类型用快排自己定义的类型用归并排序？因为基本类型不需要稳定性，而自己定义的类型需要具有稳定性算法

5. **一致性Hash算法，一致性Hash算法的应用**

6. **荷兰国旗问题**

7. **遍历一颗二叉树**

   递归遍历左右节点

   左节点：2n+1

   右节点：2n+2

### JVM

1. **JVM的内存结构（模型）**

   线程私有：每个线程独立包含程序计数器、栈、本地方法栈（线程私有意味着生命周期和线程一致）

   线程共享：堆、堆外内存（【方法区】永久代或元空间、CodeCache 代码缓存（也就是 JIT 编译产物））

   垃圾回收是和堆、方法区有关。其它的是线程私有，不存在垃圾回收

   [Stack Memory and Heap Space in Java | Baeldung](https://www.baeldung.com/Java-stack-heap)

   > JDK6 以后常量池移入堆中
   >
   > String 常量池位置：
   >
   > - jdk1.6: 永久代（方法区）
   > - jdk1.7: 堆内存
   > - jdk1.8: 堆里面的元空间

   ![Java 堆栈空间](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/Java%20%E5%A0%86%E6%A0%88%E7%A9%BA%E9%97%B4.jpg)

   <img src="https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/JVM%20%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B.jpg" alt="JVM 内存模型" style="zoom: 33%;" />

   一个进程包含着整个运行时数据区对应的是一个 JVM 实例，进程里面的每个线程拥有一套 `虚拟机栈`、`本地方法栈`、`程序计数器`，同时所有线程共享 `方法区` 和 `堆` 空间

   - 栈（虚拟机栈）：每一个方法就是一个栈帧

     - 栈是什么：是每个线程创建时对应创建的虚拟机栈，其内部保存一个个的栈帧（Stack Frame），也就是对应着一次次方法的调用

     - 生命周期：它的生命周期是跟随线程的，线程结束时栈内存也就释放了，所以对于栈来说不存在垃圾回收问题

     - 作用：存储方法的局部变量，包括 8 种基本数据类型和对象的应用地址 reference address（真正的对象在堆空间中），并参与方法的调用与返回（也就是入栈和出栈）

     - 栈运行原理：压栈和出栈遵循 FILO 先进后出

     - 开发中可能遇到异常 java.lang.StackOverflowError：出现这个异常，在项目中定位代码时，发现同事的代码里进行了递归调用方法，结果栈调用层级过多，导致线程栈满了，就会出现此异常。

       > 可以使用 VM 启动参数 -Xss 来设置线程的最大栈空间
       
       ```java
       // 默认不设置栈大小情况下 i = 9879
       // 设置栈大小：-Xss256k，i = 2304
       private static int i = 1;
       public static void main(String[] args) {
           System.out.println(i++);
           main(args);
       }
       ```

     **栈帧：**

     - 局部变量表

       定义为一个数字数组，主要用于存储方法参数和定义在方法体内的局部变量，局部变量包括各类包括 8 种基本数据类型和对象的应用地址 reference address，以及 returnAddress 类型。

       由于局部变量表是建立在线程的栈上，是线程的私有数据，因此不存在数据安全问题

       局部变量表的最大容量在编译期就确定下来了，用字节码看是保存在方法的 Code 属性的 Maximum local variables 中，在方法运行期间是不会改变局部变量表的大小的。

       局部变量表中的变量只在当前方法调用中有效，当方法调用结束后会随着栈帧的销毁而销毁。

       ```java
       public static void main(String[] args) {
           LocalVariablesTest test = new LocalVariablesTest();
           int num = 10;
           test.test1();
       }
       private void test1() { }
       ```

       > IDEA 插件 Jclasslib 和 javap -c 命令都能查看：
       >
       > Ljava：L 是指引用类型变量

       ![局部变量表字节码](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/%E5%B1%80%E9%83%A8%E5%8F%98%E9%87%8F%E8%A1%A8%E5%AD%97%E8%8A%82%E7%A0%81.jpg)

       **Slot 的理解**

       局部变量表的单位是一个个 Slot，JVM 会为局部变量中的每一个 Slot 都分配一个访问索引，通过索引即可访问到对应的局部变量值。

       如果当前帧是由 <font color='red'> 构造方法 </font> 或者 <font color='red'> 实例方法（非静态方法的普通方法）</font> 创建的，那么该对象引用 this 将会存放在 index 为 0 的 Slot 处，其余的参数按照参数表顺序继续排列。他们两个方法可以用是因为他们对应的局部变量表中是有变量声明的，而静态方法则会编译出错。

       > double、long 会占据两个 Slot，相当于字节码里面的两个 Index

       局部变量表中的变量也是重要的垃圾回收根节点，只要被局部变量表中直接或间接引用的对象都不会被回收。如果局部变量表的变量不存在了，那么指针也就不存在了，垃圾就会被回收

       ![JVM 栈的局部变量表 Slot 理解](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/JVM%20%E6%A0%88%E7%9A%84%E5%B1%80%E9%83%A8%E5%8F%98%E9%87%8F%E8%A1%A8%20Slot%20%E7%90%86%E8%A7%A3.jpg)

     - 操作数栈 OperandStack

       主要用于保存计算过程的中间结果，同时作为计算过程中变量临时的存储空间

       操作数栈的最大容量在编译期就确定下来了，用字节码看是保存在方法的 Code 属性的 Maximum stack size 中（相当于类里面有几个方法）

       如果被调用的方法带有返回值的话，其返回值将会被压入当前栈帧的操作数栈中，并更新 PC 寄存器中下一条需要执行的字节码指令。

       代码追踪（执行流程）：[JVM/1_内存与垃圾回收篇/5_虚拟机栈 · 陌溪/LearningNotes - 码云 - 开源中国](https://gitee.com/moxi159753/LearningNotes/tree/master/JVM/1_内存与垃圾回收篇/5_虚拟机栈#代码追踪)

     - 动态链接 Dynamic Linking（指向运行时常量池的方法引用）

       字节码中，所有的变量和方法引用都会作为符号引用保存在常量池，常量池运行以后就在方法区了（运行时加载进发放区的所以也叫运行时常量池）

       作用：将这些符号引用转换为调用方法的直接引用

       > 为什么需要动态链接？为什么需要指向运行时常量池的方法引用
       >
       > 因为这样就在不同的方法里，都可以共享的调用同一份常量或者方法引用。这样就只需要存储一份，节省了空间
       >
       > 为什么需要运行时常量池？
       >
       > 常量池提供了一些符号和常量，便于指令识别。在不同的方法里都有可能调用同一份数据（两个方法调用同一个 num -> int num = 1;），没必要各自拥有一份，只需要放入一份到运行时常量池大家去引用即可

       **方法的调用：**

       ![方法调用](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/%E6%96%B9%E6%B3%95%E8%B0%83%E7%94%A8.jpg)

       在 JVM 中，将符号引用转换为调用方法的直接引用与方法的绑定机制相关。methodB 调用 methodA 时，字节码体现就是符号引用 `#6` 会去对应的常量池找到直接引用 `CONSTANT_Methodref_info` ，找到对应的方法

       虚方法：编译期已经确定，运行期不可变的叫虚方法。有静态方法、私有方法、final 方法、实例构造器、父类方法

       非虚方法：除虚方法外都为非虚方法，比如多态，不确定运行哪个子类方法

       普通调用指令：

       - invokestatic：调用静态方法，解析阶段确定唯一方法版本
       - invokespecial：调用 `<init>` 方法、私有及父类方法，解析阶段确定唯一方法版本
       - invokevirtual：调用所有虚方法
       - invokeinterface：调用接口方法

       动态调用指令：

       - invokedynamic：动态解析出需要调用的方法，然后执行

       > 对应的打印如下字节码

       ![方法调用的虚方法与非虚方法字节码](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/%E6%96%B9%E6%B3%95%E8%B0%83%E7%94%A8%E7%9A%84%E8%99%9A%E6%96%B9%E6%B3%95%E4%B8%8E%E9%9D%9E%E8%99%9A%E6%96%B9%E6%B3%95%E5%AD%97%E8%8A%82%E7%A0%81.jpg)

       ```java
       public class Son extends Father {
       
           public Son() {
               super();
           }
       
           public Son(int age) {
               this();
           }
       
           // 不是重写的父类的静态方法，因为静态方法不能被重写
           public static void showStatic(String str) {
               System.out.println("son " + str);
           }
       
           private void showPrivate(String str) {
               System.out.println("son private " + str);
           }
       
           public void show() {
               // =========================== 非虚方法 ===========================
               showStatic("good"); // invokestatic
               super.showStatic("nice"); // invokestatic
               showPrivate("Kimochi"); // invokestatic
               super.showNormalMethod(); // invokestatic
               showFinal(); // 虽然显示的是 invokevirtual，但因为被 final 修饰，不能被子类重写，所以也是非虚方法
               // =========================== 虚方法 ===========================
               showNormalMethod(); // invokevirtual，子类会重写父类方法，编译期确定不下来
               info(); // invokevirtual
       
               MethodInterface in = null;
               in.methodA(); // invokeinterface 要想运行成功，需要子类实现方法，重写时又不知子类是谁。所以表现为虚方法
           }
       
           private void info() { }
       
           public static void main(String[] args) {
               Son son = new Son();
               son.show();
           }
       }
       
       class Father {
       
           public Father() {
               System.out.println("father 的空参构造");
           }
       
           public static void showStatic(String str) {
               System.out.println("father " + str);
           }
       
           public final void showFinal() {
               System.out.println("father show final");
           }
       
           public void showNormalMethod() {
               System.out.println("father normal method");
           }
       }
       
       interface MethodInterface {
           void methodA();
       }
       ```

       函数式接口创建对象为 invokedynamic，创建的对象是谁的确定不了，是根据等号右边的值来确定的，有点类似于 Python 这样的动态语言，info = 13，根据 13 来确定是 int 类型

       ```java
       Func func = s -> { // invokedynamic
           return true;
       }
       ```

       **虚方法表：**

       JVM 为了提高性能呢，它在类的方法区建立一个虚方法表 virtual method table（非虚方法不会出现在表中）来实现使用索引表来代替查找。这样在子类重写之后，就不用向上一层层判断有没有改方法，而是直接用自己的方法

       虚方法表会在类加载的 `链接阶段` 被创建并开始初始化，类的变量初始值准备完成之后，JVM 会把该类的方法表也初始化完毕。

       如果类中重写了方法，那么调用时会在自己重写了的虚方法表中查找，如果没有才到 Object 的虚方法表中查找

       <img src="https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/%E6%96%B9%E6%B3%95%E8%B0%83%E7%94%A8%E4%B9%8B%E8%99%9A%E6%96%B9%E6%B3%95%E8%A1%A8.jpg" alt="方法调用之虚方法表" style="zoom: 33%;" />

       ```java
       public class CockerSpaniel extends Dog implements Friendly{
           @Override
           protected void finalize() { }
           public void eat(){ }
           @Override
           public void sayHello() { }
           @Override
           public void sayGoodbye() { }
           public static void main(String[] args) {
               System.out.println(new CockerSpaniel().toString());
           }
       }
       class Dog {
           public void sayHello(){ }
       
           @Override
           public String toString() {
               return "Dog";
           }
       }
       interface Friendly {
           void sayHello();
           void sayGoodbye();
       }
       ```

       > 比如没有重写的指向 Object，重写了的指向自己，比如自定义对象 Son，它没重写 toString()，父对象 Father 也没有重写 toString()，那么调用 Son.toString() 时就会调用 Object 的 toString() 

     - 方法返回地址 Return Address

       存放调用该方法的 PC 寄存器的值。一个方法的结束，有两种方式：1. 正常执行完成 2. 出现未处理的异常，非正常退出，无论通过哪种方式退出，在方法退出后都返回到该方法被调用的位置

       i. 方法正常退出时，<font color='red'>调用者的程序计数器的值会作为返回地址，即调用该方法指令的下一条指令地址</font>

       ii. 方法异常退出时，不会给调用者返回值，返回地址要通过异常表来确定，栈帧不会保存这部分信息

   - ==堆（重点优化）：==年轻代、年老代

     ![JVM metaspace](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/JVM%20metaspace.png)

     年轻代是类的诞生、成长、消亡的区域，而它又分为 Eden 区和 Survivor 区，所有的类都是在 Eden 区被 new 出来的。Survivor 区呢又分为 Survivor 0 区和 Survivor 1 区。当 Eden 的空间用完时，程序又需要创建对象，JVM 的垃圾回收器就会触发 Minor GC 采用 [复制清除算法](#fuzhiqingchu) 对 Eden 区进行垃圾回收，将 Eden 区中不再被其他对象所引用的对象进行销毁。然后将 Eden 区中剩余的对象移动到 S0 区。若 S0 区也满了，再对该区进行复制清除，然后移动到 S1 区。就这样反复 15 次后还没回收掉再移动到年老代。若年老代也满了，那此时就会触发 FullGC 进行内存清理，这时使用到的是标记压缩算法。年老代执行了 Full GC 后发现依然无法进行对象的保存，就会产生 OOM 异常 "OutOfMemoryError"，通常 Full GC 所消耗的性能是 Minor GC 的十倍以上

     > GC 策略的话，会根据实际业务来测试
     >
     > - 服务器如果是单核的，一般会用 serial 的 GC 机制
     >
     > - 多线程的可以使用 parallel 的 GC 机制
     > - 对事务有要求且希望不要因为 Full GC 的卡顿导致服务器的卡顿或是延迟，希望更低的延迟的话，我们会采用 CMS 的策略（为什么延迟比较低：不是等堆满了才回收的，而是采用分段回收）
     >
     > 如果出现 Java.lang.OutOfMemoryError：Java heap space 异常，说明 Java 虚拟机的堆内存不够。
     >
     > 原因有二：
     >
     > - Java 虚拟机的堆内存设置不够，可以通过参数 - Xms、-Xmx 来调整。
     > - 代码中创建了大量对象，并且长时间不能被垃圾收集器收集（GC 链能够指向 GC ROOTS）
     >
     > 新 new 出来的对象在堆里面，堆又在内存里面

   - 方法区(Java 8 前叫 Permanent Gen 永久带)  几乎不会被回收，Java 8 开始称之为元数据，所属于堆

   - PC 寄存器：存储下一条要执行的指令地址

     - 有什么用？因为 CPU 需要不停的切换各个线程，切换回来的时候得知道接着从哪开始继续执行。

     - 为什么设置为线程私有？还是为了准确地记录各个线程正在执行的当前字节码指令地址，切换回来的时候得知道接着从哪开始继续执行。

   - 本地方法栈

     虚拟机栈用于管理 Java 方法的调用，而本地方法栈管理本地方法库、本地接口的调用，线程私有，内存可动态配置大小

   - 本地接口、本地方法库

     执行非 Java 代码，比如原子引用 AtomicInteger 底层就是 native 方法，用它的就是因为 C++ 底层是汇编，执行效率是最高的

2. [Memory Management in Java Interview Questions (+Answers) | Baeldung](https://www.baeldung.com/Java-memory-management-interview-questions)

3. **JVM内存模型**

   - 类加载器：负责加载class文件

   - 运行时内存分为两部分

     - 线程私有内存（栈内存）：本地方法栈，PC程序寄存器

     - 线程共享内存（堆内存）：静态变量、常量、类信息、运行时常量池、实例变量(new出来的对象)

       Method Area 方法区（class文件在方法区）：方法区是被所有线程共享，所有字段和方法字节码，以及一些特殊方法如构造函数，接口代码也在此定义。简单说，所有定义的方法的信息都保存在该区域，此区属于共享区间。 静态变量+常量+类信息+运行时常量池存在方法区中，实例变量存在堆内存中

       > 总结：栈管运行、堆管存储

4. **谈谈你对类加载器的理解**

   > Java 中的所有类，必须被装载到 JVM 中才能运行，这个装载工作是由 JVM 中的类装载器完成的，类装载器所做的工作实质是把类文件从硬盘读取到内存中

   类加载器的作用：

   - 类加载器子系统负责从文件系统或者网络中加载 class 文件， class 文件在文件开头有特定的文件标识（cofe babe）

   - ClassLoader 只负责把 .class 字节码文件加载到内存，至于它是否可以运行则由 Execution Engine 决定

     > 过年相亲，七大姑八大姨把女孩带过来了，成不成就靠我自己（Execution Engine）来决定

   - 加载的类信息存放于一块称为方法区的内存空间（1.8 后叫元空间）。除了类的信息外，方法区中还会存放运行时常量池信息，可能还包括字符串字面量和数字常量（这部分常量信息是 class 文件中常量池部分的内存映射）

     > 运行时常量池：常量池在堆中，运行时把它加载到内存里，就叫运行时常量池

   虚拟机自带的类加载器：

   - 启动类加载器 BootstrapClassLoader
     - 由 C/C++ 实现，嵌套在 JVM 内部，用来加载 $JAVA_HOME/jre/lib/rt.jar 等 Java 核心库，用来提供 JVM 自身需要的类
     - 它就是顶层父类加载器，它加载 `扩展类加载器` 和 `应用类加载器` ，由于是用 C++ 实现的，所以用 Java 代码打印它是个 null
     - 出于安全考虑，启动类加载器只加载包名为 java、javax、sun 等开头的类
   - 扩展类加载器 ExtensionClassLoader
     - 由 Java 语言编写，由 sun.misc.Launcher$ExtClassLoader 实现且继承于 ClassLoader，父类为启动类加载器
     - 加载 jre/lib/ext 扩展目录的类库，如果我们自己创建的 jar 放在此目录下，也会自动由它加载
   - 应用程序类加载器 ApplicationClassLoader
     - 也是 Java 语言编写，由 sun.misc.Launcher$AppClassLoader 实现且继承于 ClassLoader，父类为扩展类加载器
     - 负责加载环境变量 classpath 或系统属性 java.class.path 指定路径下的类库
     - 它是程序中默认的类加载器，一般我们自己创建的 Java 应用类都是由它来完成加载，通过 ClassLoader.getSystemClassLoader(); 来获取该类加载器

   ```java
   public static void main(String[] args) {
       Object obj = new Object();
       System.out.println(obj.getClass().getClassLoader());
       MyObject myObject = new MyObject();
       System.out.println(myObject.getClass().getClassLoader());
       System.out.println(myObject.getClass().getClassLoader().getParent());
       System.out.println(myObject.getClass().getClassLoader().getParent().getParent());
   }
   // null
   // sun.misc.Launcher$AppClassLoader@18b4aac2
   // sun.misc.Launcher$ExtClassLoader@74a14482
   // null
   ```

   **JVM类加载的过程（生命周期）| 类加载机制**

   三阶段五步骤

   加载 Loading - 链接 Linking（验证、准备、解析） - 初始化 Initialization

   加载、验证、准备、解析和初始化五个阶段

   - 加载 Loading：

     i. 通过一个类的全限定类名来获取此类的二进制流

     ii. 将字节流所代表的静态存储结构转化为方法区的运行时数据结构

     iii. 在堆中（也就是内存中）生成一个代表该类的 java.lang.Class 对象

     > 方法区是比较虚的概念，具体落地 1.8 以前叫永久代，1.8 以后叫元空间

   - 验证 Verify：目的在于确保 Class 文件的字节流中包含符合当前虚拟机要求，保证被加载类的一个正确性

     比如每个 .class 文件都会以 COFE BABE 开头

   - 准备 Prepare：

     - 为类变量分配内存并且设置该类变量的默认初始值

       > 比如 private static int a = 1; 这里 a 的准备阶段初始值就是 0

     - 但是不包含 final 修饰的 static 常量，因为 final 在编译时就会分配内存，声明的 5 就是 5，后期都不能被修改了

     - 这里不会被实例变量分配初始化，因为还没创建对象，还是在类的加载过程

       > 类变量会分配到方法区；而实例变量是随着对象一起分配到 Java 堆中

   - 解析 Resolve：符号引用替换为直接引用 

     - 符号引用：用 #1 2 3 来间接引用

       > Java 中，一个 Java 类将会编译成一个 class 文件。在编译时，Java 类并不知道所引用的类的实际地址，因此只能使用符号引用来代替。比如 org.simple.People 类引用 org.simple.Language 类，在编译时 People 类并不知道 Language 类的实际内存地址

     - 直接引用：指针直接指向实际的内存地址

   - 初始化 Initialization：

     ==在此步骤初始化时，类加载器会把静态代码块和静态变量的显示赋值都会放到一个叫 `<clinit>()` 的构造方法中执行==

     - 是执行<font color='red'>类构造器方法</font> `<clinit>()` 的过程，它不需要定义，是 javac 编译器自动收集类中的所有类变量的赋值和静态代码块的语句合并而来的

       > 注意：要 **变量赋值** 和 **静态代码块** 两者条件同时出现，字节码文件里才会出现 `<clinit>()` 
       >
       > ```java
       > private static int num = 1;
       > 
       > static {
       >     num = 2;
       > }
       > ```

       ![ClassLoader 初始化](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/ClassLoader%20%E5%88%9D%E5%A7%8B%E5%8C%96.jpg)

       > number 变量定义可以放在下面，但是链接阶段下的 prepare 阶段会先把 number = 0 --> inital: 20 --> 10

     -  `<clinit>()` （class init）：不同于<font color='red'>类的构造器</font> ，构造器在 JVM 视角下对应的是 `<init>()` 方法，任何一个类声明以后，内部都会存在一个类的无参构造，<font color='red'>所以任何一个类的字节码对应 Methods 中都会有 `<init>()` </font> 方法

     - 如果该类具有父类，JVM 会保证先执行父类的 `<clinit>()` ，也就是子类加载之前先加载父类

       ```java
       public static void main(String[] args) {
           System.out.println(Son.b);
       }
       static class Father {
           public static int A = 1;
           static {
               A = 2;
           }
       }
       static class Son extends Father {
           public static int b = A;
       }
       // 打印 2，子类加载前先加载父类
       ```

     - JVM 必须保证一个类的 `<clinit>()` 方法（只会加载一次，之后都是操作它的缓存）会多线程下被同步加锁来保证线程安全

       ```java
       public class DeadThreadTest {
       
           public static void main(String[] args) {
               new Thread(() -> {
                   System.out.println(Thread.currentThread().getName() + " 开始");
                   DeadThread deadThread = new DeadThread();
                   System.out.println(Thread.currentThread().getName() + " 结束");
               }, "T1").start();
       
               new Thread(() -> {
                   System.out.println(Thread.currentThread().getName() + " 开始");
                   DeadThread deadThread = new DeadThread();
                   System.out.println(Thread.currentThread().getName() + " 结束");
               }, "T2").start();
           }
       
       }
       
       class DeadThread {
       
           static {
               if (true) {
                   System.out.println(Thread.currentThread().getName() + " 正在初始化当前类...");
                   while (true) { // while 执行，一直循环相当于加锁
                   }
               }
           }
       }
       ```

5. **父类双亲委派机制**

   > 主要起一个保护作用，项目中 String 类肯定大家都在用，假如我把自定义的 String 类传给同事，那他打开整个项目一下就挂了，为了防止被攻击呢，就有了 `父类双亲委派机制`

   what：Java 虚拟机对 class 文件采用的是 <font color='red'> 按需加载 </font> 的方式，也就是说当需要使用该类时才会将它的 class 文件加载到内存生成 class 对象。而且加载某个类的 class 文件时，Java 虚拟机采用的是 <font color='red'> 双亲委派机制 </font>，即把请求交由父类处理，它是一种任务委派模式

   how 工作原理：

   - 如果一个类加载器收到了类加载请求，它并不会自己先去加载，而是把这个请求委托给父类加载器执行

   - 如果父类加载器还存在其父类加载器，就进一步向上委托，依次递归，请求最终将到达顶层启动类加载器

   - 如果父类加载器可以完成类加载任务，就成功返回，倘若父类加载器无法完成此加载任务，子加载器才会尝试自己去加载。

     >  比如我们自己写个 String 类，类加载器会一层层向上找，先找应用类加载器，没有接着找扩展类加载器，还没有到顶层 Bootstrap 启动类加载器去找，发现 rt.jar 下面有 String 类了，就会直接加载，而不加载我们自己写的 String 类

     <img src="https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/%E7%88%B6%E7%B1%BB%E5%8F%8C%E4%BA%B2%E5%A7%94%E6%B4%BE%E6%9C%BA%E5%88%B6.jpg" alt="父类双亲委派机制" style="zoom:33%;" />

   why 优势：

   - 避免类的重复加载

   - 保护程序安全，防止核心 API 被随意篡改，出于安全考虑，Bootstrap 启动类加载器只加载包名为 java、javax、sun 等开头的类，我们自己创建的 java.lang 包下的类运行会报错

     > 自定义类：java.lang.String；自定义类：java.lang.MyTest（报错：阻止创建 java.lang 开头的类）

6. **请你谈谈 JVM 垃圾回收机制（Java 中如何判断一个对象是否是一个垃圾 | JVM 垃圾回收时如何确定垃圾 | 你知道有哪些垃圾回收算法吗？）**

   [JVM 垃圾回收算法 - 尚硅谷 JVM 全套教程，百万播放，全网巅峰（宋红康详解 java 虚拟机）_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1PJ411n7xZ?p=138&spm_id_from=pageDriver)

   线程共享的部分堆、方法区需要垃圾回收

   <font color='red'>什么是垃圾？垃圾是指在程序运行中没有任何指针指向的对象</font>

   常见垃圾回收算法：

   一共两类：1. 标记阶段：引用计数算法、可达性分析算法 2. 清除阶段：标记清除算法、复制算法、标记压缩算法

   - 引用计数算法（Reference Counting）：每个对象保存一个整型的引用计数器属性。用于记录对象被引用的情况。对于一个对象 A，只要有任何一个对象引用了 A，则 A 的引用计数器就加 1；当引用失效时，引用计数器就减 1. 只要对象对象 A 的引用计数器值为 0，就代表可以进行回收。

     缺点：无法处理循环引用问题

     ```java
     public class RefCountGC {
         Object reference = null; // 属性变量
         // 这个成员属性的唯一作用就是占用一点内存
         private byte[] bigSize = new byte[5 * 1024 * 1024];
         public static void main(String[] args) {
             RefCountGC obj1 = new RefCountGC();
             RefCountGC obj2 = new RefCountGC();
             obj1.reference = obj2;
             obj2.reference = obj1;
             obj1 = null;
             obj2 = null;
             // 显示的执行垃圾收集行为，判断obj1 和 obj2是否被回收？
             System.gc();
         }
     }
     ```

     ```java
     // 执行 System.gc(); 垃圾回收前
     Heap
      PSYoungGen      total 151040K, used 18012K [0x0000000717780000, 0x0000000722000000, 0x00000007c0000000)
       eden space 129536K, 13% used [0x0000000717780000,0x0000000718917260,0x000000071f600000)
       from space 21504K, 0% used [0x0000000720b00000,0x0000000720b00000,0x0000000722000000)
       to   space 21504K, 0% used [0x000000071f600000,0x000000071f600000,0x0000000720b00000)
      ParOldGen       total 345600K, used 0K [0x00000005c6600000, 0x00000005db780000, 0x0000000717780000)
       object space 345600K, 0% used [0x00000005c6600000,0x00000005c6600000,0x00000005db780000)
      Metaspace       used 2656K, capacity 4486K, committed 4864K, reserved 1056768K
       class space    used 282K, capacity 386K, committed 512K, reserved 1048576K
     ```

     ```java
     // 执行 System.gc(); 垃圾回收后
     Heap
      PSYoungGen      total 151040K, used 1295K [0x0000000717780000, 0x0000000722000000, 0x00000007c0000000)
       eden space 129536K, 1% used [0x0000000717780000,0x00000007178c3ee8,0x000000071f600000)
     ```

   - 可达性分析算法

     <img src="https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/%E5%8F%AF%E8%BE%BE%E6%80%A7%E5%88%86%E6%9E%90%E7%AE%97%E6%B3%95.jpg" alt="可达性分析算法" style="zoom:33%;" />

     有效地解决在引用计数算法中循环引用的问题，防止内存泄漏的发生。按照引用计数算法来说右边的不是垃圾，而在可达性分析算法里面它是垃圾

     **JVM 如何判断一个对象是否该被 GC？ | 什么叫 GCRoots？**

     从根节点出发进行搜索遍历，能被根节点直接或间接的用引用链连接着的就是可达对象，否则为不可达对象，就可以标记为垃圾对象

     **哪些能能作为 GC Roots 对象（顶点）？**

     i. 虚拟机栈中引用的对象（也就是栈帧中的局部变量表）

     ii. 方法区中的 static 修饰的类静态属性引用的对象

     iii. 方法区中常量引用的对象

     iv. 本地方法栈中 JNI（java native interface）引用的对象，比如线程中的 start 方法，就是 native 方法

     ```java
     public class GCRootsDemo {
     
     //    private static final GCRootsDemo3 g3 = new GCRootsDemo3(); // 常量引用的对象
     //    private static GCRootsDemo2 g2 = new GCRootsDemo2();; // 类静态属性引用的对象
     
         public static void main(String[] args) {
             m1();
         }
     
         private static void m1() {
             GCRootsDemo g1 = new GCRootsDemo(); // 虚拟机栈中引用的对象（栈帧中的局部变量表）
         }
     }
     ```

     **Object 下的 finalize() 方法（对象的 finalization 机制）**

     Java 语言提供了对象终止（finalization）机制来允许开发人员提供对象被销毁之前的自定义处理逻辑。

     当垃圾回收器发现没有引用指向一个对象时会先调用这个对象的 finalize () 方法来先自我拯救一下。

     finalize () 方法允许在子类中被重写，用于在对象被回收时进行资源释放。通常在这个方法中进行一些资源释放和清理的工作，比如关闭文件、套接字和数据库连接等（有点像 servlet，在 destroy 销毁前可以自定义处理逻辑）。

     不推荐主动去调用 finalize () 方法，原因有三点：

     i. 调用 finalize () 方法会导致对象复活

     ii. finalize () 方法执行的时间是由 GC 线程决定的，所以如果不发生 GC 的话， finalize () 就永远不会执行

     iii. 如果自己重写的很烂的话（比如死循环）会严重影响 GC 的性能

     finalize () 方法导致的三种状态

     如果从所有的根节点都无法访问到某个对象，说明该对象己经不再使用了。按理说该对象需要被垃圾回收。但事实上呢它处于一种 “刀下留人” 的状况，所以它涉及了三种状态：1. 可触及 2. 可复活 3. 不可触及

     i. 就是从根节点开始通过引用链寻找是可以到达该对象的，此时对象会被判定为 `可触及` 状态

     ii. 对象的所有引用都被释放了，也就是引用链断了，本来该被垃圾收集器回收，但是对象觉得自己还能调用重写的 finalize () 方法复活一下，此时为 `可复活` 状态，如果此时该对象能和引用链的对象连接上就不会被回收

     iii. 调用完 finalize () 方法后对象没有复活会进入 `不可触及` 状态，就再也不可能复活了，因为 finalize () 方法只能被调用一次

     > 判断一个对象是否可被回收，会经历两次标记，具体过程：
     >
     > i. 对象 A 到 GC Roots 没有引用链，则会进行第一次标记
     >
     > ii. 判断对象是否有必要执行 finalize () 方法
     >
     > 如果对象 A 没有重写 finalize () 方法，或者 finalize () 方法已经调用过了，则对象 A 会被判定为不可触及的。
     >
     > 如果对象 A 重写 finalize () 方法，且还未执行过，那么 obj 会被插入到 F-Queue 队列中，由一个虚拟机自动创建的、低优先级的 Finalizer 线程触发其 finalize () 方法执行。
     >
     > 调用完重写的 finalize () 方法后，GC 会对 F-Queue 队列中的对象进行第二次标记，如果此时该对象能和引用链的对象连接上就不会被回收。否则就再也不可能复活了，因为 finalize () 方法只能被调用一次
     
     ```java
     public class CanReliveObj {
         // 类变量，属于GC Roots的一部分
         public static CanReliveObj canReliveObj;
         public static void main(String[] args) throws InterruptedException {
             canReliveObj = new CanReliveObj();
             canReliveObj = null;
             System.gc();
             System.out.println("-----------------第一次gc操作------------");
             // 因为Finalizer线程的优先级比较低，暂停2秒，以等待它
             Thread.sleep(2000);
             if (canReliveObj == null) {
                 System.out.println("obj is dead"); // 不重写 finalize () 第一次就是 dead
             } else {
                 System.out.println("obj is still alive"); //  重写了的话会触发一次 “刀下留人”，然后拯救自己一次
             }
             System.out.println("-----------------第二次gc操作------------");
             canReliveObj = null; // 第二次把它置为 null，由于 finalize() 方法只能调用一次，这一次就得死了
             System.gc();
             // 下面代码和上面代码是一样的，但是 canReliveObj却自救失败了
             Thread.sleep(2000);
             if (canReliveObj == null) {
                 System.out.println("obj is dead");
             } else {
                 System.out.println("obj is still alive");
             }
         }
         // =========================== print out ===========================
         // 调用当前类重写的finalize()方法
         // -----------------第一次gc操作------------
         // obj is still alive
         // -----------------第二次gc操作------------
         // obj is dead
         // 此方法只会调用一次
         @Override
         protected void finalize() throws Throwable {
             super.finalize();
             System.out.println("调用当前类重写的finalize()方法");
             canReliveObj = this; // 当前待回收的对象在 finalize () 方法中与引用链上的对象 obj 建立了联系
         }
     }
     ```
     
     **内存泄漏（&工具使用）：**当不想再使用一个对象时，这个对象还在被直接或间接的引用着，没办法被垃圾收集器回收掉，可以先使用 JVisualVM 工具捕获 heap dump 文件，然后用 Eclipse 出的 Memory Analyzer (MAT) 工具查看这个对象对应这条链路的 GC Roots 是谁
     
     使用 VM 命令 -Xms8m -Xmx8m -XX:+HeapDumpOnOutOfMemoryError 模拟堆内存溢出，使用 JProfiler 打开生成的堆文件 java_pid26000.hprof
     
     ```java
     Exception in thread "main" java.lang.OutOfMemoryError: Java heap space
     	at com.liuilin.JVM.GC.HeapOOM.<init>(HeapOOM.java:17)
     	at com.liuilin.JVM.GC.HeapOOM.main(HeapOOM.java:23)
     java.lang.OutOfMemoryError: Java heap space
     Dumping heap to java_pid26000.hprof ...
     Heap dump file created [4998650 bytes in 0.011 secs]
     ```
     
     ```java
     public class HeapOOM {
     
         byte[] buffer = new byte[1024 * 1024]; // 创建 1M 文件
     
         public static void main(String[] args) {
             List<HeapOOM> list = new ArrayList<>();
             Date date = new Date();
             for (int i = 1; i <= 100; i++) {
                 list.add(new HeapOOM());
                 // 暂停一会儿线程
                 try { TimeUnit.MICROSECONDS.sleep(200); } catch (InterruptedException e) { e.printStackTrace(); }
             }
             System.out.println("数据添加完毕");
             new Scanner(System.in).next();
             list = null; // 使对象为空，好像垃圾收集器回收对象
             date = null;
             System.out.println("list and date Object 已清空，请继续操作");
             new Scanner(System.in).next();
             System.out.println("已结束");
         }
     }
     ```
     
   - <span id='fuzhiqingchu'>复制清除算法（Mark - Sweep）</span>：发生在年轻代，复制对象之后，谁空谁是 To 区
   
     当堆中的有效内存空间（available memory）被耗尽的时候，就会停止整个程序（也被称为 stop the world），然后会进行标记、清除操作
   
     - 标记：GC 会从引用根节点开始遍历，<font color='red'>标记所有被引用的对象</font>，一般是在对象的 Header 中记录为可达对象
       - 清除：GC 会对堆内存从头到尾进行线性的遍历，如果发现某个对象在其 Header 中没有标记为可达对象，就会将其回收
   
     <img src="https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/GC%20%E6%A0%87%E8%AE%B0%E6%B8%85%E9%99%A4%E7%AE%97%E6%B3%95.jpg" alt="GC 标记清除算法"  />
   
     **优缺点：**它的优点的通用性高，比较容易理解，但它也有很多缺点：
   
     - 但是它的效率不太高，会进行两次的数据全遍历
       - 在进行 GC 时，会停止整个程序（Stop The World），导致系统停顿
       - 这种方式清理出来的空闲空间是不连续的，会产生内存碎片。而且还需要维护一个空闲列表，列表也要占用内存空间
   
     **何为清除？**
   
     这里所谓的清除并不是真的置空，而是把需要清除的对象地址保存在空闲的地址列表里。下次有新对象需要加载时呢先判断垃圾的位置空间够不够，够的话才存放
   
   - 复制算法
   
     ![GC 复制算法](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/GC%20%E5%A4%8D%E5%88%B6%E7%AE%97%E6%B3%95.jpg)
   
   
      [JVM Garbage Collectors | Baeldung](https://www.baeldung.com/jvm-garbage-collectors)

7. **你说你做过 JVM 调优和参数配置，请问如何盘点查看 JVM 系统默认值**

   查看一个 JVM 的初始值：

   jps：查看 java 进程和应用程序主类的完整包名

   jinfo -flag：查看某个 JVM 属性值是否开启

   jinfo -flags：查看当前能查看到的所有参数

   > Non-default：默认的参数信息
   >
   > Command line：自己设置的参数信息

   **JVM 参数类型**

   - -X 参数

   - -XX 参数

     - Boolean 类型：-XX:+ 或 - 某个属性值，+ 表示开启；- 表示关闭

       > -XX:+PrintGCDetails：打印 GC 收集细节

     - KV 设置值类型：-XX: 属性 Key = 属性值 Value

       > -XX:MetaspaceSize=1024m	# 设置元空间为 1 G
       >
       > -XX:MaxTenuringThreshold=15	# 设置到老年代所要经过的次数

     那有两个经典参数：-Xms 和 -Xmx 你如何解释呢？（坑题！）

     它两还是属于 -XX 参数，相当于

     -Xms = -XX:InitialHeapSize

     -Xmx = -XX:MaxHeapSize

   **查看参数盘点家底**

   > `=` 是默认参数，`:=` 是修改后的最新参数 

   java -XX:+PrintFlagsInitial	# 查看初始值

   java -XX:+PrintFlagsFinal	# 查看最终值（初始值可能被修改掉）

   java -XX:+PrintCommandLineFlags -version	# 打印命令行参数，最后可以看到用的什么垃圾收集器

   ```java
   PS C:\Users\Daniel> java -XX:+PrintCommandLineFlags -version
   -XX:InitialHeapSize=530074624 -XX:MaxHeapSize=8481193984 -XX:+PrintCommandLineFlags -XX:+UseCompressedClassPointers -XX:+UseCom
   pressedOops -XX:-UseLargePagesIndividualAllocation -XX:+UseParallelGC
   openjdk version "1.8.0_292"
   OpenJDK Runtime Environment (build 1.8.0_292-b10)
   OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode)
   ```

   ```bash
   PS C:\Users\Daniel> jps
   10704
   11024 Jps
   22872 RemoteMavenServer36
   11548 HelloGC
   26572 Launcher
   PS C:\Users\Daniel> jinfo -flag PrintGCDetails 11548
   -XX:+PrintGCDetails
   PS C:\Users\Daniel> jinfo -flag MetaspaceSize 11548
   -XX:MetaspaceSize=21807104
   PS C:\Users\Daniel> jinfo -flag MaxTenuringThreshold 11548
   -XX:MaxTenuringThreshold=15
   PS C:\Users\Daniel> jinfo -flags 22856
   Attaching to process ID 22856, please wait...
   Debugger attached successfully.
   Server compiler detected.
   JVM version is 25.292-b10
   Non-default VM flags: -XX:CICompilerCount=4 -XX:InitialHeapSize=530579456 -XX:MaxHeapSize=8482979840 -XX:MaxNewSize=2827485184
   -XX:MinHeapDeltaBytes=524288 -XX:NewSize=176685056 -XX:OldSize=353894400 -XX:+UseCompressedClassPointers -XX:+UseCompressedOops
    -XX:+UseFastUnorderedTimeStamps -XX:-UseLargePagesIndividualAllocation -XX:+UseParallelGC
   Command line:  -javaagent:C:\Users\Daniel\AppData\Local\JetBrains\Toolbox\apps\IDEA-U\ch-0\212.4746.92\lib\idea_rt.jar=9231:C:\
   Users\Daniel\AppData\Local\JetBrains\Toolbox\apps\IDEA-U\ch-0\212.4746.92\bin -Dfile.encoding=UTF-8
   
   ```
   
8. **你平时工作用过的 JVM 常用基本配置参数有哪些？**

   [Java 8 JVM 参数 Java Platform, Standard Edition Tools Reference for Oracle JDK on Windows, Release 8](https://docs.oracle.com/javase/8/docs/technotes/tools/windows/index.html)

   - -Xms（ -XX:+InitialHeapSize）memory startup：初始化大小内存，默认为物理内存的 1/64

   - -Xmx（-XX:+MaxHeapSize）memory max：最大分配内存，默认为物理内存的 1/4

   - -Xss（-XX:+ThreadStackSize）stack size：设置单个线程栈的大小，默认为 512k~1024k（值依赖于是哪个平台）

   - -Xmn？（-XX:+MetaspaseSize）：该值越大，出现 Stack Overflow 异常的概率就变小了

     元空间的本质和永久代类似，都是对 JVM 规范中方法区的实现。不过元空间与永久代之间最大的区别在于<font color='red'>元空间并不在虚拟机中，而是使用本地物理内存</font>。因此，默认情况下元空间的大小仅受本地内存限制

   - -XX:+PrintGCDetails：输出 GC 收集日志的详细信息
   
     ```java
     // 设置启动 VM option 为 -Xms10m -Xmx10m -XX:+PrintGCDetails
     public class HelloGC {
         public static void main(String[] args) throws InterruptedException {
             System.out.println("Hello Java");
             byte[] bytes = new byte[50 * 1024 * 1024]; // new 50m 对象，引发OOM
         }
     }
     // ================================== print out ==================================
     // [GC (Allocation Failure) [PSYoungGen: 1631K->488K(2560K)] 1631K->692K(9728K), 0.0009002 secs] [Times: user=0.00 sys=0.00, real=0.00 secs]
     // [GC (Allocation Failure) [PSYoungGen: 488K->496K(2560K)] 692K->748K(9728K), 0.0005780 secs] [Times: user=0.00 sys=0.00, real=0.00 secs]
     // [Full GC (Allocation Failure) [PSYoungGen: 496K->0K(2560K)] [ParOldGen: 252K->585K(7168K)] 748K->585K(9728K), [Metaspace: 3221K->3221K(1056768K)], 0.0056517 secs] [Times: user=0.09 sys=0.00, real=0.01 secs]
     // [GC (Allocation Failure) [PSYoungGen: 0K->0K(2560K)] 585K->585K(9728K), 0.0008178 secs] [Times: user=0.00 sys=0.00, real=0.00 secs]
     // [Full GC (Allocation Failure) [PSYoungGen: 0K->0K(2560K)] [ParOldGen: 585K->568K(7168K)] 585K->568K(9728K), [Metaspace: 3221K->3221K(1056768K)], 0.0055486 secs] [Times: user=0.00 sys=0.00, real=0.00 secs]
     // Heap
     //  PSYoungGen      total 2560K, used 89K [0x00000000ffd00000, 0x0000000100000000, 0x0000000100000000)
     //   eden space 2048K, 4% used [0x00000000ffd00000,0x00000000ffd16510,0x00000000fff00000)
     //   from space 512K, 0% used [0x00000000fff00000,0x00000000fff00000,0x00000000fff80000)
     //   to   space 512K, 0% used [0x00000000fff80000,0x00000000fff80000,0x0000000100000000)
     //  ParOldGen       total 7168K, used 568K [0x00000000ff600000, 0x00000000ffd00000, 0x00000000ffd00000)
     //   object space 7168K, 7% used [0x00000000ff600000,0x00000000ff68e140,0x00000000ffd00000)
     //  Metaspace       used 3278K, capacity 4496K, committed 4864K, reserved 1056768K
     //   class space    used 349K, capacity 388K, committed 512K, reserved 1048576K
     // Exception in thread "main" java.lang.OutOfMemoryError: Java heap space at com.liuilin.JVM.GC.HelloGC.main(HelloGC.java:10)
     ```
     
     - GC：新生代
     
       ![GC 分配内存详解（PrintGCDetails）](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/GC%20%E5%88%86%E9%85%8D%E5%86%85%E5%AD%98%E8%AF%A6%E8%A7%A3%EF%BC%88PrintGCDetails%EF%BC%89.jpg)
     
       新生区占 1/3，养老区占 2/3 = 2560k + 9728k = 10m
     
     - FullGC：老年代
     
       规律：名称 - GC前内存占用 -> GC后内存占用（该区内存总大小）
     
   - -XX:SuvivorRatio
   
     新生代（Young 区）中 Eden、s0 和 s1 空间的比例默认 - XX:SurvivorRatio=8， Eden:S0:S1 =8:1:1 
   
     设置 -XX:SurvivorRatio=4， Eden:S0:S1 = 4:1:1
   
     SurvivorRatio 值就是设置 eden 区的比例占多少
   
     > Young 区配置小了 GC 就会很频繁，老年代需要配的很大
   
   - -XX:NewRatio：配置年轻代与年老代在堆结构的占比
   
     默认 - XX:NewRatio=2 新生代占 1 老年代 2，年轻代占整个堆的 1/3
   
     假如 - XX:NewRatio=4 新生代占 1，老年代 4，年轻代占整个堆的 1/5。NewRatio 值就是设置老年代的占比，剩下的 1 给新生代
   
     新生代 1/3 ，老年代 2/3
   
     > -Xms10m -Xmx10m -XX:+PrintGCDetails -XX:+UseSerialGC -XX:NewRatio=2
   
   - -XX:MaxTenuringThreshold：设置垃圾最大年龄，也就是 From 和 To 之间移动的次数
   
     -XX MaxTenuring Threshold=0：如果设置为0的话，则年轻代对象不经过 Survivor区，直接进入年老代。对于年老代比较多的应用，可以提高效率。
   
     如果这个值设置的比较大，那么年轻代对象会在 Survivor 区进行多次复制，这样可以增加对象在年轻代的存活时间，不让它那么容易的进入年老代。理论上是这样，但是 Java8 开始就这个值就只能设置为 0-15 之间
   
   典型 JVM 参数设置案例：
   
   配置前查看初始参数
   
   ```java
   -XX:G1ConcRefinementThreads=8 
   -XX:GCDrainStackTargetSize=64 
   -XX:InitialHeapSize=530074624 // 530m
   -XX:MaxHeapSize=8481193984 // 8G
   -XX:+PrintCommandLineFlags // 
   -XX:ReservedCodeCacheSize=251658240 
   -XX:+SegmentedCodeCache 
   -XX:+UseCompressedClassPointers 
   -XX:+UseCompressedOops 
   -XX:+UseG1GC // G1 垃圾收集器
   -XX:-UseLargePagesIndividualAllocation 
   ```
   
   配置参数后： -Xms128m -Xmx4096m -Xss1024k -XX:MetaspaceSize=512m -XX:+PrintCommandLineFlags -XX:+PrintGCDetails -XX:+UseSerialGC
   
   ```java
   -XX:InitialHeapSize=134217728 // 128m
   -XX:MaxHeapSize=4294967296 // 4G
   -XX:MetaspaceSize=536870912 // 512m
   -XX:+PrintCommandLineFlags
   -XX:+PrintGCDetails
   -XX:ThreadStackSize=1024 // 1024k
   -XX:+UseCompressedClassPointers
   -XX:+UseCompressedOops
   -XX:-UseLargePagesIndividualAllocation
   -XX:+UseSerialGC // 并行垃圾收集器
   ```
   
   ```java
   public static void main(String[] args) throws InterruptedException {
       long totalMemory = Runtime.getRuntime().totalMemory(); // JVM 总内存量
       long maxMemory = Runtime.getRuntime().maxMemory(); // JVM 试图使用的最大内存量
       System.out.println("TOTAL_MEMORY(-Xms) = " + totalMemory + "（字节）" + (totalMemory / 1024 / 1024) + "MB");
       System.out.println("MAX_MEMORY(-Xmx) = " + maxMemory + "（字节）" + (maxMemory / 1024 / 1024) + "MB");
   }
   // ==================================== print out(32G computer) ====================================
   // TOTAL_MEMORY(-Xms) = 508559360（字节）485MB
   // MAX_MEMORY(-Xmx) = 7540834304（字节）7191MB
   ```
   
9. **Java 中的四种引用类型及其含义（强软弱虚引用的区别以及 GC 对他们执行怎样的操作）**

   - 强引用：就算 OOM 了也不回收，就是普通的引用 Person p = new Person ()；

     ```java
     private static void softRefMemoryEnough() {
         Object o1 = new Object();
         SoftReference<Object> softReference = new SoftReference<>(o1);
         System.out.println("o1 = " + o1);
         System.out.println("softReference = " + softReference.get());
         o1 = null;
         System.gc();
         System.out.println("o1 = " + o1);
         System.out.println("softReference = " + softReference.get());
     }
     // ================================== print out ==================================
     // o1 = java.lang.Object@75b84c92
     // softReference = softReference = java.lang.Object@75b84c92
     // o1 = null
     // softReference = java.lang.Object@75b84c92
     ```

   - 软引用：需要用 `java.lang.ref.SoftReference` 类来实现，JVM 内存够用时不会被回收，不足时会被回收。通常用在对内存敏感的程序中，像 MyBatis 里面的一些内部类用的就是软引用

     ```java
     /**
      * VM 配置，故意产生大对象并配置小的内存，让它内存不够用了导致 OOM，看软引用的回收情况
      * -Xms10m -Xmx10m -XX:+PrintGCDetails
      */
     private static void softRefMemoryNotEnough() {
         Object o1 = new Object();
         SoftReference<Object> softReference = new SoftReference<>(o1);
         System.out.println("o1 = " + o1);
         System.out.println("softReference = " + softReference.get());
         o1 = null;
         try {
             byte[] bytes = new byte[30 * 1024 * 1024]; // new 30m 大小的对象，导致 OOM
         } catch (Exception e) {
             e.printStackTrace();
         } finally {
             System.out.println("o1 = " + o1);
             System.out.println("softReference = " + softReference.get());
         }
     }
     // o1 = java.lang.Object@75b84c92
     // softReference = java.lang.Object@75b84c92
     // o1 = null
     // softReference = null
     ```

   - 弱引用 WeakReference：需要用 `java.lang.ref.WeakReference` 类来实现，只要发生 GC 就会被回收无论 JVM 内存是否足够都会回收该对象占用的内存

     ```java
     public class WeakReferenceDemo {
         public static void main(String[] args) {
             Object o1 = new Object();
             WeakReference<Object> weakReference = new WeakReference<>(o1);
             System.out.println("o1 = " + o1);
             System.out.println("weakReference = " + weakReference.get());
     
             o1 = null;
             System.gc();
     
             System.out.println("o1 = " + o1);
             // 此时只将 o1 置为了 null，然后进行 GC，没有动 WeakReference 对象，但结果是只要进行了 GC，软引用对象就会被回收
             System.out.println("weakReference = " + weakReference.get());
         }
         // ================================ print out ================================
         // o1 = java.lang.Object@75b84c92
         // weakReference = java.lang.Object@75b84c92
         // o1 = null
         // weakReference = null
     }
     ```

     你既然知道弱引用的话，能谈谈 WeakHashMap 吗？

     只要 key 置为 null 并触发 GC，弱引用对象就会被回收
     
     ```java
     private static void weakHashMap() {
         WeakHashMap<Integer, String> weakHashMap = new WeakHashMap<>();
         Integer key = new Integer(2); // 必须要是 new Integer(2) 对象，因为下面需要把 key 置为 null
         String value = "WeakHashMap";
         weakHashMap.put(key, value);
         System.out.println("weakHashMap = " + weakHashMap);
         key = null; // 只要 key 置为 null 并触发 GC，弱引用对象就会被回收
         System.out.println("weakHashMap = " + weakHashMap);
         System.gc();
         System.out.println("weakHashMap = " + weakHashMap);
     }
     // weakHashMap = {2=WeakHashMap}
     // weakHashMap = {2=WeakHashMap}
     // weakHashMap = {}
     ```
     
   - 虚引用 PhantomReference：
   
     虚引用需要java.lang.ref.PhantomReference类来实现
   
     虚引用并不会决定对象的生命周期。如果一个对象仅持有虚引用，<font color='red'> 那么它就和没有任何引用一样，在任何时候都可能被垃圾回收器回收 </font>，它不能单独使用也不能通过它访问对象，虚引用必须和引用队列（Reference Queue）联合使用。
   
     虚引用的主要作用是跟踪对象被垃圾回收的状态。只是提供了一种确保对象被 finalize 以后，做某些事情的机制。PhantomReference 的 get 方法总是返回 nul，因此无法访问对应的引用对象。其意义仅仅在于监控对象的回收状态，假如一个对象已经进入 finalization 阶段，可以在 GC 回收的前做进一步处理
   
     也就是说设置虚引用关联的唯一目的就是 <font color='red'> 在这个对象被收集器回收的前收到一个系统通知或者后续添加进一步的处理 </font>。类似于 Servlet 的生命周期，在 destory 之前可以再做点事情。比如打印一句话 —— 我要被回收啦（相当于一种通知机制）
   
     ```java
     public class PhantomReferenceDemo {
         public static void main(String[] args) throws InterruptedException {
             Object o1 = new Object();
             ReferenceQueue<Object> referenceQueue = new ReferenceQueue<>();
             PhantomReference<Object> phantomReference = new PhantomReference<>(o1, referenceQueue);
     
             System.out.println("o1 = " + o1);
             System.out.println("phantomReference = " + phantomReference.get());
             System.out.println("referenceQueue = " + referenceQueue.poll());
     
             o1 = null;
             System.gc();
             Thread.sleep(500); // 确保执行完 GC
             System.out.println("==================== ok ====================");
     
             System.out.println("o1 = " + o1);
             System.out.println("phantomReference = " + phantomReference.get());
             // 打印出来对象，原因是弱引用被垃圾回收前一刻会放入引用队列
             System.out.println("referenceQueue = " + referenceQueue.poll());
         }
         // ================================ print out ================================
         // o1 = java.lang.Object@75b84c92
         // phantomReference = null
         // referenceQueue = null
         // ==================== ok ====================
         // o1 = null
         // phantomReference = null
         // referenceQueue = java.lang.ref.PhantomReference@6bc7c054
     }
     ```
     引用队列 ReferenceQueue：软 / 若 / 虚引用都有个机制，被回收前需要保存在引用队列下
     
     > 软 / 若 / 虚引用的对象调 get 方法只会返回 null，但是一旦发生 GC，就会把对象放入到引用队列里
     
     软 / 弱应用实际应用场景：假如有一个应用需要读取大量本地图片（安卓手机、Chrome 浏览器和前端都是需要加载大量图片），每次请求都从硬盘读取的话很慢，会严重影响系统性能。可以加载进缓存，但是如果一次性全部加载进内存中又有可能造成 OOM
     
     解决（软引用、弱引用）：设计思路是用一个 HashMap 来保存图片的路径和相应图片对象关联的软引用（或弱引用）之间的映射关系，在内存不足时， JVM 会自动回收这些缓存图片对象所占用的空间，从而有效地避免了 OMM 的问题
     
     > ```java
     > Map<String, SoftReference<Bitmap>> imageCache = new HashMap<String,SoftReference<Bitmap>>();
     > ```
     >
     > 内存够的时候用缓存 + 内存，不够要 GC 时，就把缓存的那部分释放加给内存，从而避免 OOM
     >
     > MyBatis 缓存源码那块大量使用到的就是软引用

10. **GC 的常见算法，CMS 以及 G1 的垃圾回收过程，CMS 的各个阶段哪两个是 Stop the world 的，CMS 会不会产生碎片，G1 的优势**

11. **GC 每个算法的优缺点和原理(标记清除和标记整理算法的理解以及优缺点)**

    - 标记清除算法（年老代）

      - 标记：标记的过程其实就是遍历所有的GC Roots，然后将所有GC Roots可达的对象标记为存活对象
      - 清除：清除的过程会遍历堆中所有的对象，将没有标记的对象全部清除掉
      - 缺点：内存不连续，对象放不下时有可能提前触发GC；效率不高：先标记，后清除

    - 复制清除算法（分配担保原则）（年轻代​）

      - 原理：将原有的内存空间分为两块，每次只使用其中一块，在垃圾回收时，将正在使用的内存中的存活对象复制到未使用的内存块中，然后清除正在使用的内存块中的所有对象
      - 缺点：内存缩小为原来的一半
      - 优点： 效率高 ，存放的对象本身就是一些朝生夕死的对象（没有太多引用指向它，移动起来高效）

      > JVM规范中，Eden区和Survivor区比例1 ：1，而商业版JVM实现上，它的比例是8：1：1
      >
      > 分配担保原则： 年轻代在内存中只有1/4 ，相对整个堆而言较小，而且还要分配两片内存给s0，s1。所以如果此时出现了一个大对象，这个对象就会直接进入年老代

    - 标记压缩算法/标记整理算法（年老代）

      - 原理：标记所有从根节点开始的可达对象，将将所有的存活对象压缩到内存的一端，然后将剩下的死亡对象和空闲对象全部回收
      - 缺点：效率低（对象自身特征决定的，一般都是些池对象，被很多引用指向的对象，移动起来费劲）
      - 优点： 没有内存碎片

    - 分代收集算法

      - 存活率低：少量对象存活，适合复制算法：在新生代中，每次GC时都发现有大批对象死去，只有少量存活（新生代中98%的对象都是“朝生夕死”），那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成GC
      - 存活率高：大量对象存活，适合用标记-清理/标记-整理：在老年代中，因为对象存活率高、没有额外空间对他进行分配担保，就必须使用“标记-清理”/“标记-整理”算法进行GC

12. **请你谈谈 OOM？| 你工作中都遇到过哪些异常（错误）**

    ![the_superclass_of_all_errors_and_exceptions](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/the_superclass_of_all_errors_and_exceptions.jpg)

    - java.lang.StackOverflowError

    - java.lang.OutOfMemoryError: Java heap space

    - java.lang.OutOfMemoryError: GC overhead limit exceeded：连续进行多次 GC，98% 的时间都拿去做 GC，但却回收了不到 2% 的堆内存，效果非常不理想

      ```java
      public class GCOverhead {
          public static void main(String[] args) {
              int i = 0;
              List<String> list = new ArrayList<>();
              try {
                  while (true) {
                      list.add(String.valueOf(++i).intern());
                  }
              } catch (Throwable e) {
                  System.out.println("---i: " + i);
                  e.printStackTrace(); // java.lang.OutOfMemoryError: GC overhead limit exceeded
                  throw e;
              }
          }
      }
      ```

      > 经历了多轮 FullGC 发现垃圾还是回收不动：[Full GC (Ergonomics) [PSYoungGen: 2047K->2047K(2560K)] [ParOldGen: 7082K->7082K(7168K)] 9130K->9130K(9728K), [Metaspace: 3290K->3290K(1056768K)], 0.0288561 secs] [Times: user=0.09 sys=0.00, real=0.03 secs] 
      >
      > 新 new 的字符串对象在常量池，Java8 之后在 MetaSpace

    - java.lang.OutOfMemoryError: Direct buffer memory

      ```java
      // 配置参数：-Xms10m -Xmx10m -XX:+PrintGCDetails -XX:MaxDirectMemorySize=5m
      public class DirectBufferMemory {
          public static void main(String[] args) {
              System.out.println("配置的 MaxDirectMemory：" + sun.misc.VM.maxDirectMemory() / (double) 1024 / 1024 + "MB");
              //暂停一会儿线程
              try { TimeUnit.SECONDS.sleep(3); } catch (InterruptedException e) { e.printStackTrace(); }
              // Exception in thread "main" java.lang.OutOfMemoryError: Direct buffer memory
              ByteBuffer byteBuffer = ByteBuffer.allocateDirect(6 * 1024 * 1024);
          }
      }
      ```

      导致原因：写 NIO 程序经常使用 ByteBuffer 来读取或者写入数据，这是一种基于通道（Channe）与缓冲区（Buffer）的 I/O 方式，它可以使用 Native 函数库直接分配堆外内存，然后通过一个存储在 Java 堆里的 DirectByteBuffer 对象作为这块内存的引用进行操作。这样就能在一些场景中显著提高性能，因为避免了在 Java 堆 和 Native 堆中来回复制数据

      ByteBuffer.allocate (capability) 第一种方式是分配 JVM 堆内存，属于 GC 管辖范围，由于需要拷贝所以速度相对较慢

      ByteBuffer. allocteDirect (capability) 第二种方式是分配 OS 本地内存，不属于 GC 管辖范围，由于不需内存拷贝所以速度相对较快。

      但如果不断分配本地内存，堆内存很少使用，那么 VM 就不需要执行 GC, DirectByteBuffer 对象们就不会被回收，这时候堆内存充足，但本地内存可能已经使用光了，再次尝试分配本地内存就会出现 OutOfMemoryError，那程序就直接崩溃了。

      > 本地物理内存是一开始 JVM 分配的是总物理内存的 1/4，比如 32G 内存，1/4 就是 8G

    - java.lang.OutOfMemoryError: unable to create new native thread：高并发请求服务器时，经常会出现该异常，准确的讲该 native thread 异常与对应的平台有关

      ```java
      public class UnableCreateNewThread {
          public static void main(String[] args) {
              // 没有跳出循环的条件，会一直创建线程
              for (int i = 0; ; i++) {
                  System.out.println("i = " + i);
                  new Thread(() -> {
                      //暂停一会儿线程，让每一个线程都阻塞在这里，诱导发生 OOM
                      try { TimeUnit.SECONDS.sleep(Integer.MAX_VALUE); } catch (InterruptedException e) { e.printStackTrace(); }
                  }, ""+i).start();
              }
          }
      }
      ```

      导致原因：

      - 应用程序创建了太多线程（一个应用进程创建多个线程超过系统承载极限）
      - 应用服务器并不允许你 linux 的应用程序创建这么多线程，系统默认允许单个进程可以创建的线程数是 1024 个。应用创建超过这个数量，就会报 java.lang.OutOfMemoryError: unable to create new native 

      解决办法：

      - 想办法降低你应用程序创建线程的数量，分析应用是否真的需要创建这么多线程，如果不是就需要修改代码将线程数降到最低
      - 但对于有的应用，确实需要创建很多线程，远超过 linux 系统的默认普通用户 1024 个线程的限制（root 用户不受限），那可以通过修改 Linux 服务器配置，扩大 linux 的默认限制

    - java.lang.OutOfMemoryError: Metaspace

      JVM 参数

      -XX:MetaspaceSize=10m -XX:MaxMetaspaceSize=10m

      ```java
      public class MetaspaceOOM {
          static class OOMTest{}
          public static void main(String[] args) {
              int i = 0;
              try {
                  while (true) {
                      i++;
                      Enhancer enhancer = new Enhancer();
                      enhancer.setSuperclass(OOMTest.class);
                      enhancer.setUseCache(false);
                      enhancer.setCallback(new MethodInterceptor() {
                          @Override
                          public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable {
                              return methodProxy.invokeSuper(o,args);
                          }
                      });
                      enhancer.create();
                  }
              } catch (Throwable e) {
                  // --- 多少次后发生了异常：540
                  // java.lang.OutOfMemoryError: Metaspace
                  System.out.println("--- 多少次后发生了异常："+i);
                  e.printStackTrace();
              }
          }
      }
      ```

      Java8 以后用的是 Metaspace 来替代永久代，Metaspace 是方法区在 HotSpot 中的实现，它与永久代最大的区别在于 Metaspace 并不在虚拟机内存而是使用的是本地内存

      永久代（java8 后被 Metaspace 原空间取代了）存放了以下信息：1. 虚拟机加载的类信息（像什么 rt.jar 包中的 Object.class、ArrayList.class 等等 2. 常量池 3. 静态变量 3. 即时编译后的代码

      导致原因：元空间初始大小只有 20m 左右，在元空间里面不断的加载静态类，最终会把 Metaspace 撑爆

      > java -XX:+PrintFlagsInitial 查看的 MetaspaceSize = 21810376（20m 多）

13. 哈哈哈

    年轻代：用复制算法

    年老代：用标记清除算法、标记整理算法

    Garbage Collectors 垃圾收集器：
    
    ![Garbage Collectors](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/Garbage%20Collectors.jpg)
    
    - Serial（串行垃圾收集器）顺序、依次的意思：它为单线程环境设计且只用一个线程进行垃圾收集，收集垃圾时会暂停用户线程，所以不适合服务器环境
    - Parallel（并行垃圾收集器）：多个垃圾收集线程并行工作，垃圾收集时用户线程也是要暂停的。比较适用于科学计算 / 大数据处理首台处理等弱交互场景
    - CMS（并发垃圾收集器）：用户线程和垃圾收集线程同时执行（不一定是并行，可能是交替执行），它不需要停顿用户线程。适用对响应时间有要求的场景，停个 0.01s 当然可以，停个七八秒的话那肯定不行。互联网公司一般都用它
    - Garbage first（G1 垃圾收集器）：Java8 开始用 G1 收集器。G1 垃圾回收器将堆内存分割成不同的区域然后并发的对其进行垃圾回收
    
    > 停顿操作：就是 STW（Stop-the-world）
    >
    > 好比在餐厅吃饭，一个清洁工过来和你说，您先起来一下我打扫下卫生（Serial）；多个清洁工过来和你说（Parallel）；多个清洁工说您先搬到另一桌吃着，我们打扫完您再回来（CMS）
    
14. **怎么查看服务器默认垃圾收集器是哪个？生产上是如何配置垃圾收集器的？谈谈你对垃圾收集器的理解？**

    > http://www.ityouknow.com/jvm/2017/09/03/jvm-command.html
    >
    > Server / Client 模式分别什么意思？。32 位 Windows 系统默认只会使用 Client 的 JVM 模式。32 位其它系统，2G 内存同时有 2 个 CPU 以上用 Server 模式，低于该配置还是 Client 模式。64 位 only server 模式
    >
    
    - 年轻代
    
      - 串行收集器

      - 并行收集器
    
        年轻代里面有 Eden 区，天天 new 对象，会打扫的频繁一些，也就是在频繁创建对象的 Young 区有多个 GC 线程。年老代是活过 15 次才移过来，所以不用频繁打扫，还是只有一个 GC 线程
    
        JVM 参数：-XX:+UseParNewGC（启动 ParNew 收集器，只影响年轻代，不影响年老代）
    
        开启上述参数后，会使用：ParNew（年轻代用）+ Serial Old（年老代用）的收集器组合，新生代使用复制算法，老年代采用标记整理算法
    
        > -Xms10m -Xmx10m -XX：+PrintGCDetails-XX：+PrintCommandL ineFlags -XX：+UseserialGC（DefNew+Tenured）
        >
        > -Xms10m -Xmx10m -XX：+PrintGCDetails -XX：+PrintCommandLineFlags -XX：+UseParNewGC（ParNew+Tenured）
        >
        > -Xms10m -Xmx10m -XX：+PrintGCDetails -XX：+PrintCommandL ineFlags -XX：+UseParalleLGC（PSYoungGen+ParoldGen）
        >
        > 4.1-Xms10m-xmx10m-xx：+PrintGCDetails-xx：+PrintCommandLineFLags-xx：+UseParalleloldGC（PSYoungGen+ParoldGen）100
        >
        > 4.2不加就是默认 UseParal lelGC-Xms10m -Xmx10m -XX：+PrintGCDetails -XX：+PrintCommandLineFlags（PSYoungGen+ParoldGen）
        >
        > -Xms10m -Xmx10m -XX：+PrintGCDetails -XX：+PrintCommandLineFlags -XX：+UseConcMarkSweepGC
      - 并行收集器
    
        Parallel ScavengeParNew 收集器类似也是一个新生代垃圾收集器，使用的是复制算法，也是一个并行的多线程的垃圾收集器，俗称吞吐量优先收集器。一句话：串行收集器在年轻代和年老代的并行化
    
        它关注的重点是：可控制的吞吐量
    
        > 吞吐量（Thoughput = 运行用户代码时间 /（运行用户代码时间 + 垃圾收集时间），也即比如程序运行 100 分钟，垃圾收集时间 1 分钟，吞吐量就是 99%）。高吞吐量意味着高效利用 CPU 的时间，它多用于在后台运算而不需要太多交互的任务。
    
    - 年老代
    
      - 串行收集器
    
      - 并行收集器
    
      - ==并发标记清除收集器 CMS==
    
        cMs收集器（Concurrent Mark Sweep：并发标记清除）是一种以获取最短回收停顿时间为目标的收集器。适合应用在互联网站或者B/S系统的服务器上，这类应用尤其重视服务器的响应速度，希望系统停顿时间最短。CMS非常适合堆内存大、CPU核数多的服务器端应用，也是G1出现之前大型应用的首选收集器。
    
        四个步骤：
    
        - 初始标记（initial mark）：标记 GCRoots 可达对象，耗时短
    
        - 并发标记（concurrent mark）：从第一步标记的对象出发，并发的标记可达对象
    
        - 重新标记（CMS remark）：可能有的对象又重新被用起来了，就不能清除，需要修正一下标记结果
    
        - 并发清除（CMS concurrent sweep）和用户线程一起：清除 GCRoots 不可达对象，和用户线程一起工作，不需要暂停工作线程。然后根据标记结果直接清理对象
    
          优点：并发收集停顿低
    
          缺点：
    
          i. 并发执行对 CPU 资源压力大
    
          > 由于并发进行，CMS在收集与应用线程会同时会增加对堆内存的占用，也就是说，CMS必须要在老年代堆内存用尽之前完成垃圾回收，否则CMS回收失败时，将触发担保机制，串行老年代收集器将会以STW的方式进行一次GC，从而造成较大停顿时间I
    
          ii. 采用的标记清除算法会导致大量内存碎片
    
    - 如何选择垃圾收集器？
    
      | 参数                                     | 新生代垃级收集器             | 新生代算法                             | 老年代垃圾收集器                                             | 老年代算法 |
      | :--------------------------------------- | ---------------------------- | -------------------------------------- | ------------------------------------------------------------ | :--------- |
      | -XX:+UseSerialGC                         | SerialGC                     | 复制                                   | SerialOldGC                                                  | 标整       |
      | -XX:+UseParNewGC                         | ParNew                       | 复制                                   | SerialOldGC                                                  | 标整       |
      | -XX:+UseParallelGC/-XX:+UseParallelOldGc | Parallel[Scavenge]           | 复制                                   | Parallel Old                                                 | 标整       |
      | -XX:+UseConcMar kSweepGC                 | ParNew                       | 复制                                   | CMS+SerialOld 的收集器组合（Serial Old 作为 CMS 出错的后备收集器） | 标清       |
      | -XX:+UseG1GC                             | G1 整体上采用标记 - 整理算法 | 局部是通过复制算法，不会产生内存碎片。 |                                                              |            |


15. **G1 垃圾收集器**

    > -Xms10m-xmx10m-xx:+PrintGcDe-xx:PrintCommand ineFlags-xx:UseG1GC

    G1 是什么：

    CMS垃圾收集器虽然减少了暂停应用程序的运行时间，但是它还是存在着内存碎片问题。于是，为了去除内存碎片问题，同时又保留CMS垃圾收集器低暂停时间的优点，也就是 STW 时间更可控，G1在停顿时间上添加了预测机制，用户可以指定期望停顿时间，JAV7发布了一个新的垃圾收集器-G1垃圾收集器。JDK9中 G1 已经成为了默认收集器
    
    特点：
    
    1：G1能充分利用多CPU、多核环境硬件优势，尽量缩短STW
    2：G1整体上采用标记整理算法，局部是通过复制算法，不会产生内存碎片。
    3：宏观上看G1之中不再区分年轻代和老年代。把内存划分成多个独立的子区域（Region），可以近似理解为一个围棋的棋盘。
    4：G1收集器里面讲整个的内存区都混合在一起了，但其本身依然在小范围内要进行年轻代和老年代的区分，保留了新生代和老年代，但它们不再是物理隔离的，而是一部分 Region的集合且不需要是连续的，也就是说依然会采用不同的GC方式来处理不同的区域。
    5：G1虽然也是分代收集器，但整个内存分区不存在物理上的年轻代与老年代的区别，也不需要完全独立的 survivor（to space）堆做复制准备。G1只有逻辑上的分代概念，或者说每个分区都可能随G1的运行在不同代之间前后切换；
    
    底层原理：
    
    Region 区域化垃圾收集器：最大好处是化整为零，避免全内存扫描，只需要按照区域来进行扫描即可。
    
    > 如果一个对象特别特别大，应该直接放到养老区
    
    实战 - 配置 G1 参数： 开始 G1 + 设置最大内存 + 最大 GC 停顿时间 -XX:+UseG1GC -Xmx32g -XX:MaxGCPauseMillis=100
    
    > 100 毫秒就把垃圾收集完
    
    和 CMS 相比有两个优势：1. 不会产生内存碎片 2. 可以精准的控制停顿时间，它是把堆（年轻代和年老代）划分为多个固定大小的区域，每次根据允许停顿的时间去收集垃圾最多的区域。
    
    我是个订单微服务，启动的时候需要堆内存大一些，不要用默认的，java -server -Xms1024m -Xmx1024m -X:+UseG1GC -jar springboot1.0-SNAPSHOT.war
    
    > 有没有把 SpringBoot 部署到 Undertow 下面？部署了的，我们在 Undertow 下用 Jmeter 做压测，QPS 比 tomcat 要高 3000 多
    
16. **生产环境服务器变慢，谈谈你的诊断思路和性能评估？| 说说 Linux 下你常用的命令？**

    > 系统慢一般情况下是两种情况：一个是 CPU，还有个是磁盘 IO

    - 整机：top

      uptime：系统性能命令的精简版，可以打印出 load average

      load average 系统平均负载值：v1.57+0.89+0.40=2.86/3=0.953*100% 大于 60%，说明系统负担比较重，需要优化程序

    - CPU：vmstat -n 2 3（查看 CPU）

      一般 vmstat 工具的使用是通过两个数字参数来完成的，第一个参数是采样时间间隔数单位是秒，第二个参数是采样的次数

      procs

      - r：运行和等待 CPU 时间片的进程数，原则上 1 核的 CPU 的运行队列不要超过 2，整个系统的运行队列不能超过总核数的 2 倍，否则代表系统压力过大
      - b：等待资源的进程数，比如正在等待磁盘 I/0、网络 I/0 等。

      CPU

      - us：用户进程消耗 CPU 时间百分比，us 值高代表用户进程消耗 CPU 时间多，如果长期大于 50%，则需要优化程序
      - sy：内核进程消耗的 CPU 时间百分比
      - us+sy：参考值为 80%，如果 us+sy 大于 80%，说明可能存在 CPU 不足。
      - id：处于空闲的 CPU 百分比
      - wa：系统等待 IO 的 CPU 时间百分比
      - st：来自于一个虚拟机偷取的 CPU 时间的百分比工

      > 查看额外：
      >
      > - 查看所有 CPU 核信息：mpstat -P ALL 2
      > - 每个进程使用 CPU 的用量分解信息：pidstat -u 1 -p 进程编号

    - 内存：free

      应用程序可用内存/系统物理内存>70%内存充足

      应用程序可用内存/系统物理内存<20%内存不足，需要增加内存

      20%<应用程序可用内存/系统物理内存<70%内存基本够用

      查看额外：pidstat -p 进程号 -r 采样间隔秒数

    - 硬盘：df [-h]

      查看文件系统磁盘空间使用情况（-h：human readable）以人类可读的格式

    - 磁盘 IO：iostat

      大表存储，大批量的进行磁盘 IO 就会导致 IO 慢，长时间 IO 慢也会导致系统出问题

      > iostat -xdk 2 3：2 秒取样一次，共取样 3 次
      >
      > -x     Display extended statistics. 显示扩展统计信息。
      >
      > -d     Display the device utilization report. 显示设备利用率报告。
      >
      > -k     Display statistics in kilobytes per second. 以千字节每秒显示统计信息。
      >
      > - rkB/s 每秒读取数据量 kB
      > - wkB/s 每秒写入数据量 kB
      > - svctm I/O 请求的平均服务时间，单位毫秒
      > - await I/O 请求的平均等待时间，单位毫秒；值越小，性能越好
      > - <font color='red'>util 一秒里有百分几的时间用于 IO 操作。接近 100% 时，表示磁盘带宽跑满了，就需要优化程序或者增加磁盘</font>
      > - rkB/s、wkB/s 根据系统应用不同会有不同的值，但有规律遵循：长期、超大数据读写，肯定不正常，需要优化程序的读取。
      > - svctm 的值与 await 的值很接近，表示几乎没有 IO 等待，磁盘性能好，如果 await 的值远高于 svctm 的值，则表示 l/O 队列等待太长，需要优化程序或换更快的磁盘。

      查看额外：pidstat -d [采样间隔秒数] -p [进程号]

    - 网络 IO：ifstat

      ifstat l

      可以查看各个网卡的 in、out，观察网络负载情况，程序网络读写是否正常，程序网络优化增加网络带宽

17. **假如生产环境出现 CPU 占用过高，请谈谈你的分析思路和定位？ | 谈谈你记忆深刻的故障**

    一般出故障了，你如何调试 + 排查 + 检索？

    Google + Stack Overflow + GitHub

    i. 首先我会用 top 命令找到 CPU 占比最高的程序

    ii. 然后用 ps -ef | grep java 或 jps 进一步定位，来得知是怎样一个后台程序惹的事

    <font color='red'>iii. 定位到具体线程或代码</font>

    ​	ps -mp [进程 id] -o THREAD,tid,time

    ​	参数解释：-m：显示所有线程；-p 进程使用 CPU 时间；-o 参数后为用户自定义格式 [线程的 id，时间]

    iv. 定位到线程后将 10 进制的线程 ID 转换为 16 进制英文小写格式（可以用命令 printf "%x\n" 或计算器）

    v. jstack [线程 id] | grep [线程 16 进制 id] -A60

    ​	打印前 60 行，然后直接找到项目的包名并定位到具体行数

18. **对于 JDK 自带的 JVM 监控和性能分析工具用过那些？一般你是怎么用的？**

    - jps

    - jinfo

    - jmap（抓取内存快照）

      Case：

      - 映射对快照：jmap -heap [进程 ID]

      - 抓取堆内存：

        - 生成 hprof 文件并下载到本地

          

        - MAT 分析插件工具

    - jstat（统计信息监视工具）

19. **JVM优化**

    https://www.liaoxuefeng.com/article/1336345083510818

    MaxPermSize：permanent最大永久大小

    最深刻的就是最近做的在线教育项目，其中我在里面有负责一个课程搜索模块，就是把用户搜索的内容进行分词然后把相关度高的结果返回到页面，当时把后台模块写好了以后是可以在本地系统上运行的，但是用 Jenkins 构建以后一运行就遇到了 OOM 内存溢出错误，当时遇到的这个问题的时候第一时间想起来了以前看的关于虚拟机的书，于是我在虚拟机的配置上添加了一个参数 `- XX：+HeapDumpOnOutOfMemoryError` ，尝试将 dump 文件保留到本地，然后采用了 jmap 命令去分析这个 dump 文件，发现年老代那个参数 Old Generation 每次一启动都是满着的，然后去看了虚拟机的配置参数，发现虚拟机中的 - xmx 参数只有 256M，而程序需要的内存大小为 500M（因为需要加载一个 MIT 的提取名词的包），所以最后通过设置成 1024M 解决了这个问题，我感觉通过这个学到了很多

    > Permanent Generation: 永久代是保存类文件的地方。这些是类编译后的结果和 JSP 页面。如果这个空间已满，它将触发一个 Full GC 垃圾收集。如果 Full GC 不能清除旧的未引用类，并且又没有空间扩展永久代的话，就会抛出一个 OOM， JVM 就崩溃了
    >
    > 我推测是与 jvm 垃圾回收有关。jvm 虚拟机中将堆分为新生代和老年代，而当 new 了一个对象以后，由于是强引用，这个对象在经历 minorGC 的时候，年龄会变大，在达到参数 MaxTenuringThreshold 值的时候，就会进入到老年代中。一直进行这个过程，那么老年代中的活着的对象就会越来越多，最后老年代满了以后发生 fullGC，而 fullGC 是很耗时间的，尤其是当老年代越大，那么 fullGC 就越耗时间。这个系统周期性出现这个问题的就是由于对象周期性地把老年代填充满了，然后 jvm 虚拟机周期性地去进行 fullGC 去回收垃圾，当回收的时候系统性能就下降，当回收结束时系统性能就上升。
    >
    > 那么如何解决呢？通过调整新生代与老年代的比例（该值可以通过参数 –XX：NewRatio 来指定），调低老年代占的内存大小，这样老年代很快就满了，就会提前进行 fullGC，直到调整到发生 fullGC 时候对于系统性能影响不大的时候（用户察觉不出来），那么调优结束。

    1：建议用 64 位操作系统，内存的利用率更多，吞吐量更大

    2：XMX 和 XMS 设置一样大，MaxPermSize 和 MinPermSize 设置一样大，这样可以减轻伸缩堆大小带来的压力

    3：调试的时候设置一些打印参数，如 - XX:+PrintClassHistogram -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC -Xloggc:log/gc.log，这样可以从 gc.log 里看出一些端倪出来。

    4：系统停顿的时候可能是 GC 的问题也可能是程序的问题，多用 jmap 和 jstack 查看，或者 killall -3 Java，然后查看 Java 控制台日志，能看出很多问题。有一次，网站突然很慢，jstack 一看，原来是自己写的 URLConnection 连接太多没有释放，改一下程序就 OK 了

    5：如果用了缓存，那么年老代应该大一些，缓存的 HashMap 不应该无限制长，建议采用 LRU 算法的 Map 做缓存，LRUMap 的最大长度也要根据实际情况设定。

    6：垃圾回收时 promotion failed 是个很头痛的问题，一般可能是两种原因产生 第一个原因是救助空间不够，救助空间里的对象还不应该被移动到年老代，但年轻代又有很多对象需要放入救助空间； 第二个原因是年老代没有足够的空间接纳来自年轻代的对象；这两种情况都会转向 Full GC，网站停顿时间较长。 第一个原因我的最终解决办法是去掉救助空间，设置 - XX:SurvivorRatio=65536 -XX:MaxTenuringThreshold=0 即可， 第二个原因我的解决办法是设置 CMSInitiatingOccupancyFraction 为某个值（假设 70），这样年老代空间到 70% 时就开始执行 CMS，年老代有足够的空间接纳来自年轻代的对象

    7：采用并发回收时，年轻代小一点，年老代要大，因为老年代用的是并发回收，即使时间长点也不会影响其他程序继续运行，网站不会停顿

    > JVM调优第一层：参数看得懂，-XX:+PrintCommandLineFlags 和jinfo等等用起来， JVM参数出来，能看到如何设置
    >
    > JVM调优第二层：知道有GUI工具可以看，不用看黑乎乎，从大家都知道的Jconsole到各种开源的卖钱的APM，手上得有一两个熟悉的
    >
    > 这两层是入门，大部分5年年资以下的初级面试官基本上处于这个层次
    >
    > JVM调优第三层，懂GC：JVM有大量的问题可以看，最主要的一个我们先看GC，你们生不逢时，多了一个G1 GC，要理解的概念两倍了，不过多少个收集器，如何工作，青年代，老年代的特性，都要看看
    >
    > JVM调优第四层：读head dump和thread dump，看看有什么异常，比如锁之类的
    >
    > 作者：萝魏紫
    > 链接：https://www.zhihu.com/question/268821097/answer/343893846

20. **获取 dump 文件方式**

    在命令行使用 jmap

    使用 JVisualVM 导出

21. **常见堆内存分析工具**

    | 产品功能                                                     | MAT  | JProfiler | Visual VM | jhat | jmap | hprof |
    | ------------------------------------------------------------ | ---- | --------- | --------- | ---- | ---- | ----- |
    | 对象关联分析、深浅堆、GC ROOT、内存泄漏检测、线程分析、提供自定义程序扩展扩展 | Y    | N         | N         | N    | N    | N     |
    | 离线全局分析                                                 | Y    | N         | Y         | Y    | N    | N     |
    | 内存实时分配情况                                             | N    | Y         | Y         | Y    | Y    | Y     |
    | OQL                                                          | Y    | N         | Y         | N    | N    | N     |
    | 内存分配堆栈、热点比例                                       | N    | Y         | N         | N    | N    | N     |
    | 堆外内存分析                                                 | N    | N         | N         | N    | N    | N     |

    

### JUC 多线程

多线程模板的企业级口诀：高并发下前提一定是高内聚、低耦合

上联：线程操作资源类

下联：判断干活唤醒通知

横批：严防多线程下的虚假唤醒（使用 while）

> 并发编程高级面试解析
>
> 一、synchronized 相关问题
>
> 1. synchronized 用过吗，其原理是什么
> 2. 你刚才提到获取对象的锁，这个锁到底是什么？如何确定对象的锁
> 3. 什么是可重入性，为什么说 synchronized 是可重入锁？
> 4. JVM 对 Java 的原生锁做了哪些优化？
> 5. 为什么说 synchronized 是非公平锁？
> 6. 什么是锁消除和锁粗化？
> 7. 为什么说 synchronized 是一个悲观锁？乐观锁的实现原理又是什么？什么是 CAS，它有
> 8. 乐观锁一定就是好的吗？
>
> 二、可重入锁 ReentrantLock 及其他显式锁相关问题
> 1. 跟 synchronized 相比，可重入锁 ReentrantLock 其实现原理有什么不同？
> 2. 那么请谈谈 AQS 框架是怎么回事儿？
> 3. 请尽可能详尽地对比下 synchronized 和 ReentrantLock 的异同。
> 4. ReentrantLock 是如何实现可重入性的？

[多线程 - 廖雪峰的官方网站](https://www.liaoxuefeng.com/wiki/1252599548343744/1255943750561472)

《Java并发编程的艺术》

[本书简介・深入浅出 Java 多线程](http://concurrent.redspider.group/)

[不可不说的 Java “锁” 事 - 美团技术团队](https://tech.meituan.com/2018/11/15/java-lock.html)

[Java中的多线程你只要看这一篇就够了 - 简书](https://www.jianshu.com/p/40d4c7aebd66)

你来讲讲你了解的 JUC（多线程）：volatile（可见性、禁止指令重排 - 单例、不保证原子性）- CAS - 原子引用

[java.util.concurrent (Java Platform SE 8 )](https://docs.oracle.com/javase/8/docs/api/) 

[java.util.concurrent.atomic (Java Platform SE 8 )](https://docs.oracle.com/javase/8/docs/api/)

[java.util.concurrent.locks (Java Platform SE 8 )](https://docs.oracle.com/javase/8/docs/api/)

Java 中 ReentrantLock 和 synchronized 都是可重入锁，可重入锁的一个优点是可一定程度避免死锁

![Java 主流锁](http://rjnbyrj6g.hn-bkt.clouddn.com/interview/Java%20%E4%B8%BB%E6%B5%81%E9%94%81.png)

1. ~~**为什么要使用多线程（讲讲你了解的多线程）（弱）**~~

   **多任务**

   现代操作系统都可以执行多任务。多任务就是同时运行多个任务，例如：上网用的 Chrome、写代码用的 IDEA，都在后台同时跑着

   而 CPU 执行代码都是一条一条顺序执行的，但即使是单核 CPU，也可以同时运行多个任务。因为操作系统执行多任务实际上就是让 CPU 对多个任务轮流交替执行。例如，让浏览器执行 0.001 秒，让 IDEA 执行 0.001 秒，在人看来，CPU 就是在同时执行多个任务。 即使是多核 CPU，因为通常任务的数量远远多于 CPU 的核数，所以多核下任务也是交替执行的。

   > 顺（走）-单（核）-操（做）-多（核）

   **进程**

   在计算机中，我们把一个任务称为一个进程，Chrome 浏览器是一个进程，IDEA 则是另一个进程

   某些进程内部还需要同时执行多个子任务（也就是线程）。例如，我们在使用 IDEA 时，IDEA 可以一边码代码，一边进行拼写检查。所以进程和线程的关系就是：一个进程可以包含一个或多个线程

   操作系统调度的最小任务单位其实不是进程，而是线程。常用的 Windows、Linux 等操作系统都采用抢占式多任务，如何调度线程完全由操作系统决定，程序自己是不能决定什么时候执行，以及执行多长时间的。

   **进程 vs 线程**

   和多线程相比，多进程的缺点在于：

   - 创建进程比创建线程开销大，尤其是在 Windows 系统上
   - 进程间通信比线程间通信要慢，因为线程间通信就是读写同一个变量，速度很快

   而多进程的优点在于：

   多进程稳定性比多线程高，因为在多进程的情况下，一个进程崩溃不会影响其他进程，互相隔离的。而在多线程的情况下，任何一个线程崩溃会直接导致整个进程崩溃。

   **多线程**

   Java 语言内置了多线程支持：一个 Java 程序实际上是一个 JVM 进程，JVM 进程用一个主线程来执行 `main()` 方法，在 `main()` 方法内部我们又可以启动多个线程。此外，JVM 还有负责垃圾回收的其他工作线程等。

   和单线程相比，多线程编程的特点在于：多线程经常需要读写共享数据，并且需要同步。例如，播放电影时，就必须由一个线程播放视频，另一个线程播放音频，两个线程需要协调运行，否则画面和声音就不同步

   > 为什么使用多线程？
   >
   > 多线程特点：1. 异步（同步就好比是我只对咱们公司情有独钟，一直等到咱们公司给我回复才行；异步是广撒网，谁先回应就去哪）2. 并行，可以实现同时处理多任务，提升效率

   **使用场景**

   - 网络请求分发：客户端发送请求给 Tomcat，Tomcat 内部有一个 servletSocket 来进行监听并进行分发请求
   - 文件导入 / 导出：如果数据量大的话，我们可以每 1w 条数据进行一个分片去导入 / 导出

   > 并发：同一时间服务端能承受的吞吐量。比如 Tomcat 同一时间的并发是 150
   >
   > 并行：当前 CPU 能处理的线程数量，一般一个核跑一个线程，8 核 CPU 同时可以跑 8 个线程。单核 CPU 其实也可以多线程执行，不过算是伪多线程，是利用的时间片切换来执行的
   
2. **Java 实现多线程有哪几种方式？**

      - 继承 Thread（static class MyThread extends Thread ）

        new MyThread ().start ()； 

        生产中不要中 `new Thread()` 方式，这样数据量大的话会导致系统中线程越来越多，CPU 资源占用越来越大，上下文切换也越来越快，性能就急剧下降。可以使用线程池来固定 n（比如 10 个，一般每个核跑一个线程就可以了，8 核就设置 8 个）个线程

      - 实现 Runable

        new Thread (Runnable).start()

      - Callable + FutureTask

        new Thread (FutureTask).start()

        ```java
        public class CallableDemo {
            public static void main(String[] args) throws ExecutionException, InterruptedException {
                // 两个线程：1. main 2. FutureTask
                FutureTask futureTask = new FutureTask<>(new MyCallable());
                FutureTask futureTask1 = new FutureTask<>(new MyCallable());
                new Thread(futureTask, "FutureTask").start();
        //        new Thread(futureTask, "FutureTask1").start(); // 同样的执行应该复用，不应该再计算第二次。所以 Callable 不会打印第二次
                new Thread(futureTask1, "FutureTask").start(); // 用多个 FutureTask 才会打印两次
        //        Integer res2 = (Integer) futureTask.get(); // 代码放在这个位置会阻塞，等待线程执行完 MyCallable 里面的 3s 仍无后才走 res1 打印
        
                Integer res1 = 100;
        //        while (!futureTask.isDone()) { }
                System.out.println("先执行 futureTask，结果为：" + res1);
                // 要求获得 Callable 线程的计算结果，如果没有计算完成就要去抢，会导致阻塞，得把值结算完成
                Integer res2 = (Integer) futureTask.get(); // 代码放在这个位置不会阻塞，可以做到先走上面的代码打印 res1，再走 MyCallable 计算结果并等待线程返回，不会因为 MyCallable 里的执行 3s 任务而造成阻塞等待
                System.out.println(res1 + res2);
            }
            // ================================ print out ================================
            // 先执行 futureTask，结果为：100
            // FutureTask running...
            // FutureTask running...
            // 1100
        
        }
        
        class MyCallable implements Callable {
        
            @Override
            public Integer call() throws Exception {
                System.out.println(Thread.currentThread().getName() + " running...");
                // 暂停一会儿线程
                TimeUnit.SECONDS.sleep(3);
                return 1000;
            }
        }
        ```

   - 使用线程池 Executors.newCachedThreadPool () 代码：

     ```java
     public class HowToCreateThread {
     
       public static void main(String[] args) {
             new MyThread().start();
             new Thread(new MyRunnable(), "Runnable").start();
             new Thread(new FutureTask<>(new MyCallable()), "Callable").start();
     
             ExecutorService service = Executors.newCachedThreadPool();
             service.execute(() -> {
                 System.out.println(Thread.currentThread().getName() + " running...");
             });
             service.shutdown();
     
             new Thread(() -> {
                 System.out.println(Thread.currentThread().getName() + " thread running...");
             }, "RunnableWithLambda").start();
         }
     
         static class MyThread extends Thread {
     
             @Override
             public void run() {
                 System.out.println(Thread.currentThread().getName());
             }
     
         }
     
         static class MyRunnable implements Runnable {
     
             @Override
             public void run() {
                 System.out.println(Thread.currentThread().getName());
             }
         }
     
         static class MyCallable implements Callable<Integer> {
     
             @Override
             public Integer call() throws Exception {
                 System.out.println(Thread.currentThread().getName());
                 return 1;
             }
         }
     }
     ```

3. **JDK 1.0 就已经有了 Runnable 接口了，为什么还会有 Callable 接口呢？**

   | Runnable   | Callable                                                     |
   | ---------- | ------------------------------------------------------------ |
   | 没有返回值 | 有返回值，假如 100 个线程中有 2 个线程执行出错，可以返回是哪两个线程的问题 |
   | 不能抛异常 | 可以抛异常，假如 100 个线程中有 2 个线程执行出错，可以看到不同的异常信息 |

   Callable 配合的有一个 Future 类，通过 Future 可以了解任务执行情况，或者取消任务的执行，还可获取任务执行的结果

4. **如何控制线程执行的顺序（ABC三个线程如何保证顺序执行）**

   方法一：可以用线程类的 join () 方法去保证多线程的顺序性，里面调用的是 Object 的 wait () 方法

   join：让主线程等待，子线程结束以后才能继续执行

   ```java
   public static void main(String[] args) throws InterruptedException { 
       thread1.start()；
       threadl.join()；
       thread2.start()；
       thread2.join()；
       thread3.start()；
   }
   ```

   方法二：ExecutorService executor = Executors.newSingleThreadExecutor()；

   它是创建了只有一个线程的线程池和不限数量的一个队列，如果通过这个方法去执行，它会把线程放到一个队列里面，而这个队列属于 FIFO，这样就达到了一个排队的效果

   ```java
   static ExecutorService executorService = Executors.newSingleThreadExecutor()；
   public static void main(String[] args) throws InterruptedException{
       executorService.submit(threadl)；
       executorService.submit(thread2)；
       executorService.submit(thread3)；
       executorService.shutdown())；
   }
   ```

5. **线程的状态都有哪些？（线程的生命周期）**

   在线程的状态枚举中分类 6 种状态，分别是新建 NEW、可运行 RUNNABLE（就绪、运行中）、阻塞 BLOCKED、等待 WAITTING、定时等待 TIMED_WAITTING、终止 TERMINATED

   NEW — 尚未通过 Thread.start() 启动的新 Thread 实例

   RUNNABLE — 一个正在运行的线程。它被称为可运行的，因为在任何给定时间它都可能正在运行或等待来自线程调度程序的下一个时间段。当您在其上调用 Thread.start() 时，新线程进入 RUNNABLE 状态

   BLOCKED — 如果一个正在运行的线程需要进入一个同步部分，但由于另一个线程持有该部分的监视器而不能这样做，它就会被阻塞 

   WAITING — 如果线程等待另一个线程执行特定操作，则该线程进入此状态。例如，一个线程在调用它持有的监视器上的 Object.wait() 方法或另一个线程上的 Thread.join() 方法时进入此状态

   TIMED_WAITING — 同上，但线程在调用 Thread.sleep()、Object.wait()、Thread.join() 和其他一些方法的定时版本后进入此状态 

   TERMINATED — 线程已完成其 Runnable.run() 方法的执行并终止

   ![线程的生命周期](http://rjnbyrj6g.hn-bkt.clouddn.com/interview/%E7%BA%BF%E7%A8%8B%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F.jpg)

6. **请谈谈你对 volatile 的理解（volatile 关键字的作用 | 请你谈谈 JMM）**

   > 硬盘（MySQL）< 内存（Redis）< CPU
   >
   > 数据一般存储在 MySQL 硬盘上，硬盘的 IO 是纯硬件的，它有它的上线，达到一定阈值后很难有突破，除非硬件上有突破。而现在大数据时代需要快速的存取，那么可以把部分数据不存在 MySQL，转为存储在 Redis 这样的内存里面。
   >
   > 虽然 CPU 的速度远远大于内存，但是 CPU 只管计算不管存储，假如 CPU 计算完了还没传给内存，就会一直在那耗着，这肯定不行，所以中间就需要有一个缓存的东西 —— 一二三级缓存（CPU Cache Memory），来把计算完的东西存放在缓存里

   首先讲到 volatile 需要先了解一个概念就是 JMM

   JMM：JMM（Java 内存模型 Java Memory Model）本身是一种抽象的概念，主要用它来解决并发过程中的可见性、有序性和原子性问题，JVM 进程实际底层去运行的是线程，每个线程创建时 JVM 都会为其创建一个工作内存。而 JMM 中规定所有变量都必须存储在主内存中且所有线程都可访问，如果<font color='red'>线程要对变量进行操作（读取或赋值等）必须在到拷贝到工作内存中进行</font>，对变量操作完之后再将变量写回主内存

   > JMM 关于同步规定（了解）：
   > i. 线程解锁前，必须把共享变量的值刷新回主内存
   > ii. 线程加锁前，必须读取主内存的最新值到自己的工作内存
   > iii. 加锁解锁是同一把锁
   > 主内存是共享内存区域。由于工作内存是线程的私有区域，因此线程之间无法访问对方的工作内存；由于不能直接操作主内存中的变量，线程之间的通讯（传值）必须通过主内存来完成，工作内存存的是共享变量副本

     ![JMM 内存模型](http://rjnbyrj6g.hn-bkt.clouddn.com/interview/JMM%20%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B.jpg)

   **volatile 是 JVM 提供的比 synchronized 轻量级的同步机制，它有三大特性：保证内存的可见性、禁止指令重排序、但不保证原子性。**

   - 可见性

     > 当多个线程操作同一个共享变量时，就有可能触发线程并发问题。使用 volatile 关键字修饰的变量，保证了其在多线程之间的可见性。也就是每次读取到 volatile 修饰的变量时一定是最新的数据
     
     ![JMM 内存可见性](http://rjnbyrj6g.hn-bkt.clouddn.com/interview/JMM%20%E5%86%85%E5%AD%98%E5%8F%AF%E8%A7%81%E6%80%A7.jpg)
     
     比如当前是个主内存为 32G 的电脑，我新 new 一个 Goods 商品对象，新 new 的对象呢是在堆中，堆又是在内存中。Goods 对象它有个库存属性 num = 10，当有多个线程（T1、T2、T3）来操作同一个共享变量 num 时，就有可能触发线程并发问题。~~（由于 JMM 规定不允许直接修改主内存的值，而是要把主内存值拷回到各自线程的工作内存进行操作也就是共享变量副本）~~，此时线程的值都为 10，当 T1 线程把 num 值修改为 9，接着修改完之后把数据写回主内存。此时线程之间的数据是不可见的，那就需要一种机制就是当线程修改了数据写回主内存，就要立马通知到其他线程来获取最新数据。这种机制就叫内存的可见性。那解决方案呢就是使用 volatile 关键字修饰修饰变量，来保证了其在多线程之间的可见性。也就是每次线程来读取到 volatile 修饰的变量时一定是最新的数据

     [**如何解决可见性问题代码**](#visualAndAtomicResolve)：[interview/VolatileDemo.java at master · liuilin/interview](https://github.com/liuilin/interview/blob/master/interview_code/src/com/liuilin/JUC/VolatileDemo.java)

   - 禁止指令重排序

     这也就是对应着 JMM 的第二个特性 —— 有序性。比如高考试卷发下来一般是先把会做的做了，不按照题的顺序做，这就是重排序。但是用了 volatile 来禁止指令重排之后，相当于强制要求必须按题的顺序做，不可以跳着做

     > 计算机在执行程序时，为了提高性能，编译器和处理器一般会做<font color='red'>指令重排</font>，那分为三点：
     > i. 单线程环境下要确保程序最终执行结果和代码顺序执行的结果一致
     >
     > ii. 处理器在进行重排序时必须要考虑指令之间的<font color='red'>数据依赖性</font>（比如 a = a + 5，要先有 int a = 0 才行）
     >
     > iii. 多线程环境中线程交替执行，由于编译器优化重排的存在，两个线程使用的变量无法保持一致性，以至于结果也会不同

     **你在那些地方用到过 volatile？**

     ==实际应用 —— [单例设计模式](#singleton)==

     单例：保证一个类在内存中只有一个实例，同时设置构造方法为私有，防止别的对象来 new

     ```java
     /**
      * 懒汉模式
      */
     public class LazySingleton {
         private static LazySingleton instance = null;
         private LazySingleton() {}
         public static LazySingleton getInstance() {
             if (instance == null) {
                 instance = new LazySingleton();
             }
             return instance;
         }
     }
     ```

     上面版本在单线程下完全没问题，但在多线程情况下就会出现线程安全问题，也就是两个线程同时判断 `instance == null` 时都判断成功，往下走就同时 new 了对象。那怎么解决呢，能想到的最简单的方法就是加锁， 当然我们把 synchronized 放到静态代码方法上保证单例，但这肯定是不推荐的，因为它太重了。锁住整个方法的话，里面有很多不需要的东西也锁住了。虽然保证了数据一致性，但是并发性和系统性能就下降了。那么就需要更改把 synchronized 放到同步代码块中，同时在加锁前后都做一次 if 判断，确认对象完全为 null 了才创建实例对象，这也就是 <font color='red'>DCL 双重校验锁</font>（Double-checked locking）

     ```java
     private static volatile LazySingleton instance = null;
     // ...
     public static LazySingleton getInstance() {
         if (instance == null) { // 这样效率更高，不然每个线程都要走一遍同步锁，先判断为空就返回
             synchronized (LazySingleton.class) {
                 if (instance == null) { // 保证线程安全，只 new 一个对象
                   instance = new LazySingleton();
                 }
             }
         }
         return instance;
     }
     ```

     **禁止指令重排 | 加 synchronized 保证线程安全了，为什么还要加 volatile？**

     看似没啥问题了，但由于指令重排的存在，程序还是存在线程安全问题，因为实际上 `new LazySingleton();` 操作底层字节码有有三步操作：

     i. new：分配内存空间（这个对象占多大字节就分配多大，[过程见](#newObject)），此时 int i = 0

     ii. invokespecial：初始化对象（调用默认的 `<init>` 构造方法后 i = 1;）

     iii. astore_1：设置 instance 引用指向刚分配的内存地址（此时才 instance != null）

     ```java
     // 简写为 T，用来举例
     class T {
         int i = 1;
     }
     T t = new T();
     // 字节码...
     // 创建 Singleton 对象实例，分配内存
     0 new #2 <com/liuilin/JVM/T>
     //3 dup
     // 调用默认的 <init> 构造方法初始化对象
     4 invokespecial #3 <com/liuilin/JVM/T.<init> : ()V>	
     // 存入局部方法变量表
     7 astore_1
     //8 return
     ```

     由于步骤 2 和步骤 3 不存在数据依赖性，所以会发生指令重排序。（数据依赖性就是指比如 a = a + 5，那你得现有 int a = 0 才行），单线程下指令重排不会影响结果，但是多线程情况下就不一样了，指令排序之后可能先执行步骤 3，再执行步骤 2，先执行步骤 3 来设置 instance 的指向刚分配的内存地址。接着另一个线程进来判断 instance 不为 null，就直接返回了，但返回的是一个未初始化对象的实例，因为步骤 2 的初始化对象步骤没有走，此时会数据出错，也就是出现了线程安全问题。

     **解决方案呢就是使用 volatile 来修饰 instance 实例**

     实现细节：JVM 使用了屏障来禁止屏障两边的指令重排，JVM 的 happens-before 原则规定了 8 种情况下都得加屏障，不允许重排序，其中就有 volatile   

     > as if serial：不管如何重排序，单线程执行的结果不会改变
     > 
     > 用于理解指令重排案例：

     ```java
     public class Resource {
     
     int a = 0;
     boolean flag = false;
     
     public static void main(String[] args) {
       Resource resource = new Resource();
       new Thread(() -> {
           resource.method1();
      }, "T1").start();
     
      new Thread(() -> {
         resource.method2();
       }, "T2").start();
     }
     
      public void method1() {
       flag = true;        // 语句 2
          a = 1;              // 语句 1
      }
     
      // 多线程环境中线程交替执行，由于编译器优化重排的存在，两个线程使用的变量（flag 和 a）无法保持一致，导致结果也不同
      public void method2() {
               if (flag) {
              a = a + 5;      // 语句 3
            System.out.println(Thread.currentThread().getName() + " --- return value --- " + a);
          }
      }
     }
     ```

     > 指令重排只是有一定概率发生，并不是每次都发生
     > 
     > 情况一：不会发生指令重排，T1 线程走完 method1，T2 线程走完 method2，a = 1 + 5 = 6
       > 
       > 情况二：会发生指令重排，由于 `语句 1`  `语句 2` 没有数据依赖性 ，所以可能发生指令重排，T1 线程会先执行 `flag = true`  ，然后 T1 线程被挂起，此时 a = 0，T2 线程走完 method2 并打印输出 a = 0 + 5 = 5
       >  
       >  情况三：和情况二一样还是会发生指令重排，在 T2 线程走完 `语句3` a = 0 + 5 = 5 还没打印时，T2 线程就被挂起了，接着 T1 线程执行 a = 1，此时最终打印输出的值就为 1

   - 不保证原子性：

     what：原子性是不可分割且保证完整性，也就是某个线程正在做某个具体业务时，中间不可以被加塞或分割。要么同时成功，要么同时失败。

     ![volatile 不保证原子性](http://rjnbyrj6g.hn-bkt.clouddn.com/interview/volatile%20%E4%B8%8D%E4%BF%9D%E8%AF%81%E5%8E%9F%E5%AD%90%E6%80%A7.jpg)

     在操作 num++ 原子操作时，首先线程会把主内存对象数据拷贝到各自线程的工作内存，T1 线程进行 +1 操作然后写回主内存，修改完后正要通知其他线程的时候被挂起了。此时 T2 线程也进行 num++ 操作把数据 11 写回主内存了，这时之前的 num = 11 就被覆盖。因此本来三个线程应该累加到 13 的，由于上面数据被覆盖了，导致无法完成原子操作，最终数据就变为 11。

     [**如何解决原子性问题代码实现：**](#visualAndAtomicResolve)[interview/VolatileDemo.java at master · liuilin/interview](https://github.com/liuilin/interview/blob/master/interview_code/src/com/liuilin/JUC/VolatileDemo.java)

     还是可以加 synchronized 可以解决，但是它太重了。那除此外呢我们还可以用 [java.util.concurrent.atomic (Java Platform SE 8 )](https://docs.oracle.com/javase/8/docs/api/) 包下的 AtomicInteger 来保证原子性（底层是用的 CAS）

     > 由于 num++ 是不可分割的操作，对应的字节码解读出来 num++ 做了三个步骤：
     
     ```java
     public void addPlusPlus();
       descriptor: ()V
       flags: (0x0001) ACC_PUBLIC
       Code:
         stack=3, locals=1, args_size=1
            0: aload_0							// 从局部变量 0 中装载引用类型值
            1: dup								// 复制栈顶部一个字长内容
            // getfield：获得初始值
            2: getfield      #2                  // Field num:I
            // iconst_1 + iadd：执行 +1
            5: iconst_1
            6: iadd
            // putfield：把累加后的值写回主内存
            7: putfield      #2                  // Field num:I
           10: return
         LineNumberTable:
           line 12: 0
         line 13: 10
         LocalVariableTable:
         Start  Length  Slot  Name   Signature
               0      11     0  this   Lcom/liuilin/JUC/GoodsByte;
     ```
     
     <span id='visualAndAtomicResolve'>**解决可见性和原子性代码实现**</span>
     
     ```java
     /**
      * 1. 验证 volatile 可见性
      *  1.1 假如 int number = 0; number 变量之前根本没有添加 volatile 关键字修饰，没有可见性
      *  1.2 加入 volatile 后可以解决可见性问题
      *
      * 2. 验证 volatile 不保证原子性
      *  2.1 原子性指的是什么意思？
      *  不可分割，完整性，也即某个线程正在做某个具体业务时，中间不可以被加塞或分割。需要整体完整。要么同时成功，要么同时失败。
      *  2.2 volatile 不保证原子性案例
      *  2.3 why
      *  2.4 如何解决原子性
      *      - 加 synchronized
      *      - 使用 JUC 下面的 AtomicInteger
      *
      * @author liuqiang
      * @since 2021-07-29
      */
     public class VolatileDemo {
         public static void main(String[] args) {
             // visualByVolatile();
             Goods goods = new Goods();
             for (int i = 1; i <= 20; i++) {
                 new Thread(() -> {
                     for (int j = 1; j <= 1000; j++) {
                         // 实际值应该是 20000 才对，但是由于不保证原子性，最终值不到 20000
                         goods.addPlusPlus(); 
                         // goods.addMyAtomic();
                     }
                 }, String.valueOf(i)).start();
             }
             // 需要等待前面的所有线程都计算完成后再走后面的 main 线程（为什么是大于 2？因为后台默认有两个线程：1. main 线程；2 GC 线程）
             while (Thread.activeCount() > 2) {
                 Thread.yield(); // 挂起不执行此线程
             }
             System.out.println(Thread.currentThread().getName() + " --- int final num value --
             System.out.println(Thread.currentThread().getName() + " --- AtomicInteger final nu
         }
         // volatile 可以保证可见性，及时通知其它线程，主物理内存的值已经被修改。
         private static void visualByVolatile() {
             Goods goods = new Goods();
             new Thread(() -> {
                 System.out.println(Thread.currentThread().getName() + " --- come in");
                 // 执行业务逻辑
                 try { TimeUnit.SECONDS.sleep(3); } catch (InterruptedException e) { e.printSta
                 goods.buyGoods();
                 System.out.println(Thread.currentThread().getName() + " --- update");
             }, "T1").start();
             // main 线程
             while (goods.num == 10) {
                 // main 现在一直会在这里循环等待，直到商品库存不等于 10 才跳出循环
             }
             System.out.println(Thread.currentThread().getName() + " --- mission complete.");
         }
     }
     class Goods {
         // int num = 10;
         volatile int num = 10; // 用 volatile 保证数据可见性，没加 volatile 会导致 main 线程一直等待不结束
         AtomicInteger atomicInteger = new AtomicInteger();
         public void buyGoods() {
             this.num = this.num - 1;
         }
         // 注意此时 number 前面是加了 volatile 关键字修饰的，所以说 volatile 不保证原子性
         public void addPlusPlus() {
             num++;
         }
         public void addMyAtomic() {
             atomicInteger.getAndIncrement();
         }
     }
     ```

7. **讲一讲 Atomiclnteger，为什么要用 CAS 而不是 synchronized？（CAS知道吗？如何实现的？CAS有什么缺陷，该如何解决）**

   CAS ---> Unsafe ---> CAS 底层思想---> CAS 缺点（ABA问题） ---> 原子引用更新 ---> 如何规避 ABA 问题

   > 助记：CU底缺原

   ```java
   public static void main(String[] args) {
           AtomicInteger atomicInteger = new AtomicInteger(10);
           System.out.println(atomicInteger.compareAndSet(10, 9) + " current value: " + atomicInteger.get());
           System.out.println(atomicInteger.compareAndSet(10, 8) + " current value: " + atomicInteger.get());
       }
   // true current value: 9
   // false current value: 9
   ```

   CAS（全称为：Compare And Swap 比较并交换）：是一种无锁算法，在不使用锁（没有线程被阻塞）的情况下实现多线程之间的变量同步。最底层是 <font color='red'> 一条 CPU 并发原语 </font> ，作用是把工作内存的真实值和线程的期望值进行比较，如果相同就把最新值更新到工作内存中，否则就要重新获得主物理内存的最新值，不停的自旋直到成功为止。并发原语的指令执行是原子性的，执行过程不可被中断，也就保证了线程安全

   > <font color='red'>无锁算法 -> 多线程之间变量同步 -> 并发原语 -> 内存值和期望值 -> 原子的</font>：无多并内原

   ![CAS 解决 volatile 的原子性问题](http://rjnbyrj6g.hn-bkt.clouddn.com/interview/CAS%20%E8%A7%A3%E5%86%B3%20volatile%20%E7%9A%84%E5%8E%9F%E5%AD%90%E6%80%A7%E9%97%AE%E9%A2%98.jpg)

   既然 volatile 无法保证原子性，那这回商品对象 Goods 的 num 值用 new AtomicInteger (10) 对象包裹起来，T1 线程想要修改主内存的值时，此时会把物理内存的真实值 10 和 T1 线程的期望值 10 进行比较，发现相同就替换，调用方法 getAndIncrement () 来进行 +1 操作来替换主内存值。接着 T2 线程过来发现主物理内存的真实值 11 和线程的期望值 10 不相同，结果就修改失败，然后会重新读取主内存的真实值到 T2 线程的工作内存，再次去比较并替换，直到成功为止

   > 并发原语：一般是指由若干条指令组成的程序段，用来实现某个特定功能，在执行过程中不可被中断。一旦开始执行，就不能被中断，否则就会出现操作错误，造成系统混乱。它不会造成数据不一致问题（也就说明它是线程安全的）

   **CAS 底层原理是什么（代码层面）？如果知道，谈谈你对 Unsafe 的理解**

   CAS 底层调用的是 Unsafe 类：用 OpenJDK8 来查看 Unsafe 类的源码时，里面主要有两个方法来构成了 CAS 操作，一个是 `getIntVolatile()` ，另一个是 `compareAndSwapInt()` 

   ```java
   // AtomicInteger 源码
   public class AtomicInteger extends Number implements java.io.Serializable 
       private static final long serialVersionUID = 6214790243416807050L;
       // setup to use Unsafe.compareAndSwapInt for updates
       private static final Unsafe unsafe = Unsafe.getUnsafe();
       private static final long valueOffset;
       static {
           try {
               valueOffset = unsafe.objectFieldOffset
                   (AtomicInteger.class.getDeclaredField("value"));
           } catch (Exception ex) { throw new Error(ex); }
       }
       private volatile int value;
   ```

   ```java
   // ------------------------- JDK 8 -------------------------
   // AtomicInteger 自增方法
   public final int getAndIncrement() {
     return unsafe.getAndAddInt(this, valueOffset, 1);
   }
   
   // Unsafe.class
   // var1：当前对象（AtomicInteger 对象）；var2：当前对象内存地址的值（偏移地址）；var5：期望值；var4：需要增加的数
   public final int getAndAddInt(Object var1, long var2, int var4) {
     int var5;
     do {
         // 获得当前对象的内存地址的值（相当于把主物理内存值拷贝到工作内存）
         var5 = this.getIntVolatile(var1, var2);
         // 如果当前对象 o 的 offset（内存地址的值）与
     } while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4));
     return var5;
   }
   
   // ------------------------- OpenJDK 8 -------------------------
   // Unsafe.java
   public final int getAndAddInt(Object o, long offset, int delta) {
      int v;
      do {
          // 期望值：获得当前对象的内存地址的值（把主物理内存的真实值拷贝到工作内存）
          v = getIntVolatile(o, offset);
          // 如果当前对象 o 的 offset（内存地址的值）与工作内存的期望值 v 相等，那么就替换主内存的值为 v + 需要变动的值 delta（v + 1）
      } while (!compareAndSwapInt(o, offset, v, v + delta));
      return v;
   }
   
   public final native boolean compareAndSwapInt(Object o, long offset, int expected, int x); // 底层是 C 或 C++ 的代码
   ```

   根据 OpenJDK 8 的源码我们可以看出，用 `getIntVolatile()` 获得当前对象的内存地址值（相当于把主物理内存值拷贝到本地线程的工作内存），然后 `compareAndSwapInt()` 判断如果 `内存值` 与工作内存的 `期望值 v` 相等，那么就替换主内存的值为线程的最新值（ `v + 需要变动的值 delta（v + 1）`），否则返回 false，继续循环进行 CAS，直到设置成功为止才退出循环并且将旧的期望值返回。<font color='red'>CAS 最最底层是 JDK 通过调用汇编指令 lock cmpxchg（compare and exchange）</font>，cmpxchg 它也不是原子的，也可能被别的线程打断，需要加 lock 锁来锁住内存总线来实现原子性

   > 整个 “比较并更新” 操作封装在 compareAndSwapInt () 中，在 JNI（Java Native Interface） 里是借助于一个 CPU 指令完成的，属于原子操作，可以保证多个线程都能够看到同一个变量的最新值。

   **为什么要用 CAS 而不是 synchronized？**

   因为 synchronized 加锁之后，同一时间段只能有一个线程来访问，一致性得到了保障，但并发性下降（因为锁太重，锁住了整个方法或代码块）；而 CAS 是无锁（不需要向内核申请锁，自己在用户态就解决了），不断的对内存值进行比较并替换，这样就既保证了数据一致性，又保证了并发性。

   **CAS 缺点：**CPU - 共享变量 - ABA 问题

   1. **循环时间长且 CPU 开销大：**CAS 操作不成功会导致一直自旋，里面有个 do while 循环，会给 CPU 带来非常大的开销。

   2. **只能保证一个共享变量的原子操作：** 操作多个共享变量时就得加锁来保证原子性

      不过好在 Java 从 1.5 开始 JDK 提供了 AtomicReference 类来保证引用对象之间的原子性，可以把多个共享变量放在一个对象里来进行 CAS 操作。

   3. **ABA 问题：**首先是先把数据拷贝到 T1、T2 线程的工作内存，CAS 在操作值的时候会去检查内存值是否发生变化，没有发生变化才会更新内存值。比如内存值原来是 A，后来变为了 B，接着又变为了 A，然后 T1 线程挂起，T2 线程进行 CAS 比较时发现内存值没有发生变化，然后改为最新值 C。但是实际上内存值是有变化的，这就是 CAS 的 ABA 问题。

      ![ABA 问题](http://rjnbyrj6g.hn-bkt.clouddn.com/interview/ABA%20%E9%97%AE%E9%A2%98.jpg)

      ABA 问题解决思路：

      > JDK 除了提供原子整型 AtomicInteger 之外，还提供了可自定义泛型的原子引用类 AtomicReference<V>

      - 在变量前面添加版本号，每次变量更新的时候都把版本号 +1，这样变化过程就从 “A－B－A” 变成了 “1A－2B－3A”。
      - JDK 从 1.5 开始提供了 AtomicStampedReference 类通过增加版本戳的方式来解决 ABA 问题
      
      ```java
      public class ABA {
      
          static AtomicReference<Integer> atomicReference = new AtomicReference<>(10);
          private static AtomicStampedReference<Integer> atomicStampedReference = new AtomicStampedReference<>(10, 1);
      
          public static void main(String[] args) {
              System.out.println("==================== ABA 问题产生 ===================");
              new Thread(() -> {
                  atomicReference.compareAndSet(10, 11);
                  atomicReference.compareAndSet(11, 10);
              }, "T1").start();
      
              new Thread(() -> {
                  // 确保上面的 T1 线程先完成 ABA 问题
                  try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); }
                  System.out.println(atomicReference.compareAndSet(10, 100) + " --- " + atomicReference.get());
              }, "T2").start();
      
              // 确保上面线程已执行完毕
              try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); }
              System.out.println("=================== ABA 问题解决 ===================");
              new Thread(() -> {
                  int stamp = atomicStampedReference.getStamp();
                  System.out.println(Thread.currentThread().getName() + " --- " + "第 1 次版本号：" + stamp);
                  //暂停一会儿线程
                  try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); }
                  atomicStampedReference.compareAndSet(10, 11, atomicStampedReference.getStamp(), atomicStampedReference.getStamp() + 1);
                  System.out.println(Thread.currentThread().getName() + " --- " + "第 2 次版本号：" + atomicStampedReference.getStamp());
                  atomicStampedReference.compareAndSet(11, 10, atomicStampedReference.getStamp(), atomicStampedReference.getStamp() + 1);
              }, "T3").start();
      
              new Thread(() -> {
                  int stamp = atomicStampedReference.getStamp();
                  System.out.println(Thread.currentThread().getName() + " --- " + "第 1 次版本号：" + stamp);
                  // 确保上面的 T3 线程先完成 ABA 问题
                  try { TimeUnit.SECONDS.sleep(3); } catch (InterruptedException e) { e.printStackTrace(); }
                  boolean res = atomicStampedReference.compareAndSet(10, 100, stamp, stamp + 1);
                  System.out.println(Thread.currentThread().getName() + " --- " + res + "，当前版本号：" + atomicStampedReference.getStamp());
                  System.out.println(Thread.currentThread().getName() + " --- " + "最新值" + atomicStampedReference.getReference());
              }, "T4").start();
          }
          // ============================= ABA 问题产生 =============================
          // true --- 100
          // ============================= ABA 问题解决 =============================
          // T3 --- 第 1 次版本号：1
          // T4 --- 第 1 次版本号：1
          // T3 --- 第 2 次版本号：2
          // T4 --- false，当前版本号：3
          // T4 --- 最新值10
      }
      ```
      
      CAS 它其实是一种乐观锁的实现

8. **悲观锁 VS 乐观锁，请讲一下它们的优缺点**

   悲观锁与乐观锁是一种广义上的理念。在 Java 和数据库中都有对应的实际落地。

   what：

   - 对于同一个数据进行并发操作时，悲观锁认为它在操作数据时一定有别的线程来修改数据，因此它会先加锁来确保数据安全。Java 中 synchronized 关键字和 Lock 的实现类都是悲观锁

   - 而乐观锁认为它在操作数据时不会有别的线程修改数据，所以不会添加锁，只是在更新数据的时候去判断之前有没有其它线程修改了数据。如果数据没有被修改，那么线程会将最新的数据写入主内存。如果数据已经被其他线程修改了就会自旋去获取最新值再进行操作

     > 乐观锁在 Java 中是通过使用无锁编程来实现，最常采用的是 CAS 算法，Java 原子引用类中的递增操作就通过 CAS 自旋实现的。

   why：

   - 悲观锁适合写操作多的场景，像数据库这样需要保证数据一致性的，在写操作时加锁来保证数据一致性

     > 传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁

   - 乐观锁适合读操作多的场景，来提高系统的并发性能

     > 在数据库中可以定义一个 version 字段，在操作之前先获取当前数据的 version，在执行将获得的 version 当作参数传递到数据库，提交时判断当前的 version 和传入的 version 是否一致，如果不一致则认为别人已经操作过当前 version，就不允许提交

   ![悲观锁 VS 乐观锁](http://rjnbyrj6g.hn-bkt.clouddn.com/interview/%E6%82%B2%E8%A7%82%E9%94%81%20VS%20%E4%B9%90%E8%A7%82%E9%94%81.png)

   how：

   光说概念有些抽象，我们来看下乐观锁和悲观锁的调用方式示例：

   ```java
   // ------------------------- 悲观锁的调用方式 -------------------------
   // synchronized
   public synchronized void testMethod() {
   	// 操作同步资源
   }
   // ReentrantLock
   private ReentrantLock lock = new ReentrantLock(); // 需要保证多个线程使用的是同一个锁
   public void modifyPublicResources() {
   	lock.lock();
   	// 操作同步资源
   	lock.unlock();
   }
   
   // ------------------------- 乐观锁的调用方式 -------------------------
   private AtomicInteger atomicInteger = new AtomicInteger();  // 需要保证多个线程使用的是同一个AtomicInteger
   atomicInteger.incrementAndGet(); //执行自增1
   ```

9. **谈谈你对 synchronized 关键字的理解（synchronized关键字的用法，优缺点 | synchronized 和 Lock 的区别）**

   [【Java面试】用架构师思维带你理解，关于Synchronized锁升级的原理_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1FZ4y197qE/?spm_id_from=333.788)

   [只有马士兵老师能把多线程与高并发最底层的 synchronized 原理和 Volatile 关键字的字节码原语讲的这么透彻_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1tz411q7c2?p=1)

   [synchronized 你以为你真的懂？ - 知乎](https://zhuanlan.zhihu.com/p/127884116)

   [偏向锁 - 知乎](https://zhuanlan.zhihu.com/p/26475023)

   ==synchronized 加锁，同一时间段只有一个线程来访问，其它线程会被 block 阻塞住，线程访问完之后才到下一个线程访问。它的一致性得到了保障但并发性、系统性能下降==

   在 JDK 早期的实现是重量级锁，底层是依赖于操作系统的 Mutex Lock（互斥锁）。但如果同步代码块里面的代码非常简单，那 synchronized 去申请锁的时间就会比代码执行的时间还要长（比如+1操作），这就是为什么 JDK 6 以前 synchronized 效率低的原因。由于后来 Java 越来越多的开始处理高并发程序。于是从 1.6 开始 JDK 为了减少申请锁和释放锁带来的性能消耗就引入了 “偏向锁” 和 “轻量级锁”，这才有了锁升级的概念

   > 助记：M（没）重 M（没） 简 高 减
   > 拿 HotSpot 来说，HotSpot 的对象头主要包括两部分数据：Mark Word（标记字段）、Klass Pointer（类型指针）。运行期间 Mark Word 里存储的数据会随着锁标志位的变化而变化。Klass Pointer 指向了当前类对象（虚拟机通过这个指针来确定这个对象是哪个类的实例）

   ![synchronized 锁升级](http://rjnbyrj6g.hn-bkt.clouddn.com/interview/synchronized%20%E9%94%81%E5%8D%87%E7%BA%A7.jpg)

   新 new 的对象，如果偏向锁未启动会是 `普通对象` ，已启动的话是 `匿名偏向` ，因为还没有线程获得锁，无法偏向哪个线程，所以是匿名偏向，加了锁之后呢是偏向锁。当第一个线程去访问某把锁，会先在这个对象头上面的 mark word 上记录当前线程的 ID。实际上是没有给该对象加锁的（此时是偏向锁）

   **无锁：**有多个线程修改同一个值，必定会有一个线程能修改成功，而其他修改失败的线程会不断重试直到修改成功

   **偏向锁（实际也没有锁）：**偏向锁是指一段同步代码一直被一个线程所访问，那么该线程会自动获取锁，就不用去找操作系统申请锁。

   > 偏向锁默认是开启的，但是会 4s 延迟启动，因为 JVM 在启动的时候内部有很多东西需要上锁（明确有很多线程），所以直接上轻量级锁或重量级锁了
   >
   > 有点类似于我想在厨房门口贴着 ”正在使用“ 。多数情况下我们不需要把门锁住，这样不用去操作系统那里申请锁效率会更高。

   为什么会有偏向锁？

   像 StringBuffer 中的一些同步方法，大部分时间只有自己一个线程在做 append() 操作，就不需要去申请锁

   > 新 new 出来的对象，偏向锁未启动时（延时 4s）是普通对象（无锁状态 001）
   >
   > 而偏向锁启动后（延时 4s 之后）是匿名偏向（偏向锁 101）
   >
   > 怎么实现效果，sleep(5)  线程睡 5 s，或者把 JVM 参数 `-XX:BiasedLockingStartupDelay=0` 偏向锁启动延时设置为 0

   **轻量级锁：**此时又来了两个线程（室友），发现上面有贴牌，肯定不让你一个人用，于是就开始采用 CAS 自旋锁操作，三个线程一起争抢。当然偏向锁会偏向于刚开始的 T1 线程，它还是会优先的执行，其他两个线程会用 CAS 自旋来读取最新值，看现在还是不是你 T1 线程在占用，如果 T1 线程用完了，就会把贴牌撕下来，剩下的两个线程先把贴牌贴上谁就占用（也就是谁先把自己的线程 ID 贴到 mark word 上谁就得到了这把锁），其他线程又继续自旋。

   > 偏向锁 -> 轻量级锁：只有有线程来竞争就会升级为轻量级锁（CAS 不需要向内核申请大锁）

   **重量级锁：**

   为什么已经有了轻量级锁了之后还需要重量级锁呢？

   如果此时有大量的线程。在轻量级锁时一个线程在做操作，其他所有线程都在做 CAS 自旋特别消耗 CPU 资源，假如有 1000 个线程在自旋等待的话情况更糟糕。此时就会升级为重量级锁，重量级锁相当于一个等待队列（waitset），它会把其他所有的线程全部放到等待队列，然后由操作系统调度

   轻量级锁 -> 重量级锁：1.6 以前 while 循环自旋 10 次或者有多个线程在等待，线程数超过 CPU 核数的 1/2。线程就会升级为重量级锁。因为自旋太消耗 CPU 了，加入等待队列后不消耗 CPU。JVM 调优呢是可以调整自旋次数以及线程数的，不过 1.6 开始已经不用管了，JVM 做了自适应自旋，自己来计算并升级为重量级锁

   偏向锁 -> 重量级锁：还有一种情况是偏向锁里的线程有特别耗时的操作或者调用 wait () ，就会造成 `重度竞争` ，直接升级为重量级锁

   

   synchronized 的底层字节码实现：会发现有一个进入的操作 `monitorenter`，和两个出去的操作 `monitorexit` 。上锁解锁的意思，那为什么一个会 `monitorenter` 会对应两个 `monitorexit` 呢？是因为第一个是正常退出，第二个 `monitorexit` 为了做异常退出，确保一定要退出

   ```shell
   0 new #2 <java/lang/Object>
   3 dup
   4 invokespecial #1 <java/lang/Object.<init> : ()V>
   7 astore_1
   8 aload_1
   9 dup
   10 astore_2
   11 monitorenter
   12 getstatic #3 <java/lang/System.out : Ljava/io/PrintStream;>
   15 aload_1
   16 invokestatic #4 <org/openjdk/jol/info/ClassLayout.parseInstance : (Ljava/lang/Object;)Lorg/openjdk/jol/info/ClassLayout;>
   19 invokevirtual #5 <org/openjdk/jol/info/ClassLayout.toPrintable : ()Ljava/lang/String;>
   22 invokevirtual #6 <java/io/PrintStream.println : (Ljava/lang/String;)V>
   25 aload_2
   26 monitorexit
   27 goto 35 (+8)
   30 astore_3
   31 aload_2
   32 monitorexit
   33 aload_3
   34 athrow
   35 return
   ```

10. **那么请谈谈 AQS 框架是怎么回事儿？ | ReentrantLock 原理**

      [[Deep AQS Principle\] I've drawn 35 diagrams to get you deep into AQS](https://programmer.group/deep-aqs-principle-i-ve-drawn-35-diagrams-to-get-you-deep-into-aqs.html)

      ![AQS](http://rjnbyrj6g.hn-bkt.clouddn.com/interview/AQS.jpg)

      AQS：AbstractQueueSynchronizer 抽象队列同步器，[从 ReentrantLock 的实现看 AQS 的原理及应用 - 美团技术团队](https://tech.meituan.com/2019/12/05/aqs-theory-and-apply.html)

      > 前置知识：公平锁和非公平锁、可重入锁、LockSupport、自旋锁、数据结构之链表、设计模式之模板设计模式（很多抽象类，把父类定义的足够高，具体的子类实现放在子类

      - what（state 变量+CLH 变种的双端队列）：是用来构建锁或者其它同步器组件的重量级基础框架及整个UC体系的基石，通过内置的FIFo队列来完成资源获取线程的排队工作，并通过一个int类型变量表示持有锁的状态（1代表持有锁）

      - why 最重要的基石：ReentrantLock、CountDownLatch、ReentrantReadWritelock、Semaphore、CyclicBarrier都和 AQS 有关

        锁和同步器直接关系

        锁：面向锁的使用者（屏蔽细节，直接调用 API）

        同步器：面向锁的实现者（比如Java并发大神DougLee，提出统一规范并简化了锁的实现，屏蔽了同步状态管理、阻塞线程排队和通知、唤醒机制等。）

      - how 能干嘛：加锁就会阻塞，有阻塞就会导致排队就需要一种队列来进行管理

        这个队列就是 AQS 的抽象表现。它将请求共享资源的线程封装成队列的节点（Node），通过 CAS 自旋以及 `LockSupport.park ()` 的方式，维护 state 变量的状态，使并发达到同步的控制效果

      - 底层源码：l a t a a（助记）

        - lock

          ```java
          final void lock() {
              if (compareAndSetState(0, 1))
                  setExclusiveOwnerThread(Thread.currentThread());
              else
                  acquire(1);
          }
          ```

          首先办理业务窗口 state 为 0，当前线程进行占用

        - acquire()

          第二个线程只能有 `acquire()` 

        - tryAcquire(arg)

          ```java
          protected boolean tryAcquire(int arg) {
              throw new UnsupportedOperationException();
          }
          ```

          代码直接抛异常，没有实现代码。这是典型的模板方法，父类定义的足够高，逼着你在子类进行实现

        - addWaiter(Node.EXCLUSIVE)

          开始加入等待队列

          ```java
          private Node addWaiter(Node mode) {
              Node node = new Node(Thread.currentThread(), mode);
              // Try the fast path of enq; backup to full enq on failure
              Node pred = tail;
              if (pred != null) {
                  node.prev = pred;
                  if (compareAndSetTail(pred, node)) {
                      pred.next = node;
                      return node;
                  }
              }
              enq(node);
              return node;
          }
          ```

          ```java
          private Node enq(final Node node) {
              for (;;) {
                  Node t = tail;
                  if (t == null) { // Must initialize
                      if (compareAndSetHead(new Node()))
                          tail = head;
                  } else {
                      node.prev = t;
                      if (compareAndSetTail(t, node)) {
                          t.next = node;
                          return t;
                      }
                  }
              }
          }
          ```

          T2 节点进来，此时尾指针为 null，进入 `enq()` 方法里会循环的去执行，首次会添加一个空的哨兵节点，然后再把 T2 节点放进去

          T3 节点进来，尾指针不为 null，直接加入到 T2 线程后面

        - acquireQueued(addWaiter(Node.EXCLUSIVE), arg)

          循环的抢占，T2 线程又想去抢占，继续抢占失败后把 CAS 把哨兵节点的 waitState 改为了 -1，此时就把 T2 进行 `park()` 阻塞。T3 线程也同理

          

          

          总结：首先办理业务窗口 state 为 0，当前线程 T1 进行占用。T2 线程进来只能有 `acquire()` ，最终返回 false，取反为 true 后接着往下执行

          非公平锁判断的方式，两个 if 判断，第一个是 T2 线程判断是否空闲时，T1 线程刚好办理完业务离开。第二个判断时 T2 线程判断是当前线程，T1 办理完业务起来又坐下了，就是可重入锁的情况

          `unlock()` 调用 `unpark()` 方法，并把哨兵的 -1 改为 0，state 改为 0，Thread 改为 null，接着 T2 线程去抢占。然后就要处理队列来进行 GC，把 T2 节点的 Thread 设置为 null，把哨兵节点的  前后指针指为 null，这是 T2 节点就变为了新的哨兵节点

      Node 节点 = 里面有 waitStatus 状态 + Thread 对象 + prev、next 前后指针

      ReentrantLock 底层用的 AQS，`lock()`,`unlock()` 底层调用的是内部类 Sync，Sync 类又是继承的 AQS，`new ReentrantLock()` 默认创建的是非公平锁

      JUC 里面的 LockSupport，是线程等待唤醒机制的加强版，里面的 `park()`、`unpark()` 就是去阻塞线程和解除阻塞线程

      

      

      > 三种线程等待唤醒方法：
      >
      > 方式1：使用Object中的wait（方法让线程等待，使用Object中的notify（方法唤醒线程
      >
      > Object.类中的wait、notify、notifyAlL用于线程等待和唤醒的方法，都必须在synchronized内部执行。否则要抛错
      >
      > 线程先执行 notify，另一个线程再执行 wait，会无法被唤醒
      >
      > 方式2：使用Uc包中Condition的await（方法让线程等待，使用signal（方法唤醒线程
      >
      > 和方式1一样
      >
      > 方式3：LockSupport类可以阻塞当前线程以及唤醒指定被阻塞的线程
      >
      > 基于前面两种方式的缺点，LockSupport 是采用发放许可证（permit）的方式（`unpark()` 把 permit 变为 1,`park()` 把 permit 变为 0)，就能解决所有的问题，不需要在锁块中，也不会被顺序限制
      >
      > 多次调用 `uppark()` 不会把 permit 只会变为 1 不会累加，所以再两次调用 `park()` 需要两个通行证，所以会被阻塞

11. **synchronized 和 Lock 的区别**

    |                        | Synchronized                                                 | Lock                                                         |
    | :--------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
    | 原始构成               | synchronized 是关键字属于 JVM 层面的锁，字节码的 monitorenter 指令底层是通过 monitor 对象来实现线程同步的，依赖于操作系统的 mutex lock 互斥锁 | Lock 是接口，是 API 层面的锁，也就是说明它可以被实现，被实现的类还可以被继承，类里面可以有方法等，这就比 synchronized 更灵活 |
    | 使用方法               | 不需要用户手动释放锁，JVM 会自动让线程释放对锁的占用         | ReentrantLock 需要用户手动释放锁，必须要在 finally 代码块中手动 unlock，不然会造成死锁 |
    | 等待是否可被中断       | 不可被中断，除非抛出异常或正常执行完成                       | 1. 设置超时时间 tryLock (long timeout,TimeUnit unit) <br />2. 调用 interrupt () 方法可以中断 |
    | 加锁是否公平           | 默认是非公平锁                                               | ReentrantLock 两者都可以，默认是非公平锁，但构造方法内可以传入 boolean 值，true 为公平锁，false 为非公平锁 |
    | 绑定多个条件 Condition | 没有这个机制                                                 | ReentrantLock 可用来实现分组唤醒需要唤醒的线程，可以做到非常精确，而不是像 synchronized 一样要么随机唤醒一个线程，要么全部唤醒 |

    <span id='lockDiff'>**synchronized 和 ReentrantLock（可重入锁）的区别**</span>

    同：两者都是可重入锁

    | 异： | synchronized                                                 | ReentrantLock                                                |
    | ---- | ------------------------------------------------------------ | ------------------------------------------------------------ |
    | 优点 | synchronized 是 Java 里的关键字，优势在于不需要用户手动的去释放锁，也不会造成死锁，它会在方法或代码块执行完之后，系统自动让线程释放锁 | ReentrantLock 是类，那它就比 synchronized 更灵活，可以被继承、可以有方法。ReentrantLock 可以在 try lock() 中可以给个规定时间，规定时间内拿不到锁就放弃 |
    | 缺点 | synchronized 容易导致线程积压，因为它要等第一个线程锁释放之后才执行下一个线程。比如一段代码里面有个方法要执行 5s，那么其它线程只有等它执行完 | 但它必须得在 finally 代码块中手动 unlock，不然会造成死锁     |

    Lock 优势代码案例 | 分组唤醒（了解）：

       ```java
    package com.liuilin.JUC;
    
    import java.util.concurrent.locks.Condition;
    import java.util.concurrent.locks.Lock;
    import java.util.concurrent.locks.ReentrantLock;
    
    /**
        * 题目：多线程之间按顺序调用，实现 A -> B-> C 三个线程启动，要求如下：
        *  T1 打印 1 次，T2 打印 2 次，T3 打印 3 次
        *  紧接着
        *  T1 打印 1 次，T2 打印 2 次，T3 打印 3 次
        *  来 2 轮
        *
        * @author liuqiang
        * @since 2021-08-03
        */
    public class SyncReentrantLockDemo {
    
        public static void main(String[] args) {
            ShareResource shareResource = new ShareResource();
            new Thread(() -> {
                for (int i = 1; i <= 2; i++) {
                    shareResource.print1();
                }
            }, "T1").start();
            new Thread(() -> {
                for (int i = 1; i <= 2; i++) {
                    shareResource.print2();
                }
            }, "T2").start();
            new Thread(() -> {
                for (int i = 1; i <= 2; i++) {
                    shareResource.print3();
                }
            }, "T3").start();
        }
        // =========================== print out ===========================
        // T1 print1() 打印第 1 次
        // T2 print2() 打印第 1 次
        // T2 print2() 打印第 2 次
        // T3 print3() 打印第 1 次
        // T3 print3() 打印第 2 次
        // T3 print3() 打印第 3 次
        // T1 print1() 打印第 1 次
        // T2 print2() 打印第 1 次
        // T2 print2() 打印第 2 次
        // T3 print3() 打印第 1 次
        // T3 print3() 打印第 2 次
        // T3 print3() 打印第 3 次
    }
    
    class ShareResource {
    
        private int num = 1;
        private Lock lock = new ReentrantLock();
        private Condition c1 = lock.newCondition();
        private Condition c2 = lock.newCondition();
        private Condition c3 = lock.newCondition();
    
        public void print1() {
            lock.lock();
            try {
                // 判断
                while (num != 1) {
                    c1.await();
                }
                // 干活
                for (int i = 1; i <= 1; i++) {
                    System.out.println(Thread.currentThread().getName() + " print1() 打印第 " + i + " 次");
                }
                // 唤醒通知 2 号线程
                num = 2;
                c2.signal();
            } catch (InterruptedException e) {
                e.printStackTrace();
            } finally {
                lock.unlock();
            }
        }
    
        public void print2() {
            lock.lock();
            try {
                // 判断
                while (num != 2) {
                    c2.await();
                }
                // 干活
                for (int i = 1; i <= 2; i++) {
                    System.out.println(Thread.currentThread().getName() + " print2() 打印第 " + i + " 次");
                }
                // 唤醒通知 3 号线程
                num = 3;
                c3.signal();
            } catch (InterruptedException e) {
                e.printStackTrace();
            } finally {
                lock.unlock();
            }
        }
    
        public void print3() {
            lock.lock();
            try {
                // 判断
                while (num != 3) {
                    c3.await();
                }
                // 干活
                for (int i = 1; i <= 3; i++) {
                    System.out.println(Thread.currentThread().getName() + " print3() 打印第 " + i + " 次");
                }
                // 重新唤醒通知 1 号线程
                num = 1;
                c1.signal();
            } catch (InterruptedException e) {
                e.printStackTrace();
            } finally {
                lock.unlock();
            }
        }
    }
       ```

    字节码操作

       ```java
    public static void main(String[] args) {
        synchronized (new Object()) {
        }
        new ReentrantLock();
    }
       ```

       ```shell
    public static void main(java.lang.String[]);
    descriptor: ([Ljava/lang/String;)V
    flags: ACC_PUBLIC, ACC_STATIC
    Code:
    stack=2, locals=3, args_size=1
    0: new           #2                  // class java/lang/Object
    3: dup
    4: invokespecial #1                  // Method java/lang/Object."<init>":()V
    7: dup
    8: astore_1
    9: monitorenter
    10: aload_1
    11: monitorexit
    12: goto          20
    15: astore_2
    16: aload_1
    17: monitorexit
    18: aload_2
    19: athrow
    20: new           #3                  // class java/util/concurrent/locks/ReentrantLock
    23: dup
    24: invokespecial #4                  // Method java/util/concurrent/locks/ReentrantLock."<init>":()V
    27: pop
    28: return
    Exception table:
    from    to  target type
    10    12    15   any
    15    18    15   any
    LineNumberTable:
    line 12: 0
    line 14: 10
    line 15: 20
    line 16: 28
    LocalVariableTable:
    Start  Length  Slot  Name   Signature
    0      29     0  args   [Ljava/lang/String;
    StackMapTable: number_of_entries = 2
    frame_type = 255 /* full_frame */
    offset_delta = 15
    locals = [ class "[Ljava/lang/String;", class java/lang/Object ]
    stack = [ class java/lang/Throwable ]
    frame_type = 250 /* chop */
    offset_delta = 4
       ```

       > 抢不到锁就死等的业务用 synchronized，synchronized：可重入锁、互斥性、可见性

12. **公平锁、非公平锁、可重入锁、递归锁、自旋锁，谈谈你对它们的理解？请手写一个自旋锁。**

    - 公平锁 VS 非公平锁

      公平锁：指多个线程按照申请锁的顺序来获取锁，类似于排队先到先得

      非公平锁：是指多个线程获取锁的顺序并不是按照申请锁的顺序来获取锁，有可能后申请的线程比先申请的线程优先获取锁。那在高并发的情况下，有可能会造成优先级反转或者饥饿现象

      区别：

      公平锁优缺点：公平锁的优点是等待锁的线程不会饿死。缺点是整体吞吐效率相对非公平锁要低，等待队列中除第一个线程以外的所有线程都会阻塞，CPU 唤醒阻塞线程的开销比非公平锁大。

      非公平锁优缺点：非公平锁的优点是可以减少唤起线程的开销，整体吞吐效率高，因为线程有几率不阻塞直接获得锁，CPU 不必唤醒所有线程。缺点是处于等待队列中的线程可能会饿死，等了很久都获得不到锁

      > 并发包中 ReentrantLock 的创建可以指定构造函数的 boolean 类型来得到公平锁或非公平锁，默认不传是非公平锁，synchronized 也是非公平锁

      | 异：   | 公平锁                                                       | 非公平锁                                                     |
      | ------ | :----------------------------------------------------------- | ------------------------------------------------------------ |
      | 是什么 | 指多个线程按照申请锁的顺序来获取锁，类似于队列的 FIFO        | 和公平锁相反，有可能后申请的线程比先申请的线程优先获取锁。那在高并发的情况下，有可能会造成优先级反转或者饥饿现象，也就是前面的线程一直拿不到锁被饿死了 |
      | 优点   | 依次获得锁，等待锁的线程不会饿死                             | CPU 就不用唤醒所有线程，减少了 CPU 唤醒线程的开销，提高了整体吞吐效率，因为线程有几率不阻塞而直接获得锁。 |
      | 缺点   | 整体吞吐效率相对于非公平锁要低，因为等待队列中除第一个线程以外的所有线程都会阻塞，那 CPU 唤醒阻塞线程的开销比非公平锁大 | 一直有线程插队的话，处于等待队列中的线程会一直拿不到锁       |

      > 优先级反转：前面的线程被后面的线程插队了
      >
      > 饥饿现象：前面的线程一直被插队，一直拿不到锁

    - 可重入锁（递归锁）

      可重入锁又名递归锁，指的是同一个线程在外层方法获取锁的时候，再进入该线程的内层方法也会自动获取锁（前提锁对象得是同一个对象或者 class），就不用去操作系统那里申请锁，同时也不会因为之前已经获得锁还没释放而被阻塞。Java 中 ReentrantLock 和 synchronized 都是可重入锁，它的优点是在一定程度上避免了死锁

      ```java
      package com.liuilin.JUC;
      
      import java.util.concurrent.TimeUnit;
      import java.util.concurrent.locks.Lock;
      import java.util.concurrent.locks.ReentrantLock;
      
      /**
       * 可重入锁代码演示
       * @author liuqiang
       * @since 2021-08-02
       */
      public class ReenterLockDemo {
      
          public static void main(String[] args) {
              Phone phone = new Phone();
              System.out.println("================== synchronized ==================");
              new Thread(() -> {
                  phone.sendSMS();
              }, "T1").start();
      
              new Thread(() -> {
                  phone.sendSMS();
              }, "T2").start();
      
              // 暂停一会儿线程
              try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); }
              System.out.println("================== ReentrantLock ==================");
              Thread t3 = new Thread(phone, "T3");
              Thread t4 = new Thread(phone, "T4");
              t3.start();
              t4.start();
          }
          // ================== synchronized ==================
          // T1 --- send message...
          // T1 --- send email
          // T2 --- send message...
          // T2 --- send email
          // ================== ReentrantLock ==================
          // T3 --- invoke get()...
          // T3 --- invoke set()
          // T4 --- invoke get()...
          // T4 --- invoke set()
      }
      
      class Phone implements Runnable {
      
          Lock lock = new ReentrantLock();
      
          public synchronized void sendSMS() {
              System.out.println(Thread.currentThread().getName() + " --- send message...");
              sendEmail();
          }
      
          public synchronized void sendEmail() {
              System.out.println(Thread.currentThread().getName() + " --- send email");
          }
      
          @Override
          public void run() {
              get();
          }
      
          private void get() {
              lock.lock();
              try {
                  System.out.println(Thread.currentThread().getName() + " --- invoke get()...");
                  set();
              } finally {
                  lock.unlock();
              }
          }
      
          private void set() {
              System.out.println(Thread.currentThread().getName() + " --- invoke set()");
          }
      }
      ```
      
    - 手写自旋锁：while 循环的 CAS 操作 

      ```java
      public class SpinLock {
      
          AtomicReference<Thread> atomicReference = new AtomicReference<>();
      
          public static void main(String[] args) {
              SpinLock spinLock = new SpinLock();
              new Thread(() -> {
                  spinLock.lock();
                  // 暂停一会儿线程
                  try { TimeUnit.SECONDS.sleep(5); } catch (InterruptedException e) { e.printStackTrace(); }
                  spinLock.unlock();
              }, "T1").start();
      
              // 暂停一会儿线程
              try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); }
              new Thread(() -> {
                  spinLock.lock();
                  // 暂停一会儿线程
                  try { TimeUnit.SECONDS.sleep(3); } catch (InterruptedException e) { e.printStackTrace(); }
                  spinLock.unlock();
              }, "T2").start();
          }
          // =========================== print out ===========================
          // T1 coming...
          // T2 coming...
          // T1 unlock...
          // T2 unlock...
      
          private void lock() {
              System.out.println(Thread.currentThread().getName() + " coming...");
              while (!atomicReference.compareAndSet(null, Thread.currentThread())) {
              }
          }
      
          private void unlock() {
              System.out.println(Thread.currentThread().getName() + " unlock...");
              atomicReference.compareAndSet(Thread.currentThread(), null);
          }
      }
      ```

    - 独占锁（排它锁 / 写锁）VS 共享锁（读锁）

      用了读写锁之后，既保证了数据一致性，有保证了并发性（当业务需要一个线程写，其他线程共享读）

      - 独占锁（排它锁 / 写锁）：是指该锁一次只能被一个线程所持有。如果一个线程对数据加上排它锁后，那其他线程就不能对它再加任何类型的锁。获得排它锁的线程即能读数据又能修改数据。

        > JDK 中的 synchronized 和 JUC 中 Lock 的实现类就是独占锁（互斥锁）。

      - 共享锁：指该锁可被多个线程所持有。如果线程对数据加上共享锁后，那其他线程只能对这个数据再加共享锁，但不能加排它锁。获得共享锁的线程可以并发的读数据，但不能修改数据。

        像在 ReentrantReadWriteLock 里面，它的写锁是独占锁，读锁是共享锁。由于是读写分离的，独占锁保证写操作的数据一致性，共享锁保证保证了读操作的并发性，最终它比其它锁都高效。

      > 比如我在黑板上写东西，如果只允许一个人进来读写的话，虽然保证了数据一致性，但是并发性急剧下降。应该让一个人进来写操作时同时允许其他人可以在窗户外共享读

      读写锁：写操作是原子 + 独占，过程连续不可被分割和打断

      > 解读：以下代码未加读写锁并执行到 `线程正在写入值：` 时，让线程等待 500ms 是为了模拟 3 个线程并发执行了写值 `线程正在写入值：` 操作，写操作开始挂起等待后，`get()` 方法读操作线程开始抢占读取。这样就导致写操作线程会被打断（ `线程正在写入值：` 和 `线程写入完成` 没有连续的执行完），这样数据一致性没被保证。
      >
      > 需要加锁来保证数据一致性，写操作应该加锁来保证原子性，也就是一个线程要把 `线程正在写入值：` 和 `线程写入完成` 这两部操作执行完。同时读操作时允许多个线程同时来并发读来提高并发性，所以可以看到三个线程并发打印 `线程正在读取` ，挂起 300ms 后继续打印 `线程读取完成，值为：`
      
      ```java
      public class ReadWriteLockDemo {
      
          public static void main(String[] args) {
              Cache cache = new Cache();
              for (int i = 1; i <= 3; i++) {
                  int finalInt = i;
                  new Thread(() -> {
                      cache.put(String.valueOf(finalInt), String.valueOf(finalInt));
                  }, String.valueOf(i)).start();
              }
      
              for (int i = 1; i <= 3; i++) {
                  int finalInt = i;
                  new Thread(() -> {
                      cache.get(String.valueOf(finalInt));
                  }, String.valueOf(i)).start();
              }
          }
          // =========================== before print out ===========================
          // 2 线程正在写入值：2
          // 3 线程正在写入值：3
          // 1 线程正在写入值：1
          // 1 线程正在读取
          // 2 线程正在读取
          // 3 线程正在读取
          // 1 线程读取完成，值为：null
          // 2 线程读取完成，值为：null
          // 3 线程读取完成，值为：null
          // 1 线程写入完成
          // 2 线程写入完成
          // 3 线程写入完成
          // =========================== after print out ===========================
          // 3 线程正在写入值：3
          // 3 线程写入完成
          // 1 线程正在写入值：1
          // 1 线程写入完成
          // 2 线程正在写入值：2
          // 2 线程写入完成
          // 2 线程正在读取
          // 3 线程正在读取
          // 1 线程正在读取
          // 3 线程读取完成，值为：3
          // 1 线程读取完成，值为：1
          // 2 线程读取完成，值为：2
      
      }
      class Cache{
          private volatile Map<String, Object> map = new HashMap<>();
          private ReadWriteLock rwLock = new ReentrantReadWriteLock();
      
      
          public void put(String key,Object value){
              rwLock.writeLock().lock();
              try {
                  System.out.println(Thread.currentThread().getName() + " 线程正在写入值：" + key);
                  // 暂停一会儿线程
                  try { TimeUnit.MILLISECONDS.sleep(500); } catch (InterruptedException e) { e.printStackTrace(); }
                  map.put(key, value);
                  System.out.println(Thread.currentThread().getName() + " 线程写入完成");
              } finally {
                  rwLock.writeLock().unlock();
              }
          }
      
          public void get(String key){
              rwLock.readLock().lock();
              try {
                  System.out.println(Thread.currentThread().getName() + " 线程正在读取");
                  // 暂停一会儿线程
                  try { TimeUnit.MILLISECONDS.sleep(300); } catch (InterruptedException e) { e.printStackTrace(); }
                  Object res = map.get(key);
                  System.out.println(Thread.currentThread().getName() + " 线程读取完成，值为：" + res);
              } finally {
                  rwLock.readLock().unlock();
              }
          }
      
          public void clearMap(){
              map.clear();
          }
      }
      
      ```
      
      > 凡是 Cache 缓存的东西，都要用 volatile 修饰，让别的线程立马可见。

13. **CountDownLatch、CyclicBarrier、Semaphore 使用过吗？**

    - CountDownLatch

      ![CountDownLatch](http://rjnbyrj6g.hn-bkt.clouddn.com/interview/CountDownLatch.jpg)

      让一些线程阻塞直到另一些线程完成操作后才被唤醒，比如图书馆管理员要等所有同学上完自习都走了以后自己才关门走人

      Count DownLatch 主要有两个方法，当一个或多个线程调用 await 方法时，调用线程会被阻塞，此时计数器不动。其它线程调用 countDown () 方法时会将计数器减 1（调用 countDown () 方法的线程不会阻塞），当计数器的值变为零时，调用 await 方法被阻塞的线程会被唤醒，继续执行。

      ```java
      @Getter
      @AllArgsConstructor
      enum CountryEnum {
          one(1, "齐"),
          two(2, "楚"),
          three(3, "燕"),
          four(4, "赵"),
          five(5, "魏"),
          six(6, "韩");
      
          private int code;
      
          private String desc;
      
          public static CountryEnum valueOf(Integer code) {
              return Arrays.stream(values())
                      .filter(v -> v.getCode() == code)
                      .findFirst()
                      .orElse(null);
          }
      }
      
      public class CountDownLatchDemo {
      
          public static void main(String[] args) throws InterruptedException {
              CountDownLatch countDownLatch = new CountDownLatch(3);
              for (int i = 1; i <= 3; i++) {
                  new Thread(() -> {
                      System.out.println(Thread.currentThread().getName() + " 上完自习，离开图书馆");
                      countDownLatch.countDown();
                  }, String.valueOf(i)).start();
              }
              countDownLatch.await();
              System.out.println(Thread.currentThread().getName() + " 最后关门走人");
      
              CountDownLatch countDownLatch1 = new CountDownLatch(6);
              // 暂停一会儿线程
              try { TimeUnit.SECONDS.sleep(3); } catch (InterruptedException e) { e.printStackTrace(); }
              for (int i = 1; i <= 6; i++) {
                  new Thread(() -> {
                      System.out.println(Thread.currentThread().getName() + " 国被灭");
                      countDownLatch1.countDown();
                  }, CountryEnum.valueOf(i).getDesc()).start();
              }
              countDownLatch1.await();
              System.out.println(Thread.currentThread().getName() + " 秦国一统天下");
          }
          // =========================== print out ===========================
          // 1 上完自习，离开图书馆
          // 3 上完自习，离开图书馆
          // 2 上完自习，离开图书馆
          // main 最后关门走人
          // 齐 国被灭
          // 楚 国被灭
          // 燕 国被灭
          // 韩 国被灭
          // 赵 国被灭
          // 魏 国被灭
          // main 秦国一统天下
      }
      ```

    - CyclicBarrier

      它是让一组线程到达屏障时被阻塞，直到最后一个线程到达屏障时才放行，通过 await() 方法来阻塞线程

      > 像聚会一样，当大家人都到齐了，才出发去目标地点

      ```java
      public class CyclicBarrierDemo {
      
          public static void main(String[] args) {
              CyclicBarrier cyclicBarrier = new CyclicBarrier(7, () -> {
                  System.out.println(Thread.currentThread().getName() + " --- 七颗龙珠收集完成，开始召唤神龙...");
              });
              for (int i = 1; i <= 7; i++) {
                  int finalInt = i;
                  new Thread(() -> {
                      System.out.println(Thread.currentThread().getName() + " 收集到了第" + finalInt + "龙珠");
                      try {
                          cyclicBarrier.await();
                      } catch (InterruptedException e) {
                          e.printStackTrace();
                      } catch (BrokenBarrierException e) {
                          e.printStackTrace();
                      }
                  }, String.valueOf(i)).start();
              }
          }
      }
      ```

    - Semaphore

      信号量主要用于两个目的，1. 用于多个共享资源的互斥使用 2. 用于控制线程并发数

      > 比如 4 辆车抢 2 个车位，控制刚开始只让两个线程进行停车执行业务逻辑，用完之后释放，接着让另外两个线程执行同样的操作

      ```java
      public class SemaphoreDemo {
      
          public static void main(String[] args) {
              Semaphore semaphore = new Semaphore(2);
              for (int i = 1; i <= 4; i++) {
                  new Thread(() -> {
                      try {
                          semaphore.acquire();
                          System.out.println(Thread.currentThread().getName() + " 抢到停车位");
                          // 暂停一会儿线程
                          try { TimeUnit.SECONDS.sleep(3); } catch (InterruptedException e) { e.printStackTrace(); }
                          System.out.println(Thread.currentThread().getName() + " 号线程停车 3s 后离开了停车场");
                      } catch (InterruptedException e) {
                          e.printStackTrace();
                      } finally {
                          semaphore.release();
                      }
                  }, String.valueOf(i)).start();
              }
          }
          // =========================== print out ===========================
          // 1 抢到停车位
          // 2 抢到停车位
          // 2 号线程停车 3s 后离开了停车场
          // 1 号线程停车 3s 后离开了停车场
          // 3 抢到停车位
          // 4 抢到停车位
          // 4 号线程停车 3s 后离开了停车场
          // 3 号线程停车 3s 后离开了停车场
      }
      ```

    **CyclicBarrier 和 CountDownLatch 的区别 **

    | 区别       | CountDownLatch                                               | CyclicBarrier                                                |
    | ---------- | :----------------------------------------------------------- | ------------------------------------------------------------ |
    | 运行机制   | 线程运行到某个点上之后，只是给某个数值减一，该线程会继续运行 | 当线程运行到某个点上之后，该线程就会阻塞，直到所有的线程都到达了这个点，所有线程才重新运行 |
    | 唤醒任务数 | 可以唤起多个任务                                             | 只能唤起一个任务                                             |
    | 是否可重用 | 不可重用，计数值为 0 时该 CountDownLatch 就不可再用了        | 可重用                                                       |

14. **阻塞队列知道吗？**

    我们再去银行和医院的时候不得不阻塞，那里会有一个候客区，也就是阻塞队列

    阻塞队列，顾名思义，首先它是一个队列，当阻塞队列是空时，从队列中获取元素的操作将会被阻塞。当阻塞队列是满时，往队列里添加元素的操作将会被阻塞。

    > 蛋糕店的蛋糕还没生产出来的为空的时候，消费者消费蛋糕的操作会被阻塞。
    >
    > 蛋糕柜子只能放 10 个蛋糕，再生产出的蛋糕就会被阻塞

    **优点（为什么需要 BlockingQueue？）：**

    在多线程领域：所谓阻塞，在某些情况下会挂起线程，一旦条件满足，被挂起的线程又会自动被唤醒

    BlockingQueue 可以让我们不必去关心什么时候需要阻塞线程，什么时候需要唤醒线程，因为这一切 BlockingQueue 都给搞定了

    **BlockingQueue 架构梳理和分类：**

    - <font color='red'>ArrayBlockingQueue：由数组结构组成的有界阻塞队列，按 FIFO 对元素排序 </font>

    - <font color='red'>LinkedBlockingDeque：由链表结构组成的有界（但大小默认值 Integer>MAX_VALUE）阻塞队列，按 FIFO 方式来对元素排序 </font>

    - PriorityBlockingQueue：支持优先级排序的无界阻塞队列

    - DelayQueue：使用优先级队列实现的延迟无界阻塞队列

    - <font color='red'>SynchronousQueue：不存储元素的阻塞队列，每次插入操作必须要等到另一个线程调用移除操作，否则插入操作一直处于阻塞状态 </font>

      > 候客区有三个人，第一个人办理业务 “put 1”，两秒后办完业务拿走 “take 1”，到第二个人去办理

      ```java
      public class SynchronousQueueDemo {
          public static void main(String[] args) {
              SynchronousQueue<String> blockingQueue = new SynchronousQueue<>();
              new Thread(() -> {
                  try {
                      System.out.println(Thread.currentThread().getName() + " put 1");
                      blockingQueue.put("1");
                      System.out.println(Thread.currentThread().getName() + " put 2");
                      blockingQueue.put("2");
                      System.out.println(Thread.currentThread().getName() + " put 3");
                      blockingQueue.put("3");
                  } catch (InterruptedException e) {
                      e.printStackTrace();
                  }
              }, "T1").start();
      
              new Thread(() -> {
                  try {
                      // 暂停一会儿线程，两秒后去拿 “put 1”
                      try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); }
                      System.out.println(Thread.currentThread().getName() + " take " + blockingQueue.take());
                      // 暂停一会儿线程，两秒后去拿 “put 2”
                      try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); }
                      System.out.println(Thread.currentThread().getName() + " take " + blockingQueue.take());
                      // 暂停一会儿线程，两秒后去拿 “put 3”
                      try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); }
                      System.out.println(Thread.currentThread().getName() + " take " + blockingQueue.take());
                  } catch (InterruptedException e) {
                      e.printStackTrace();
                  }
              }, "T2").start();
          }
          // ================================ print out ================================
          // T1 put 1
          // T2 take 1
          // T1 put 2
          // T2 take 2
          // T1 put 3
          // T2 take 3
      }
      ```

    - LinkedTransferQueue：由链表结构组成的无界阻塞队列

    - LinkedBlockingDeque：由了解结构组成的双向阻塞队列

    **BlockingQueue 核心方法：**

    | 方法类型 | 抛出异常  | 特殊值   | 阻塞   | 超时               |
    | -------- | --------- | -------- | ------ | ------------------ |
    | 插入     | add(e)    | offer(e) | put(e) | offer(e,time,unit) |
    | 移除     | remove()  | poll()   | take() | poll(time,unit)    |
    | 检查     | element() | peek()   | 不可用 | 不可用             |

    - 抛出异常

      当阻塞队列满时，再往队列里面 add 插入元素会抛 IllegalStateException: Queue full

      当阻塞队列空时，再往队列 Remove 元素时候回抛出 NoSuchElementException

    - <font color='red'>特殊值（工作中使用）</font>

      插入方法，成功返回 true 失败返回 false

      移除方法，成功返回元素，队列里面没有就返回 null

    - 阻塞

      当阻塞队列满时，生产者继续往队列里面 put 元素，队列会一直阻塞直到 put 数据 or 响应中断退出

      当阻塞队列空时，消费者试图从队列 take 元素，队列会一直阻塞消费者线程直到队列可用

    - 超时退出

      当阻塞队列满时，队列会阻塞生产者线程一定时间，超过后限时后生产者线程就会退出

    **实际应用**

    - 生产消费者模式

      - 传统版

        为什么要用 lock 包替代 synchronized，api 文档说 lock 更具扩展性，因为 lock 是类，可继承，可重写等等

        > `Lock` implementations provide more extensive locking operations than can be obtained using `synchronized` methods and statements. They allow more flexible structuring, may have quite different properties, and may support multiple associated [`Condition`](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/locks/Condition.html) objects.

        ```java
        package com.liuilin.JUC;
        
        import java.util.concurrent.locks.Condition;
        import java.util.concurrent.locks.Lock;
        import java.util.concurrent.locks.ReentrantLock;
        
        /**
         * 题目：一个初始值为零的变量，两个线程对其交替操作，一个加一个减 1，来 5 轮（相当于两个人操作空调，一个加一度，一个减一度，交替执行 5 次）
         *
         * 多线程模板的企业级口诀：高并发下前提一定是高内聚、低耦合
         * 上联：线程    操作  资源类（解耦，空调自带制冷制热功能，而非人的功能）
         * 下联：判断    干活  唤醒通知
         * 横批：严防多线程下的虚假唤醒
         *
         * @author liuqiang
         * @since 2021-08-03
         */
        public class ProdConsumer {
        
            public static void main(String[] args) {
                ShareData data = new ShareData();
                new Thread(() -> {
                    for (int i = 1; i <= 5; i++) {
                        try {
                            data.increment();
                        } catch (InterruptedException e) {
                            e.printStackTrace();
                        }
                    }
                }, "T1").start();
        
                new Thread(() -> {
                    for (int i = 1; i <= 5; i++) {
                        try {
                            data.decrement();
                        } catch (InterruptedException e) {
                            e.printStackTrace();
                        }
                    }
                }, "T2").start();
        
            }
        
        }
        
        class ShareData {
        
            Lock lock = new ReentrantLock();
            int num = 0;
            private Condition condition = lock.newCondition();
        
            public void increment() throws InterruptedException {
                lock.lock();
                try {
                    // 判断
                    while (num != 0) {
                        // 等待
                        condition.await();
                    }
                    // 干活
                    num++;
                    System.out.println(Thread.currentThread().getName() + " 生产一个 " + num);
                    // 唤醒通知
                    condition.signalAll();
        
                } finally {
                    lock.unlock();
                }
            }
        
            public void decrement() throws InterruptedException {
                lock.lock();
                try {
                    // 判断
                    while (num == 0) {
                        // 等待
                        condition.await();
                    }
                    // 干活
                    num--;
                    System.out.println(Thread.currentThread().getName() + " 消费一个 " + num);
                    // 唤醒通知
                    condition.signalAll();
        
                } finally {
                    lock.unlock();
                }
            }
        }
        ```

      - 阻塞队列版

        > 凡是写架构程序，给多人用时，一定要考虑通顺、适配和通用，也就是要传**接口**，不许传具体的类；写：足够的抽象，往高处写；查：足够的细节，往细节落地
      
        ```java
        public class ProdConsumer_BlockingQueueDemo {
        
            public static void main(String[] args) {
                MyResource myResource = new MyResource(new ArrayBlockingQueue<>(5));
                new Thread(() -> {
                    System.out.println(Thread.currentThread().getName() + " 生产线程启动");
                    try {
                        myResource.product();
                    } catch (InterruptedException e) {
                        e.printStackTrace();
                    }
                }, "Product").start();
        
                new Thread(() -> {
                    System.out.println(Thread.currentThread().getName() + " 消费线程启动");
                    try {
                        myResource.consume();
                    } catch (InterruptedException e) {
                        e.printStackTrace();
                    }
                }, "Consumer").start();
        
                // 暂停一会儿线程
                try { TimeUnit.SECONDS.sleep(5); } catch (InterruptedException e) { e.printStackTrace(); }
                myResource.stop();
                System.out.println(Thread.currentThread().getName() + " --- 5s 时间到，main 线程叫停一切");
            }
            // ============================== print out =============================
            // Product 生产线程启动
            // Product 开始生产，0 插入成功
            // Consumer 消费线程启动
            // Consumer 消费蛋糕队列成功，值为：0
            // Product 开始生产，1 插入成功
            // Consumer 消费蛋糕队列成功，值为：1
            // Product 开始生产，2 插入成功
            // Consumer 消费蛋糕队列成功，值为：2
            // Product 开始生产，3 插入成功
            // Consumer 消费蛋糕队列成功，值为：3
            // Product 开始生产，4 插入成功
            // Consumer 消费蛋糕队列成功，值为：4
            // main --- 5s 时间到，main 线程叫停一切
            // Product 停止生产
            // Consumer 超过 2s 没有获取到蛋糕，消费退出并停止获取
        }
        
        class MyResource {
        
            BlockingQueue<String> blockingQueue;
            // 默认开启生产和消费，volatile 保证生产后别的线程里面可见
            private volatile boolean FLAG = true;
            private AtomicInteger atomicInteger = new AtomicInteger();
        
            public MyResource(BlockingQueue<String> blockingQueue) {
                this.blockingQueue = blockingQueue;
                System.out.println(blockingQueue.getClass().getName());
            }
        
            public void product() throws InterruptedException {
                // 判断、干活、唤醒线程
                String data;
                while (FLAG) {
                    // 不能这么写，这是一个 while 循环，会导致一直不停的创建对象
                    // String data = atomicInteger.getAndIncrement() + "";
                    data = atomicInteger.getAndIncrement() + "";
                    // new ArrayBlockingQueue<>(5)，队列的总容量是 5，当生产满 5 个之后，过 2s 再生产就会返回 false
                    boolean res = blockingQueue.offer(data, 2L, TimeUnit.SECONDS);
                    if (res) {
                        System.out.println(Thread.currentThread().getName() + " 开始生产，" + data + " 插入成功");
                    } else {
                        System.out.println(Thread.currentThread().getName() + " 开始生产，" + data + " 插入失败");
                    }
                    // 1s 生产一个，挂起线程切换消费线程
                    TimeUnit.SECONDS.sleep(1);
                }
                System.out.println(Thread.currentThread().getName() + " 停止生产");
            }
        
            public void consume() throws InterruptedException {
                String res;
                while (FLAG) {
                    // 不能这么写，这是一个 while 循环，会导致一直不停的创建对象
                    // String res = blockingQueue.poll(2L, TimeUnit.SECONDS);
                    res = blockingQueue.poll(2L, TimeUnit.SECONDS); // 2s 后都还没取到 offer 进去的数据，就返回 false，进行下一步操作
                    if (res == null || "".equalsIgnoreCase(res)) {
                        FLAG = false;
                        System.out.println(Thread.currentThread().getName() + " 超过 2s 没有获取到蛋糕，消费退出并停止获取");
                        return;
                    }
                    System.out.println(Thread.currentThread().getName() + " 消费蛋糕队列成功，值为：" + res);
                }
            }
        
            public void stop() {
                FLAG = false;
            }
        
        }
        ```
      

15. **谈谈你对线程池的理解（线程池的工作原理，几个重要参数，然后给了具体几个参数分析线程池会怎么做，最后问阻塞队列的作用是什么？）**

    [Java 线程池实现原理及其在美团业务中的实践 - 美团技术团队](https://tech.meituan.com/2020/04/02/java-pooling-pratice-in-meituan.html)

    > 镜像问题：
    >
    > 阿里：线程池的构造类的方法的 5 个参数的具体意义？
    >
    > 美团：使用无界阻塞队列会出现什么问题？
    >
    > 百度：线程池用过吗都有什么参数？底层如何实现的？

    线程池 = ThreadPoolExecutor 类 + 阻塞队列

    - **为什么用线程池？优势是什么**

      new Thread () 每次运行后都需要 JVM 进行垃圾回收，那用线程池之后呢可以复用线程，也就不需要我们自己 new 线程了。就像 Spring 一样的依赖注入一样，提前注入好对象，用的时候直接从池里拿就行了

      优势：

      线程池主要作用是控制运行的线程数，处理过程中会 <font color='red'> 将任务放入队列 </font>，然后在线程创建后启动这些任务，如果 <font color='red'> 线程数量超过了最大数量，超出的线程会进行排队等候 </font>，等前面的线程执行完毕后，再从队列中取出任务来执行。

      i. 降低资源消耗：通过重复利用已创建的线程降低线程创建和销毁所造成的消耗。

      ii. 提高响应速度：当任务到达时，任务可以不等到线程创建就能立即执行。

      iii. 提高线程的可管理性：线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性。可以使用线程池可以进行统一的分配，调优和监控

      iv. 提供更多更强大的功能：线程池具备可拓展性，允许开发人员向其中增加更多的功能。比如延时定时线程池 ScheduledThreadPoolExecutor，就允许任务延期执行或定期执行。

      > // Array - Arrays | Collection - Collections | Executor - Executors

    - **线程池如何使用？**

      ![线程池UML](http://rjnbyrj6g.hn-bkt.clouddn.com/interview/%E7%BA%BF%E7%A8%8B%E6%B1%A0UML.jpg)

      编码实现：

      所有线程池底层都会调用 ThreadPoolExecutor 类

      ```java
      public ThreadPoolExecutor(int corePoolSize,
                                int maximumPoolSize,
                                long keepAliveTime,
                                TimeUnit unit,
                                BlockingQueue<Runnable> workQueue,
                                ThreadFactory threadFactory,
                                RejectedExecutionHandler handler) {
      // ...
      }
      ```

      - newFixedThreadPool(int nThreads)

        执行长期的任务，性能好很多（多个线程交替执行）
      
        ```java
        public static ExecutorService newFixedThreadPool(int nThreads) {
            return new ThreadPoolExecutor(nThreads, nThreads,
                                          0L, TimeUnit.MILLISECONDS,
                                          new LinkedBlockingQueue<Runnable>());
        }
        ```
        
      - Executors.newSingleThreadExecutor()
      
        一个任务一个任务执行的场景
      
        ```java
        public static ExecutorService newSingleThreadExecutor() {
            return new FinalizableDelegatedExecutorService
                (new ThreadPoolExecutor(1, 1,
                                        0L, TimeUnit.MILLISECONDS,
                                        new LinkedBlockingQueue<Runnable>()));
        }
        ```
      
      - Executors.newCachedThreadPool()
      
      执行很多短期异步的小程序或者负载较轻的服务器（不知道会用多少线程，不够就扩容）
      
        ```java
        public static ExecutorService newCachedThreadPool() {
            return new ThreadPoolExecutor(0, Integer.MAX_VALUE,
                                          60L, TimeUnit.SECONDS,
                                          new SynchronousQueue<Runnable>());
        }
        ```
      
        > 还有了解的 2 种
        >
        > newScheduledThreadPool(int corePoolSize)
        >
        > newWorkStealingPool() // Java 8 新出
      
        ```java
        public class ThreadPoolDemo {
        
            public static void main(String[] args) {
        //        ExecutorService threadPool = Executors.newFixedThreadPool(5); // 一池 5 个处理线程（银行 5 个处理业务窗口）
        //        ExecutorService threadPool = Executors.newSingleThreadExecutor(); // 一池 1 个处理线程（只有一个工作人员上班）
                ExecutorService threadPool = Executors.newCachedThreadPool(); // 一池 N 线程，随机扩容多线程
        
                // 池化技术先关池，再操作业务
                try {
                  for (int i = 1; i <= 10; i++) {
                        threadPool.execute(() -> System.out.println(Thread.currentThread().getName() + " 办理业务"));
                      // 暂停一会儿线程
                        try { TimeUnit.MICROSECONDS.sleep(200); } catch (InterruptedException e) { e.printStackTrace(); }
                    }
                } catch (Exception e) {
                    e.printStackTrace();
                } finally {
                    threadPool.shutdown();
                }
            }
        
        }
        ```
      
    - **ThreadPoolExecutor 类 7 大参数的含义：**

      1. corePoolSize：线程池中常驻线程核心数

         在创建了线程池后，当有请求任务来之后就会安排池中的线程去执行请求任务（今日当值线程）

         当线程池中的线程数目达到 corePoolSize 后，就会把到达的任务放到缓存队列当中；

      2. maximumPoolSize：线程池能够容纳同时执行的最大线程数，必须大于 1

      3. keepAliveTime：多余的空闲线程的存活时间

         当空闲时间达到 keepAliveTime 并且线程池数量超过 corePoolSize，那么多余的空闲线程会被销毁直到只剩下 corePoolSize 个线程为止
         
      4. unit：KeepAliveTime 的单位

      5. workQueue：阻塞队列，被提交但是还未被执行的任务

      6. threadFactory：线程工厂，是用来生成线程池中的工作线程的。用于创建线程（一般用默认的即可）

      7. handler：拒绝策略，表示当队列满了并且工作线程 >= 线程的最大线程数 maximumPoolSize

         > 助记：Candy makes Kate.Perry unbelievable，with two heads

    - **说说线程池的底层工作原理？(重要）**

      1. 在创建了线程池之后，线程池会等待提交过来的请求任务。当调用 execute () 方法时就会添加一个请求任务，此时线程池会做判断，如果正在运行的线程数量 < corePoolSize 时，会立马创建线程来执行这个任务

      2. 请求任务变多以后，正在运行的线程数量 > corePoolSize，那么会将任务<font color='red'>放入阻塞队列</font>

      3. 阻塞队列也满了而正在运行的线程数 < maximumPoolSize，那么还是会创建非核心线程来执行任务（相当于在扩容）

      4. 非核心线程也满了并且正在运行的线程数还 >=  maximumPoolSize，那么线程池会执行拒绝策略

      5. 当一个核心线程完成一个任务后，它会从阻塞队列中获取下一个任务来执行

      6. 当一个线程无事可做并且时间超过设置的 keepAliveTime 时，线程池就会判断：

         i. 如果当前运行的线程数 > corePoolSize，那么这个线程就被停掉

         ii. 当线程池执行完所有任务后最终把线程容量会收缩到 corePoolSize 的大小
      
      > 10 个用户来办理业务，又刚好是周末，只有两个值班窗口（corePoolSize），两个任务过来就直接开始办理。但此时 coolPoolSize 满了，任务继续提交了过来，那么提交过来的任务就会进入阻塞队列（银行的候客区）。阻塞队列也满了之后就会把非核心线程数（其它的窗口）都开放来加班处理任务。直到达到了 maximumPoolSize 最大线程数，就会开启拒绝策略（RejectedExecutionHandler），避免银行发生事故。当一个工作线程完成一个任务后，它会从阻塞队列中获取下一个任务来执行

    ![线程池底层工作原理](http://rjnbyrj6g.hn-bkt.clouddn.com/interview/%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%BA%95%E5%B1%82%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86.jpg)

16. **线程池用过吗？生产上你是如何设置参数的？**

    - 线程拒绝策略请你讲讲

      JDK 内置拒绝策略：

      - AbortPolicy (默认)：直接抛出 RejectedException 异常阻止系统正常运行（生产上不敢用，上来就是报异常）
      - CallerRunPolicy："调用者运行" 一种调节机制，该策略既不会抛弃任务，也不会抛出异常，而是
      - DiscardOldestPolicy：抛弃队列中等待最久的任务，然后把当前任务加入队列中尝试再次提交
      - ==DiscardPolicy：==直接丢弃任务，不予任何处理也不抛出异常。如果允许任务丢失，这是最好的拒绝策略
      
      ```java
      public static void main(String[] args) {
          ThreadPoolExecutor threadPool = new ThreadPoolExecutor(
                  2,
                  5,
                  1L,
                  TimeUnit.SECONDS,
                  new LinkedBlockingQueue<>(3),
                  Executors.defaultThreadFactory(),
                  // new ThreadPoolExecutor.AbortPolicy()
                  // new ThreadPoolExecutor.CallerRunsPolicy()
                  // new ThreadPoolExecutor.DiscardPolicy()
                  new ThreadPoolExecutor.DiscardOldestPolicy()
          );
          // 池化技术先关池，再操作业务
          try {
              for (int i = 1; i <= 10; i++) {
                  threadPool.execute(() -> System.out.println(Thread.currentThread().getName() + " 办理业务"));
              }
          } catch (Exception e) {
              e.printStackTrace();
          } finally {
              threadPool.shutdown();
          }
      }
      ```
      
    - 你在工作中单一的 / 固定数的 / 可变的三种创建线程池的方法，你哪个用的多？超级大坑

      答：一个都不用，我是用自定义的。虽然 JDK 内置了 4 种拒绝策略，但是 Executors 返回的线程池对象允许的请求队列长度是 20 多亿，相当于初始化后银行候客区就有 20 多亿的位置，任务哗的一下就把堆空间撑满了，堆积大量请求，从而导致 OOM。

      > 阿里巴巴开发手册说道
      >
      > 【强制】线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式，这样
      > 的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。
      >
      > 说明： Executors 返回的线程池对象的弊端如下：
      >
      > 1） FixedThreadPool 和 SingleThreadPool:
      >
      > 允许的请求队列长度为 Integer.MAX_VALUE，可能会堆积大量的请求，从而导致 OOM。
      >
      > 2） CachedThreadPool 和 ScheduledThreadPool:
      >
      > 允许的创建线程数量为 Integer.MAX_VALUE， 可能会创建大量的线程，从而导致 OOM。

    - 合理配置线程池你是如何考虑的？

      - CPU 密集型（里面有 while 循环，CPU 哗的一下就上来了）

        what：是该任务需要大量的运算，而没有阻塞，CPU 速度给它拉满

        CPU 密集任务只有在真正的多核 CPU 上才可能得到加速（通过多线程），单线程得不到加速，CPU 的限制

        CPU 密集型任务配置尽可能少的线程数量（一核 CPU 跑一个线程）：一般公式：CPU 核数 + 1 个线程的线程池

      - IO 密集型

        what：会频繁的去数据库读取数据，有 1w 个任务要去数据库 MySQL、Redis 取数据，就叫 IO 密集型
        
        i. 由于 IO 密集型任务线程并不是一直在执行任务，则应配置尽可能多的线程，如 CPU 核数 * 2
        
        ii. IO 密集型时，大部分线程都阻塞，故需要多配置线程数：
        
        参考公式：CPU 核数 / 1 - 阻塞系数
        
        阻塞系数在 0.8~0.9 之间
        
        比如 8 核 CPU：8/1-0.9=80 个线程数
        
        ```java
        // 获取 CPU 核数
        maximumPoolSize = Runtime.getRuntime().availableProcessors() / 1 - 0.9
        ```

17. **死锁编码及定位分析**

    <img src="http://rjnbyrj6g.hn-bkt.clouddn.com/interview/%E6%AD%BB%E9%94%81.jpg" alt="死锁" style="zoom:33%;" />

    what：死锁是指两个或两个以上的进程在执行过程中，因争夺资源而造成的一种 <font color='red'> 互相等待 </font> 的现象，若无外力干涉那它们都将无法推进下去。（相当于两个人用手枪互相指着对方）

    why：

    - 系统资源不足
    - 进程运行推进的顺序不合适
    - 资源分配不当

    代码：

    ```java
    public class DeadLockDemo {
    
        public static void main(String[] args) {
            new Thread(new ThreadLockHolder("lockA", "lockB"), "T1").start();
            new Thread(new ThreadLockHolder("lockB", "lockA"), "T2").start();
        }
    
    }
    
    class ThreadLockHolder implements Runnable {
    
        String lock1;
        String lock2;
    
        public ThreadLockHolder(String lockA, String lockB) {
            this.lock1 = lockA;
            this.lock2 = lockB;
        }
    
        @Override
        public void run() {
            synchronized (lock1) {
                System.out.println(Thread.currentThread().getName() + " 自己持有：" + lock1 + "，尝试获得：" + lock2);
                // 暂停一会儿线程
                try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); }
                synchronized (lock2) {
                    System.out.println(Thread.currentThread().getName() + " 自己持有：" + lock2 + "，尝试获得：" + lock1);
                }
            }
        }
    }
    ```

    > 注意构造方法，T1 构造的是 lock1 = lockA，T2 构造的是 lock1 = lockB
    >
    > 解读：T1 到 synchronized 那拿到 lockA，接着挂起 2s。T2 进来拿到 lockB 并挂起。都挂完后，T1 发现需要拿到 lock2（lockB）才能往下走，而 T2 线程需要拿到 lock2（lockB）才能往下走。结果出现了互相等待，从而产生死锁

    解决：

    - i. jps -l：Output the full package name for the application's main class or the full path name to the application's JAR file. （输出在运行程序的完整类名）

      ii. 找到 27484 com.liuilin.JUC.DeadLockDemo

      ii. jstack 27484：发现死锁

      ```java
      Java stack information for the threads listed above:
      ===================================================
      "ThreadB":
              at com.liuilin.JUC.ThreadLockHolder.run(DeadLockDemo.java:37)
              - waiting to lock <0x00000007178edd68> (a java.lang.String)
              - locked <0x00000007178edda0> (a java.lang.String)
              at java.lang.Thread.run(Thread.java:748)
      "ThreadA":
              at com.liuilin.JUC.ThreadLockHolder.run(DeadLockDemo.java:37)
              - waiting to lock <0x00000007178edda0> (a java.lang.String)
              - locked <0x00000007178edd68> (a java.lang.String)
              at java.lang.Thread.run(Thread.java:748)
      
      Found 1 deadlock.
      ```

      找到问题所在，修改代码并告知运维

    - jconsole：连接到对应的线程后可直接查看死锁


16. **sleep 和 wait 的区别**

    sleep () 方法可以在任何地方使用、wait () 方法则只能在同步方法或同步块中使用

    sleep () 是线程 Thread 类的方法，调用后会暂停此线程指定的时间，但监控依然保持，不会释放对象锁，到时间自动恢复

    wait () 是 Object 的方法，调用后会放弃对象锁，进入等待队列，只有调用 notify ()/notifyAll () 才能唤醒指定的线程或者所有线程

17. **谈谈你对 ThreadLocal 的了解，实现原理**

    ThreadLocal 可以看做是一个容器，容器里面存放着属于当前线程的变量。ThreadLocal 类提供了四个对外开放的接口方法

    - void set (Object value) 设置当前线程的线程局部变量的值。
    - public Object get () 该方法返回当前线程所对应的线程局部变量。 
    - public void remove () 将当前线程局部变量的值删除，目的是为了减少内存的占用
    - protected Object initialValue () 返回该线程局部变量的初始值。那 ThreadLocal 内部是如何为每一个线程维护变量副本的呢 其实在 ThreadLocal 类中有一个静态内部类 ThreadLocalMap (其类似于 Map)，用键值对的形式存储每一个线程的变量副本，ThreadLocalMap 中元素的 key 为当前 ThreadLocal 对象，而 value 对应线程的变量副本，每个线程可能存在多个 ThreadLocal

    > 可以实现单例，它为每个线程提供一个独立的变量副本。它隔离了多个线程解决了多个线程间的访问冲突。使用同步锁是以时间换空间的方式，因为要排队。用 ThreadLocal 是以空间换时间的方式，里面会创建很多个对象，一个线程里有一个，但对于这个线程来说获取的这个对象是唯一的


### 数据库相关

==SQL题练习：==

[50 道 SQL 练习题及答案与详细分析 - 简书](https://www.jianshu.com/p/476b52ee4f1b)

[SQL 数据库实战题_面试必刷 + 解析_牛客题霸_牛客网](https://www.nowcoder.com/ta/sql)

[MySQL 数据库性能优化视频教程 - 慕课网](http://www.imooc.com/learn/194)

1. **事务 [MySQL 事务 - 廖雪峰的官方网站](https://www.liaoxuefeng.com/wiki/1177760294764384/1179611198786848)**

   > 修改事务的默认提交方式：set @@autocommit = xx；1 代表自动提交，0 代表手动提交

   执行 SQL 语句的时候，在某些业务要求下一系列操作必须全部执行，而不能仅执行一部分。比如转账操作就是：

   ```sql
   -- 从 id=1 的账户给 id=2 的账户转账 100 元
   -- 第一步：将 id=1 的 A 账户余额减去 100
   UPDATE accounts SET balance = balance - 100 WHERE id = 1;
   -- 第二步：将 id=2 的 B 账户余额加上 100
   UPDATE accounts SET balance = balance + 100 WHERE id = 2;
   ```

   这两条 SQL 语句必须全部执行，或者，由于某些原因，如果第一条语句成功，第二条语句失败，就必须全部撤销。不能出现一边账户的余额扣了，但是另一边账户余额没加的情况发生。

   这种把 <font color='red'> 多条语句作为一个整体进行操作的功能，被称为数据库事务 </font>。数据库事务可以确保该事务范围内的所有操作都可以全部成功或者全部失败。如果事务失败那么效果就要和没有执行这些 SQL 一样，不会对数据库数据有任何改动。也就是我们双方都没有增减钱

   可见，数据库事务具有 ACID 这 4 个特性：

   - **事务的四大特征（ACID）：**

      - 原子性（atomicity）：将所有 SQL 作为 `原子工作单元` 执行，要么全部执行，要么全部不执行

      - 一致性（consistency）：事务完成后，所有数据的状态都是一致的，即 A 账户只要减去了 100，B 账户就必定加上了 100

      - 隔离性（isolation）：如果有多个事务并发执行，每个事务作出的修改必须与其他事务隔离

      - 持久性（durability）：即事务完成后，对数据库数据的修改会持久化存储。

        对于单条 SQL 语句，数据库系统会自动将其作为一个事务执行，这种事务被称为 **隐式事务**（也就是自动提交）。

        要手动把多条 SQL 语句作为一个事务执行，需要使用 `BEGIN` 开启一个事务，使用 `COMMIT` 提交一个事务，这种事务被称为 **显式事务**，例如，把上述的转账操作作为一个显式事务：

        ```sql
        BEGIN;
        UPDATE accounts SET balance = balance - 100 WHERE id = 1;
        UPDATE accounts SET balance = balance + 100 WHERE id = 2;
        COMMIT;
        ```

        所以多条 SQL 语句要想作为一个事务执行，就必须使用显式事务。

        `COMMIT` 是指提交事务，即试图把事务内的所有 SQL 所做的修改进行永久保存。如果 `COMMIT` 语句执行失败了，整个事务就也会失败。

        但是有些时候，我们希望主动让事务失败，这时，可以用 `ROLLBACK` 回滚事务，整个事务会失败：

        ```sql
        BEGIN;
        UPDATE accounts SET balance = balance - 100 WHERE id = 1;
        UPDATE accounts SET balance = balance + 100 WHERE id = 2;
        ROLLBACK;
        ```

        数据库事务是由数据库系统保证的，我们只需要根据业务逻辑使用它就可以。

   - **事务的隔离级别：**
     
     > 数据库查询隔离级别：`select @@tx_isolation` ；`show variables like 'tx_isolation'`
     >
     > 数据库设置隔离级别：set global transaction isolation level 级别字符串

     对于两个并发执行的事务，如果涉及到操作同一条记录的时候，会导致数据不一致性的现象，包括脏读、不可重复读、幻读等。那 MySQL 提供了四大隔离级别来让我们避免数据不一致的问题。
     
     SQL 标准定义了 4 种隔离级别，分别对应可能出现的数据不一致的情况：
     
     | Isolation Level  | 脏读（Dirty Read） | 不可重复读（Non Repeatable Read） | 幻读（Phantom Read） |
     | :--------------- | :----------------- | :-------------------------------- | :------------------- |
     | Read Uncommitted | Yes                | Yes                               | Yes                  |
     | Read Committed   | -                  | Yes                               | Yes                  |
     | Repeatable Read  | -                  | -                                 | Yes                  |
     | Serializable     | -                  | -                                 | -                    |
     
     四大隔离级别：
     
       - Read Uncommitted（读未提交）：是隔离级别最低的一种事务级别。在这种隔离级别下，一个事务会读到另一个事务更新后但未提交的数据，如果另一个事务回滚，那么当前事务读到的数据就是脏数据，这就是脏读（Dirty Read）。
     
         我们来看一个例子。
     
         首先，我们准备好 `students` 表的数据，该表仅一行记录：
     
         ```sql
         mysql> select * from students;
         +----+-------+
         | id | name  |
         +----+-------+
         |  1 | Alice |
         +----+-------+
         1 row in set (0.00 sec)
         ```
     
         然后，分别开启两个 MySQL 客户端连接，按顺序依次执行事务 A 和事务 B：
     
         | 时刻 | 事务 A                                            | 事务 B                                                       |
         | :--- | :------------------------------------------------ | :----------------------------------------------------------- |
         | 1    | SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED; | SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;            |
         | 2    | BEGIN;                                            | BEGIN;                                                       |
         | 3    | UPDATE students SET name = 'Bob' WHERE id = 1;    |                                                              |
         | 4    |                                                   | SELECT * FROM students WHERE id = 1;  <br /># 读到 A 事务未提交数据 Bob |
         | 5    | ROLLBACK;                                         |                                                              |
         | 6    |                                                   | SELECT * FROM students WHERE id = 1;                         |
         | 7    |                                                   | COMMIT;                                                      |
     
         当事务 A 执行完第 3 步时，它更新了 `id=1` 的记录，但并未提交，而事务 B 在第 4 步读取到的数据就是未提交的数据。
     
         随后，事务 A 在第 5 步进行了回滚，事务 B 再次读取 `id=1` 的记录，发现和上一次读取到的数据不一致，这就是脏读。
     
         可见，在 Read Uncommitted 隔离级别下，一个事务可能读取到另一个事务更新但未提交的数据，这个数据有可能是脏数据。
     
       - Read Committed：读已提交
     
         在 Read Committed 隔离级别下，事务可能会遇到不可重复读（Non Repeatable Read）的问题。
     
         不可重复读是指，在一个事务内，多次读同一数据，在这个事务还没有结束时，如果另一个事务恰好修改了这个数据，那么，在当前事务两次读取的数据就可能不一致。
     
         然后，分别开启两个 MySQL 客户端连接，按顺序依次执行事务 A 和事务 B：
     
         | 时刻 | 事务 A                                          | 事务 B                                            |
         | :--- | :---------------------------------------------- | :------------------------------------------------ |
         | 1    | SET TRANSACTION ISOLATION LEVEL READ COMMITTED; | SET TRANSACTION ISOLATION LEVEL READ COMMITTED;   |
         | 2    | BEGIN;                                          | BEGIN;                                            |
         | 3    |                                                 | SELECT * FROM students WHERE id = 1;<br /># Alice |
         | 4    | UPDATE students SET name = 'Bob' WHERE id = 1;  |                                                   |
         | 5    | COMMIT;                                         |                                                   |
         | 6    |                                                 | SELECT * FROM students WHERE id = 1;<br /># Bob   |
         | 7    |                                                 | COMMIT;                                           |
     
         当事务 B 第一次执行第 3 步的查询时，得到的结果是 `Alice`，随后，由于事务 A 在第 4 步更新了这条记录并提交，所以，事务 B 在第 6 步再次执行同样的查询时，得到的结果就变成了 `Bob`，因此，在 Read Committed 隔离级别下，事务不可重复读同一条记录，因为很可能读到的结果不一致。
     
        - Repeatable Read：可重复读
     
          在 Repeatable Read 隔离级别下，一个事务可能会遇到幻读（Phantom Read）的问题。
     
          > 幻读是指，在一个事务中，第一次查询某条记录，发现没有，但是当别的事务添加了这条不存在数据并提交事务后，当前事务试图更新这条不存在的记录时，竟然能成功，并且，再次读取同一条记录，它就神奇地出现了。
     
          然后，分别开启两个 MySQL 客户端连接，按顺序依次执行事务 A 和事务 B：
     
          | 时刻 | 事务 A                                              | 事务 B                                            |
          | :--- | :-------------------------------------------------- | :------------------------------------------------ |
          | 1    | SET TRANSACTION ISOLATION LEVEL REPEATABLE READ;    | SET TRANSACTION ISOLATION LEVEL REPEATABLE READ;  |
          | 2    | BEGIN;                                              | BEGIN;                                            |
          | 3    |                                                     | SELECT * FROM students WHERE id = 99;             |
          | 4    | INSERT INTO students (id, name) VALUES (99, 'Bob'); |                                                   |
          | 5    | COMMIT;                                             |                                                   |
          | 6    |                                                     | SELECT * FROM students WHERE id = 99;             |
          | 7    |                                                     | UPDATE students SET name = 'Alice' WHERE id = 99; |
          | 8    |                                                     | SELECT * FROM students WHERE id = 99;             |
          | 9    |                                                     | COMMIT;                                           |
     
          事务 B 在第 3 步第一次读取 `id=99` 的记录时，读到的记录为空，说明数据不存在记录。随后，事务 A 在第 4 步插入了一条 `id=99` 的记录并提交。事务 B 在第 6 步再次读取 `id=99` 的记录时，读到的记录仍然为空，但是，事务 B 在第 7 步试图更新这条不存在的记录时，竟然成功了，并且，事务 B 在第 8 步再次读取 `id=99` 的记录时，记录出现了。
     
          可见，幻读就是没有读到的记录，以为不存在，但其实是可以更新成功的，并且，更新成功后，再次读取，就出现了。
     
       - Serializable：串行化
     
         Serializable 是最严格的隔离级别。在 Serializable 隔离级别下，所有事务按照次序依次执行，因此，脏读、幻读等现象。
     
         虽然 Serializable 隔离级别下的事务具有最高的安全性，但是，由于事务是串行执行，所以效率会大大下降，应用程序的性能会急剧降低。如果没有特别重要的场景，一般都不会使用 Serializable 隔离级别。
     
         如果数据库没有指定隔离级别，InnoDB 默认的隔离级别是 Repeatable Read。
     
           > 相当于多线程中的锁，一个事务在操作的时候，其他事务不能进行操作，可以解决所有的问题，但是效率相当低
           >
           > 脏读是读取到了修改的数据
           >
           > 幻读是读取到了新增的数据

2. **什么是索引？索引的优缺点，什么字段上建立索引**

   > 面试题：
   >
   > 数据库中最常见的慢查询优化方式是什么？
   >
   > 为什么加索引能优化慢查询？
   >
   > 你知道哪些数据结构可以提高查询速度吗？
   >
   > 那这些数据结构既然都能优化查询速度那 MySQL 为什么会选择使用 B + 树呢？

   索引：索引是帮助 MySQL 高效获取数据的数据结构，有点像我们平时用到的 hash，可以通过 key 极快的找到对应的值

   在关系型数据库中，我们创建的索引也是需要存储在磁盘中的。如果我现在想读取 1 个字节数据，它并不是在磁盘中只取一个字节，而是取某一段连续的区域，这段连续的区域就叫 ”页“。

   页是存储器的逻辑块，操作系统往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（在许多操作系统中页大小通常为4k）

   > 主存和磁盘以页为单位交换数据。磁盘预读的时候一般长度为页的整数倍

   能快速获取数据的数据结构有很多，比如 O (1) 时间复杂度的 hash 就能快速的获取数据，假设这个 hash 表是个 8 位数组长度，那我 %8（取模运算）就会有个 0-7 的余数，就可以把数据放到对应的下标上去，index 相同的话下面就还会有个链表，HashMap 在 1.7 和 1.8 的实现还不一样。

   但是 hash 表有一定的缺点：

   i. 利用 hash 存储的话需要将所有的数据文件添加到内存比较耗费内存空间，内存空间非常的宝贵

   ii. 如果所有的查询都是等值查询，那么 hash 确实很快，但是在实际工作中更多的是范围查找数据，而不是等值查询，因此 hash 就不太适合了

   而无论是二叉树还是红黑树，它们都会因为树的深度过深而造成 IO 次数变多，影响数据读取的效率

   所以 MySQL 索引文件最终采用的数据结构是 B+ 树

   > 优点：
   >
   > - 极大的减少存储引擎需要扫描的数据量
   > - 能将随机 IO 变成顺序 IO
   > - 能够帮助我们进行分组、排序等操作

   **索引的分类**

   - 主键索引：主键是一种唯一性索引，但它必须指定为 RIMARY KEY，每个表只能有一个主键（不表示一个列，还有联合主键的存在）。
   - 唯一索引：索引列的所有值都只能出现一次，即必须唯一，值可以为空。
   - 普通索引：基本的索引类型，值可以为空，没有唯一性的限制。
   - 全文索引（使用频率低）：全文索引的索引类型为 FULLTEXT。全文索引可以在 varchar、char、text类型的列上创建（相当于是存了一篇 2G 的文章，效率比较低，不建议使用。换用 ES来做全文索引）
   - 组合索引：多列值组成一个索引，专门用于组合搜索（查询的时候经常使用 `name` 和 `age` 列，可以把它们俩做一个组合索引）

   聚簇索引：指多个字段上创建的索引，只有在查询条件中使用了创建索引时的第一个字段，索引才会被使用。所以它是遵循最左前缀法则的

   > 索引不是越多越好，因为索引最终也需要存储在磁盘中，在查询的时候意味着我们会从磁盘读取数据到内存的一个过程。索引变大以后从磁盘往内存读的效率就会变慢
   >
   > 在插入数据的时候需要对索引进行维护，就会有一个页分裂（一个页数据装不下了）和页合并（一个页数据比较少会触发合并）的问题
   >
   > 普通索引，比如不是主键的 name 列字段

   覆盖索引：用覆盖索引操作了的话，就不需要进行回表，因为 id 就是主键列（主键索引）。这个过程就叫覆盖索引（就是我建立了 3 个索引，查询的时候也按照 3 个索引的顺序在 where 后查询，而不用 select *）

   ```sql
   select * from user where name='lin';
   select id from user where name='lin';
   ```

   组合索引：会引出最左匹配原则

   ```sql
   select * from user where name='lin'; -- 会用到索引
   select * from user where name='lin' and age=10; -- 会用到索引
   select * from user where age=10; -- 不会用到索引
   select * from user where age=10 and name='lin'; -- 会用到索引（MySQL 里面的优化器会修改顺序）
   ```

6. **存储引擎 InnoDB 与 MyISAM 区别**

   |              | MyISAM                                                       | InnoDB                                   |
   | ------------ | ------------------------------------------------------------ | ---------------------------------------- |
   | 索引类型     | 非聚簇索引（索引和文件存储在不同文件中，MyISAM 是分开存储在了两个文件中） | 聚簇索引（索引和数据存储在同一个文件中） |
   | 支持事务     | 否                                                           | 是                                       |
   | 支持表锁     | 是                                                           | 是                                       |
   | 支持行锁     | 否                                                           | 是                                       |
   | 支持外键     | 否                                                           | 是                                       |
   | 支持全文索引 | 否                                                           | 是（5.6 以后支持）                       |
   | 适合业务类型 | 大量 select 查询                                             | 大量 insert、delete、update 操作         |
   
   区别与使用场景：
   
   - InnoDB 是事务安全的，支持行级锁定，包括ACID事务支持。如果应用中需要执行大量的INSERT或UPDATE操作，则应该使用InnoDB，这样可以提高多用户并发操作的性能
   
   - MyISAM 是非事务安全的，支持表级锁定，并且支持全文索引。如果应用中需要执行大量的 SELECT 查询就用 MyISAM
   
     http://blog.haohtml.com/wp-content/uploads/2017/01/timg.jpg
   
     InnoDB 使用的是聚簇索引，将主键组织到一棵 B + 树中，而行数据就储存在叶子节点上，若使用”where id = 14″这样的条件查找主键，则按照 B + 树的检索算法即可查找到对应的叶节点，之后获得行数据。若对 Name 列进行条件搜索，则需要两个步骤：第一步在辅助索引 B + 树中检索 Name，到达其叶子节点获取对应的主键。第二步使用主键在主索引 B + 树中再执行一次 B + 树检索操作，最终到达叶子节点即可获取整行数据。
   
     MyISM 使用的是非聚簇索引，非聚簇索引的两棵 B + 树看上去没什么不同，节点的结构完全一致只是存储的内容不同而已，主键索引 B + 树的节点存储了主键，辅助键索引 B + 树存储了辅助键。表数据存储在独立的地方，这两颗 B + 树的叶子节点都使用一个地址指向真正的表数据，对于表数据来说，这两个键没有任何差别。由于索引树是独立的，通过辅助键检索无需访问主键的索引树。
   
4. **explain分析语句（itkr）**

    - ==id 字段三种情况介绍==

      - 在 id 相同情况下：关联查询，执行的顺序由 table 字段上至下的（t1>t3>t2）

        ![img](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/ef75044f-246a-40c1-b2b7-faf323b6f34d-877132.jpg)

      - 在 id 不同的情况下：子查询，id 的序号会递增，id 值越大优先级越高，越先被执行（t3>t1>t2）

        ![img](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/dc10b860-29e8-4d00-91b4-cb79360ad708-877132.jpg)

      - 在 id 既有相同又有不同的情况下：id 如果相同，可以认为是一组，从上往下顺序执行，在所在组中，id 值越大，优先级越高，越先执行 DERIVED 表示衍生，意思是 s1 是通过 t3 衍生得来的。（t3> `<derived2>` > t2

        ![img](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/bdda6cdc-5b73-4e64-ae35-11a1c938ca77-877132.jpg)

    - select_type 的类型

      - SIMPLE：简单的 select 查询，查询中不包含子查询或者 union
      - PRIMARY：查询中若包含任何复杂的子查询部分，最外层的查询（最后执行的语句）就是 PRIMARY
      - SUBQUERY：在 select 或 where 列表中包含了子查询
      - DERIVED：在 FROM 列表中包含了子查询被标记为 DERIVED(衍生)，MySQL 会递归执行这些子查询，把结果放在临时表里
      - UNION：若第二个 SELECT 出现在 UNION 之后，则被标记为 UNION；若 UNION 包含在 FROM 子句的子查询中，外层 SELECT 将被标记为：DERIVED
      - UNION RESULT：从 UNION 表中获取结果的 SELECT

    - ==type 查询类型介绍==

      - system：表只有一条记录，等同于系统表，可以忽略不计

      - const：表示通过唯一索引一次就找到了，const 用于比较 primary key 或 unique 索引

      - eq_ref：唯一索引扫描，对于每个索引键，表中只有一条记录与之匹配，常见于主键或唯一索引扫描（把你们公司的程序员找出来，出来了 10 个。再把架构师也找来，这时只有 1 个）

      - **ref：**非唯一索引（包括主键索引）扫描，返回匹配某个单独值的所有行，它可能会找到多个符合条件的行，所以属于查询和扫描的混合体

        ```bash
        mysql> create index idx_name on students(name); # 给 id 和 name 列创建索引
        Query OK, 0 rows affected (0.03 sec)
        Records: 0  Duplicates: 0  Warnings: 0
        mysql> explain select id from students where name='Alice'; # 查出所有名字为 'Alice' 的记录
        +----+-------------+----------+------------+------+---------------+-------------+---------+-------+------+----------+-------------+
        | id | select_type | table    | partitions | type | possible_keys | key         | key_len | ref   | rows | filtered | Extra       |
        +----+-------------+----------+------------+------+---------------+-------------+---------+-------+------+----------+-------------+
        |  1 | SIMPLE      | students | NULL       | ref  | idx_id_name   | idx_id_name | 768     | const |    2 |   100.00 | Using index |
        +----+-------------+----------+------------+------+---------------+-------------+---------+-------+------+----------+-------------+
        1 row in set, 1 warning (0.00 sec)
        ```

      - **range：**只检索给定范围的行，使用一个索引来选择行，key 列显示使用了哪个索引一般就是在你的 where 语句中出现了 between，<、>、in 等查询这种范围扫描索引比全表扫描要好，因为它只需要开始于索引的某一点，结束于另一点，不用全部索引

        ```sql
        mysql> explain select * from students where id>1;
        +----+-------------+----------+------------+-------+---------------+---------+---------+------+------+----------+-------------+
        | id | select_type | table    | partitions | type  | possible_keys | key     | key_len | ref  | rows | filtered | Extra       |
        +----+-------------+----------+------------+-------+---------------+---------+---------+------+------+----------+-------------+
        |  1 | SIMPLE      | students | NULL       | range | PRIMARY       | PRIMARY | 4       | NULL |    1 |   100.00 | Using where |
        +----+-------------+----------+------------+-------+---------------+---------+---------+------+------+----------+-------------+
        1 row in set, 1 warning (0.01 sec)
        ```

      - index：index 与 all 的区别为 index 类型只遍历索引树。这通常比 all 快，因为索引文件通常比数据文件小也就是说虽然 all 和 index 都是读全表，但 index 是从索引中读取的，而 all 是从硬盘中读的

      - all：全表扫描，当数据达到百万级时，性能会有所下降，这时需要和架构师根据业务商量是否上索引

        > 从最好到最差依次是
        > system > const > eq_ref > ref > range > index > all（全表扫描）
        >
        > 一般来说我会保证查询至少达到 range 级别，要是能达到 ref 就更好

    - possible_keys介绍

      可能应用在这张表中的索引，可能有一个或多个，但不一定全部都被使用

    - ==key 介绍==

      实际使用的索引，如果为 NULL，则没有使用索引查询

      若使用了覆盖索引，则该索引仅出现在 key 列表中 possible_keys 只是 MySQL 理论上推测应该用到的索引，但实际上应该以 key 用到的索引为准

    - key_len 介绍

      表示索引中使用的字节数，可通过该列计算查询中使用的索引长度，这个值越短越好

    - ref 介绍

      显示索引的哪一列被使用了（`where name='Alice'` 代表 const 常量类型） 

    - ==rows==

      估算出找到结果所需要读取的行数，这个值是越小越好

    - extra

      using filesort：这个说明 MySQL 会对数据使用一个外部的索引排序，而不按照表内的索引顺序进行读取
      
      using temporary：这意味着 MySQL 对查询结果进行排序的时候使用了一张临时表。常见于 order by 和 group by
      
      using index：出现这个说明 MySQL 使用了覆盖索引，避免访问了表的数据行，效率不错

2. **索引失效**

    > 为staffs表建立了复合索引，索引列为name、age、pos

    ![img](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/cb4572c4-c85e-4eeb-9cbe-e04a56bb4b7c-877132.jpg)

    - 最佳左前缀法则

      - 依次执行三条查询SQL，索引情况正常，都使用到了

        按照索引建立的顺序依次进行查询

        ![img](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/c6e30510-90c1-4246-9f17-1fcd3ed51e6e-877132.jpg)

      - 出现索引失效情况

        未按照索引建立的顺序，跳过第一个索引name，直接使用后面两个查询

        ![img](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/0367427b-ea74-4008-933e-236bf9c234b1-877132.jpg)

      - 总结
        
        ==如果索引了多列，查询要遵守最左前缀法则。（指的是查询从索引的最左前列开始并且不要跳过索引中的列）==

    - 不要在索引列上做任何（计算、函数、（自动or手动）类型转换）操作，会导致索引失效而转向全表扫描

      在第二条sql中对name索引列查询时使用了left函数，则导致了索引失效

      ![img](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/4de861be-977e-43e2-99e9-8478dc2b433c-877132.jpg)

    - 存储引擎不能使用索引中范围条件右边的的列

      当索引条件中出现了 >、<、in 一类的范围条件时，会导致索引失效

      ![img](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/1d342ffd-16cd-4768-b302-1997d3935f7d-877132.jpg)

    - 尽量使用覆盖索引，只访问索引查询，减少 select *，而是查询需要查询的列 `select id,name` 代替

    - MySQL 中使用 is null 或者 is not null，会导致索引失效（就是 explain 出来的 key 那列会为 null）

    - like以通配符开关 '%adb'，会导致索引失效

      - 当使用like查询时，只 % 在最右边时，MySQL才会使用到索引

        ![img](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/fda5e070-a0a2-4842-9015-37f20388454c-877132.jpg)

      - 如果必须要使用 like %abd%，的解决方案，使用覆盖索引，建立的索引和查询的字段在顺序，个数上最好保持一致

        ![img](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/7d26f956-61b3-41e2-ac6b-0e9c990ae14b-877132.jpg)

    - 字符串不加单引号索引失效

      当 name 查询条件值跟的是数字时，如果不加单引号，MySQL 会自动隐式转换为数字

      ![img](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/a2e7cac8-a6b5-4a8e-9774-b5819d4bee32-877132.jpg)

    - 少用or，用它会导致索引失效

6. **SQL优化**

   https://www.cnblogs.com/Little-Li/p/8031295.html

   i. 尽量不要在 where 子句中对字段进行表达式操作或是进行 null 值判断，这样会导致全表索

   ii. like 也将导致全表扫描，只在 % 在最右边时才会走索引

   iii. 很多时候用 exists 代替 in

   iv. 一个表的索引数最好不要超过 5 个。会导致索引越来越大，检索的时候 IO 也会成指数级增大。

   > 索引并不是越多越好，索引固然可以提高相应的 select 的效率，但同时也降低了 insert 及 update 的效率，因为 insert 或 update 时有可能会重建索引

   v. 强制类型转换时会造成全表扫描（varchar 类型没有加单引号的话会导致索引失效；而且还会导致行锁变表锁）

   vi. 频繁更新的字段不适合做索引，因为更新了记录后还需要进行维护索引

   ```sql
   explain select I from user where phone=19121945219; -- key 索引列为 null
   explain select I from user where phone='19121945219'; -- key 正常索引成功
   ```

   vi. 如果索引了多列，查询要遵守最左前缀法则

   当使用索引列进行查询的时候尽不要使用表达式，而是把计算操作放到业务层而不是数据库层

   > 全值匹配我最爱，最左前缀要遵守；
   > 带头大哥不能死，中间兄弟不能断；
   > 索引列上少计算，范围之后全失效；
   > Like 百分写最右，覆盖索引不写星；
   > 不等空值还有 or，索引失效要少用；
   > VAR 引号不可丢，SQL 高级也不难！
   >
   > ```
   > 2-1 数据准备 (02:49)
   > 2-2 MySQL 慢查日志的开启方式和存储格式 (05:54)
   > 2-3 MySQL 慢查日志分析工具之 mysqldumpslow (04:32)
   > 2-4 MySQL 慢查日志分析工具之 pt-query-digest (07:00)
   > 2-5 如何通过慢查日志发现有问题的 SQL (02:47)
   > 2-6 通过 explain 查询和分析 SQL 的执行计划 (04:09)
   > 2-7 Count () 和 Max () 的优化 (07:02)
   > 2-8 子查询的优化 (03:21)
   > 2-9 group by 的优化 (03:42)
   > 2-10 Limit 查询的优化 (06:06)
   > ```

4. **请简述项目中优化 SQL 语句执行效率的方法，从哪些方面？SQL 语句性能如何分析？**

   - 查找分析查询速度慢的原因（分析 SQL 查询慢的方法）

     - 记录慢查询日志：会把所有查询慢的 SQL 都记录下来

       不建议直接打开慢查询日志进行分析，这样比较浪费时间和精力。可以直接使用 `pt-query-digest` 工具进行分析

     - 使用 show profile

       `set profiling = 1;` 开启后，服务器上执行的所有语句会检测其消耗时间并存到临时表中

       show profiles

       show profile for query [临时表 ID]：还可以通过临时表 ID 去查看 SQL 语句每一步慢在哪

     - 使用 show status

       show status 会返回一些计数器（就是指令执行的次数）

       show global status 可以查看服务器级别的所有计数器

       有时可以根据这些计数，可以猜出哪些操作代价较高或者消耗时间多

     - 使用 show processlist

       观察是否有大量线程处于不正常的状态或特征

     - 使用 explain/desc

       分析单条 SQL 语句

       ```sql
       explain select * from user\G
       ```

   - 优化查询过程中的数据访问

     - 避免使用如下 SQL 语句

       i. 查询不需要的记录，经过条件筛选了有 1000 条数据，而我们只需要 100 条。可以使用 limit 解决

       ii. 多表关联不要返回全部列，要指定列名，比如 A.id，B.name 等等

       iii. 不要总是取出全部列，select * 会让优化器无法完成索引覆盖扫描的优化

       iv. 经常查询的数据可以让它走缓存，不用每次都重新查询

       v. 是否存在扫描了额外的记录

       ​	使用 explain 进行分析的时候，如果发现查询需要扫描 200 行数据但只返回了 10 条数据，优化：

       ​	使用索引覆盖扫描，把所有用的列都放到索引中，这样存储引擎就不需要回表获取对应行就可以返回结果了

       vi. 修改数据表范式并改变数据库和表结构。（用空间来换取时间，比如第三范式为关联了其他表，效率可能偏低一点，那可以把范式降低。比如我经常去查询一张表，同时这种表关联了另一张表的一个字段，我们可以考虑在第一个表里加入这个字段为冗余字段）

     - 优化长难的复杂查询

       1. 一个复杂查询还是多个简单查询

          MysQ内部每秒能扫描内存中上百万行数据，相比之下，响应数据给客户端就要慢得多

          使用尽可能少的查询是好的，但是有时将一个大的查询分解为多个小的查询是很有必要的

       2. 切分查询

          将一个大的查询分为多个小的相同的查询

          一次性删除1000万的数据要比一次删除1万，暂停一会的方案更加损耗服务器开销（不然在这一直进行删除操作的话，会导致这张表会锁住）

       3. 分解关联查询

          可以将一条关联语句分解成多条SQL来执行，让缓存的效率更高，执行单个查询可以减少锁的竞争，在应用层做关联可以更容易对数据库进行拆分

     - 优化特定类型的查询语句

       1. 优化 count() 查询

          `count(*)` 会忽略所有的列，直接统计所有列数，因此不要使用 count(列名)，没有 `count(*)` 快

          MyISAM 中，没有任何 WHERE 条件的 `count(*)` 会非常的快

       2. 优化关联：

          i. 查询确定 ON 或 USING 者子句的列上是否有索引，没有的话会导致全表扫描

          ii. 确保 GROUP BY 和 ORDER BY 中只有一个表中的列，这样 MySQL 才有可能使用索引，不要既 GROUP BY/ORDER BY 表 1，又 GROUP BY/ORDER BY 表 2

       3. 子查询：尽量不用子查询，用关联查询来替代

       4. 优化 GROUP BY 和 DISTINCT：

          这两种查询均可使用索引来优化，是最有效的优化方法

          关联查询中，使用标识列进行分组的效率会更高

          如果不需要 ORDER BY，进行 GROUP BY 时使用 `ORDER BY null`, 这样 MySQL 不会再进行文件排序，否则会造成性能消耗

          WITH ROLLUP 超级聚合，可以挪到应用程序中处理

       5. 优化 LIMIT 分页

          LIMIT 偏移量大的时候，查询效率较低。可以记录上次查询的最大 ID，下次查询时直接根据该 ID 来查询（比如 limit 0 100 where [ID] > [上次查询最大 ID]

       6. 优化 UNION：查询 UNION ALL 的效率高于 UNION

5. **SQL注入问题**

   在拼接 sql 时，有一些 sql 的特殊关键字参与字符串的拼接会造成安全性问题 a' or 'a' = 'a 最终是个恒等式。可以用 PreparedStatement 对象来解决，“？” 作为参数占位符

9. **怎么实现锁表？数据库锁**

   [MySQL_基础 + 高级篇 - 数据库 -sql -mysql 教程_mysql 视频_mysql 入门_尚硅谷_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV12b411K7Zu?p=232)

   > MyISAM 执行 select 语句前，会自动给涉及的所有表加读锁；而在执行增删改操作前，会自动给涉及的表加写锁。
   >
   > InnoDB 存储引擎在整体并发处理能力上要远远优于 MyISAM 的表级锁定，但使用不当的话性能表现可能比 MyISAM 更差，比如产生间隙锁的时候，会从行锁变为表锁

   对数据操作类型来分为读锁和写锁：<font color='red'>读锁会阻塞写，但不会阻塞读操作；写锁会把读写都阻塞</font>

   - 读锁（共享锁）：对于同一份数据，多个线程来同时读互相不会影响

     如果 session1 的一个表加了读锁，那么 session1 可以读取加了读锁的表，但不能更新和修改锁定的表和其他表，也不能读取别的表。但是 session2 可以对 session1 加了读锁的表进行读操作，但是 session2 要进行更新锁定表时会一直等待知道获得锁，在 session1 释放锁后，session2 才可以完成插入/修改操作

   - 写锁（排它锁）：在写操作完成前，会阻断其它锁的读锁和写锁

     当 session1 的一个表加了写锁后，session1 就可以对锁定的表进行查询/更新/插入操作，session2 对锁定表的查询操作就会阻塞，直到 session1 释放锁，session2 查询成功

   对数据操作的粒度来分为表锁和行锁

   - 表锁（偏读）

     特点：偏向于 MyISAM，开销小，加锁快；无死锁；锁的粒度大，冲突度最低（没有人竞争），导致并发度也最低

     表锁分析：

     ```sql
     mysql> show status like 'table%';
     +----------------------------+-------+
     | Variable_name              | Value |
     +----------------------------+-------+
     | Table_locks_immediate      | 115   |
     | Table_locks_waited         | 0     |
     | Table_open_cache_hits      | 3     |
     | Table_open_cache_misses    | 3     |
     | Table_open_cache_overflows | 0     |
     +----------------------------+-------+
     5 rows in set (0.01 sec)
     ```

     Table_locks_immediate：产生表级锁定的次数，表示可以立即获取锁的查询次数，每立即获取锁值加1

     Table_locks_waited：出现表级锁定争用而发生等待的次数（不能立即获取锁的次数，每等待一次锁值加1），此值高表示存在着严重的表级锁竞争的情况

     MyISAM 的读写锁是优先写锁，所以它<font color='red'>不适合作为写为主的表引擎</font>，因为别的线程不能做任何操作，大量进行更新的话会造成读操作获取不到锁。从而造成阻塞（像淘宝也肯定买家库和卖家库是分开的，买家库经常浏览偏向于读，而卖家库经常进行编排偏向于写）

   - 行锁（偏写）

     偏向 InnoDB 存储引擎，开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低（你用第 38 行，我用 20 行，互相并没有交集），并发度也最高
     
     间隙锁：当我们用范围条件而不是相等条件检索数据，并请求共享或排他锁时， InnoDB 会给符合条件的已有数据记录的索引项加锁；对于键值在条件范围内但并不存在的记录，叫做 “间隙（GAP）”。（比如在操作 `where a>1 and a<5` 范围内的数据时，这里面是没有第二条记录的。那么别的 session 来插入第二条记录时会发生阻塞）
     
     如何分析行锁定？
     
     ```bash
     mysql> show status like 'innodb_row_lock%';
     +-------------------------------+-------+
     | Variable_name                 | Value |
     +-------------------------------+-------+
     | Innodb_row_lock_current_waits | 0     |
     | Innodb_row_lock_time          | 0     |
     | Innodb_row_lock_time_avg      | 0     |
     | Innodb_row_lock_time_max      | 0     |
     | Innodb_row_lock_waits         | 0     |
     +-------------------------------+-------+
     5 rows in set (0.01 sec)
     # 重要
     # Innodb_row_lock_time_avg（等待平均时长）每次等待所花平均时间
     # Innodb_row_lock_waits（等待总次数）
     # Innodb_row_lock_time（等待总时长）从系统启动到现在锁定总时间长度
     # 非重要
     # Innodb_row_lock_current_waits：当前正在等待锁定的数量
     # Innodb_row_lock_time_max：从系统启动到现在等待最常的一次所花的时间
     ```
     
     尤其是当等待次数很高，而且每次等待时长也不小的时候，我们就需要分析系统中为什么会有如此多的等待，然后根据分析结果着手指定优化计划。
     
     ```bash
     show open tables # 查看表上加过哪些锁，有 1 的代表加了锁，0 表示没加锁
     lock table [表名] read/write # 给表加读锁或写锁
     unlock tables # 给表解锁
     ```

7. **如何锁定一行**

   ```sql
   begin;
   select xxx for update; -- 锁定某一行后，其它 session 的操作会被阻塞，知道当前 session commit
   commit;
   ```

8. **常见的数据库优化手段**

   - 表的设计要规范，选取最适用的字段属性
   - 适当建立索引，在频繁作为检索条件，更新较少的字段上建立索引，以提高查询速度
   - 分表查询，有水平分割、垂直分割
   - 读写分离，读 (read)、写 (create、update、delete)
   - 使用 explain 来优化查询语句

9. **数据库连接池。**

13. **Durid的常用配置**

14. **设计一张员工部门表**

     emp_no
     birth_date
     name
     gender
     hire_date

15. [获取每个部门中当前员工薪水最高的相关信息_牛客题霸_牛客网](https://www.nowcoder.com/practice/4a052e3e1df5435880d4353eb18a91c6?tpId=82&&tqId=29764&rp=1&ru=/ta/sql&qru=/ta/sql/question-ranking)

     ```sql
     drop table if exists  `dept_emp` ; 
     drop table if exists  `salaries` ; 
     CREATE TABLE `dept_emp` (
     `emp_no` int(11) NOT NULL,
     `dept_no` char(4) NOT NULL,
     `from_date` date NOT NULL,
     `to_date` date NOT NULL,
     PRIMARY KEY (`emp_no`,`dept_no`));
     CREATE TABLE `salaries` (
     `emp_no` int(11) NOT NULL,
     `salary` int(11) NOT NULL,
     `from_date` date NOT NULL,
     `to_date` date NOT NULL,
     PRIMARY KEY (`emp_no`,`from_date`));
     INSERT INTO dept_emp VALUES(10001,'d001','1986-06-26','9999-01-01');
     INSERT INTO dept_emp VALUES(10002,'d001','1996-08-03','9999-01-01');
     INSERT INTO dept_emp VALUES(10003,'d004','1995-12-03','9999-01-01');
     INSERT INTO dept_emp VALUES(10004,'d004','1986-12-01','9999-01-01');
     INSERT INTO dept_emp VALUES(10005,'d003','1989-09-12','9999-01-01');
     INSERT INTO dept_emp VALUES(10006,'d002','1990-08-05','9999-01-01');
     INSERT INTO dept_emp VALUES(10007,'d005','1989-02-10','9999-01-01');
     INSERT INTO dept_emp VALUES(10009,'d006','1985-02-18','9999-01-01');
     INSERT INTO dept_emp VALUES(10010,'d006','2000-06-26','9999-01-01');
     
     INSERT INTO salaries VALUES(10001,88958,'2002-06-22','9999-01-01');
     INSERT INTO salaries VALUES(10002,72527,'2001-08-02','9999-01-01');
     INSERT INTO salaries VALUES(10003,43311,'2001-12-01','9999-01-01');
     INSERT INTO salaries VALUES(10004,74057,'2001-11-27','9999-01-01');
     INSERT INTO salaries VALUES(10005,94692,'2001-09-09','9999-01-01');
     INSERT INTO salaries VALUES(10006,43311,'2001-08-02','9999-01-01');
     INSERT INTO salaries VALUES(10007,88070,'2002-02-07','9999-01-01');
     INSERT INTO salaries VALUES(10009,95409,'2002-02-14','9999-01-01');
     INSERT INTO salaries VALUES(10010,94409,'2001-11-23','9999-01-01');
     ```

     获取每个部门中当前员工薪水最高的相关信息，给出 dept_no, emp_no 以及其对应的 salary，按照部门编号升序排列

     ```sql
     SELECT
     	d.dept_no,
     	d.emp_no, -- 这里是错误的，其实不能写 d.emp_no，它既不是分组字段，也不是聚合操作，原则上查询的字段只能是 GROUP BY 字段或其他的聚合操作。比如这里就只能查询 de.dept_no 和 MAX( s.salary ) salary
     	MAX( s.salary ) salary 
     FROM
     	dept_emp d,
     	salaries s 
     WHERE
     	d.emp_no = s.emp_no 
     GROUP BY
     	d.dept_no 
     ORDER BY
     	d.dept_no
     ```

    |      | 你的输出             |      | 期望输出           |
    | ---- | -------------------- | ---- | ------------------ |
    | ...  | 导致第四行答案不一致 |      |                    |
    | 4    | d004\|10003\|74057   | 4    | d004\|10004\|74057 |

     查询每个部门中薪水最高值，显示部门和薪水最高值，这没什么问题，但如果要找出薪水最高值对应的员工，按照规范，在 SELECT 中无法直接写员工的。但若不遵守规范也是可以的，直接在 SELECT 中写上其他字段（非分组字段、非聚合），其含义就是找出聚合结果对应的其他字段的值，这其实是省略了一个子查询。以下才正确

     ```sql
     SELECT
     	r.dept_no,
     	ss.emp_no,
     	r.maxSalary 
     FROM
     	(
     	SELECT
     		d.dept_no,
     		max( s.salary ) maxSalary 
     	FROM
     		dept_emp d,
     		salaries s 
     	WHERE
     		d.emp_no = s.emp_no 
     	GROUP BY
     		d.dept_no 
     	) r,
     	salaries ss,
     	dept_emp dd 
     WHERE
     	r.maxSalary = ss.salary 
     	AND r.dept_no = dd.dept_no 
     	AND dd.emp_no = ss.emp_no 
     ORDER BY
     	r.dept_no ASC
     ```

     

### 计算机网络（弱）

[网络传输的基础是什么？——交换机科普_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1kQ4y1a7Je/?vd_source=4844de7cb051be29fbaf4555af0bbd8b)

[交换机和路由器有什么区别？网关和路由又是什么意思？简单说网络2-交换机与路由器_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1fU4y1t7Ju/?vd_source=4844de7cb051be29fbaf4555af0bbd8b)

1. OSI 七层协议（TCP/IP 协议）

   - 物理层
   - 数据链路层
   - 网络层
   - 传输层（SNAT 修改的是到这层的源 IP+源端口）
   - 应用层

   > TCP/IP 规定，不同的子网之间不能直接通信，必须通过网关

### Java WEB（弱）

1. **cookie 和 session 的区别 **  

   - cookie 数据保存在客户端（最大存 4k），session 数据保存在服务端
   - session 是保存在服务器端的，每个用户都会产生一个 session。并发访问的用户十分多的话会产生非常多的 session，耗费大量的内存，考虑到减轻服务器的压力，可以将不重要的数据放在 cookie 中持久的保存，平时也用来记录用户的一些信息
   - session 的运行依赖 session id，而 session id 是存在 cookie 中的，也就是说如果浏览器禁用了 cookie，同时 session 也会失效（重写 URL，可以在 URL 中传递 session_id）

2. **Get 和 Post 请求方式的区别？**  

   - Get 请求是幂等的，无论发送多少次请求结果都应该是一致的；Post 请求是在要修改服务端数据时使用的

     > 因为这种我在网上看到说发生过事故，如果是 `<a href="/delete/xxx"` 浏览器在访问的时候会进行遍历，不知不觉的就把数据删除了

   - get 地址栏有参数显示；post 不会再地址栏显示参数 (参数是放在了请求体）

   - get 不安全会限制大小；post 相对安全而理论上不限制大小

2. **Servlet 的生命周期 **  

   - 第一次请求时 Servlet 被初始化且只初始化一次。所以 tomcat 容器中每一个 servlet 只有一个对象存在
   - 初始化后先调用 init 方法，只执行一遍
   - 每个请求，调用一遍 service -> service -> doGet/doPost。以多线程的方式运行
   - 卸载前调用 destroy 方法
   
4. **重定向和转发的区别？**  

   - 重定向是客户端行为，转发是服务器端行为
   - 重定向两次请求两次响应，转发一次请求一次响应
   - 重定向路径需要加工程名，转发的路径不需要加工程名.
   - 重定向可以跳转到任意网站，转发只能在服务器内部进行转发.
   - 重定向会导致 request 对象信息丢失。转发则不会

2. <span id='session'>**session 共享怎么做的（分布式如何实现 session 共享）**</span>

   - 可以使用 tomcat 广播机制实现 session 共享
   - 可以使用 Redis+tomcat 实现 session 共享
   - 可以使用 Spring session 完成 session 共享

3. **Servlet 是安全的吗？**

   是线程不安全的，因为 servlet 是单例模式，当多个客户端共同访问的时候线程不安全。
   尽量用局部变量，同步块，如果当前字段是不会改变的，用 final 修饰

4. **Token**

   - 客户端通过登录请求提交用户名和密码，服务端验证通过后生成一个 Token 与该用户进行关联，并将 Token 返回给客户端
   - 客户端在接下来的请求中都会携带 Token，服务端通过解析 Token 检查登录状态
   - 当用户退出登录、其他终端登录同一账号（被顶号）、长时间未进行操作时 Token 会失效，这时用户需要重新登录

### 设计模式

1. **单例模式，有五种写法，可以参考文章单例模式的五种实现方式**

   what：保证一个类仅有一个实例，并提供全局访问点

   why：

   i. 在内存里只有一个实例，减少了内存开销。特别是那种需要频繁的创建而创建的过程又无法优化的对象

   ii. 避免对资源的多重占用

   iii. 通过私有构造方法保证别的对象无法创建改对象

   **饿汉式：**

   优点：类加载的时候就完成了初始化，避免了线程安全问题

   缺点：没有延迟加载的效果，如果这个类从始至终都没有用过，会造成系统资源的浪费（用懒汉式替代）

   > 懒汉式的 instance 不能修饰为 final 的，因为在修饰前没有初始化好，设为 final 之后就不能修改了
   >
   > 饿汉式用 final 的 instance 实例的话，就得用 `static { instance = new HungrySingleton();}` 静态代码块先完成初始化，并把成员变量 instance 修改为 `private static final HungrySingleton instance` 即可

   ```java
   public class HungrySingleton {
       private static final HungrySingleton instance = new HungrySingleton(); // 类加载的时候就初始化了，final 修饰不可更改
       public HungrySingleton() { }
       public HungrySingleton getInstance() { return instance; }
   }
   ```

   <span id='singleton'>**懒汉式单例（普通版）：**</span>

   ```java
   /**
    * 懒汉模式
    */
   public class LazySingleton {
   
       // 这里用 static是因为 getInstance() 方法是静态的，而静态方法不能访问非静态成员变量，所以instance 必须是静态成员变量
       // getInstance() 方法是静态是因为构造器是私有的，只能通过 Singleton.getInstance() 方法获取对象实例
       private static LazySingleton instance = null;
   
       // 构造器是私有是为了防止其他类通过 new LazySingleton() 来创建对象实例
       private LazySingleton() {
           System.out.println(Thread.currentThread().getName() + " --- 我是构造方法");
       }
   
       public static LazySingleton getInstance() {
           if (instance == null) {
               instance = new LazySingleton();
           }
           return instance;
       }
   
   
       public static void main(String[] args) {
           // 单线程（mian 线程）情况下只构造了一个对象，没有线程安全问题
   //        System.out.println(SingletonDemo.getInstance() == SingletonDemo.getInstance());
   //        System.out.println(SingletonDemo.getInstance() == SingletonDemo.getInstance());
   
           // 打印出多行，说明构造了多个对象，会出现线程安全问题
           for (int i = 1; i <= 10; i++) {
               new Thread(() -> {
                   Singleton.getInstance();
               }, String.valueOf(i)).start();
           }
           
           // 实现 Runnable 方式测试
           // Thread t1 = new Thread(new T());
           // Thread t2 = new Thread(new T());
           // t1.start();
           // t2.start();
       }
   }
   
   class T implements Runnable {
   
       @Override
       public void run() {
           LazySingleton instance = LazySingleton.getInstance();
           System.out.println(Thread.currentThread().getName() + " " + instance);
       }
   }
   ```

   普通版懒汉式会造成线程安全问题，接着需要优化为 [DCL](#DCL) 单例。

   **DCL 双重校验锁单例：**

   ```java
   private static volatile LazySingleton instance = null;
   //...
   public static LazySingleton getInstance() {
       if (instance == null) {
           synchronized (LazySingleton.class) {
               if (instance == null) {
                   instance = new LazySingleton();
               }
           }
       }
       return instance;
   }
   ```

   JDK 中的应用

   - Runtime 类（饿汉式）

     ```java
     private static Runtime currentRuntime = new Runtime();
     public static Runtime getRuntime() {
         return currentRuntime;
     }
     ```

   Spring 中的应用：在 `public abstract class AbstractFactoryBean` 有个 `getObject()` ，会去判断单例是否已经初始化，没有的话会获取早期单例对象，早期单例对象为空的话就通过动态代理创建出来返回

   ```java
   @Override
   public final T getObject() throws Exception {
   	if (isSingleton()) {
   		return (this.initialized ? this.singletonInstance : getEarlySingletonInstance());
   	}
   	else {
   		return createInstance();
   	}
   }
   
   private T getEarlySingletonInstance() throws Exception {
   	Class<?>[] ifcs = getEarlySingletonInterfaces();
   	if (ifcs == null) {
   		throw new FactoryBeanNotInitializedException(
   				getClass().getName() + " does not support circular references");
   	}
   	if (this.earlySingletonInstance == null) {
   		this.earlySingletonInstance = (T) Proxy.newProxyInstance(
   				this.beanClassLoader, ifcs, new EarlySingletonInvocationHandler());
   	}
   	return this.earlySingletonInstance;
   }
   ```

3. **Mybatis用到了哪些设计模式**

   - 工厂方法模式（Factory Method pattern）：一对一的关系，一个工厂创建一个与其对应的对象，由子类实现创建对象的操作

     > 参考JdbcTransactionFactory和DefaultObjectFactory.create()，DefaultObjectFactory.create()代码实现如下：
     >
     > ```java
     > @SuppressWarnings("unchecked")
     > @Override
     > public <T> T create(Class<T> type， List<Class<?>> constructorArgTypes， List<Object> constructorArgs) {
     >     Class<?> classToCreate = resolveInterface(type)；
     >     // we know types are assignable
     >     return (T) instantiateClass(classToCreate， constructorArgTypes， constructorArgs)；
     > }
     > ```

   - 建造者模式（Builder pattern）：创建的对象属性比较复杂，要分步骤处理，还是就是构造函数传的参数比较多的情况，对构造函数进行拆分，最后返回一个对象

     > 具体实现也可以不用抽象的Builder，视具体情况而定，可以参考ResultMap：
     >
     > ```java
     > public ResultMap build() {
     >       if (resultMap.id == null) {
     >         throw new IllegalArgumentException("ResultMaps must have an id")；
     >       }
     >       resultMap.mappedColumns = new HashSet<String>()；
     >       resultMap.idResultMappings = new ArrayList<ResultMapping>()；
     >       resultMap.constructorResultMappings = new ArrayList<ResultMapping>()；
     >       resultMap.propertyResultMappings = new ArrayList<ResultMapping>()；
     >       for (ResultMapping resultMapping : resultMap.resultMappings) {
     >           //判断是内嵌查询还是内嵌结果集
     >         resultMap.hasNestedQueries = resultMap.hasNestedQueries || resultMapping.getNestedQueryId() != null；
     >         resultMap.hasNestedResultMaps = resultMap.hasNestedResultMaps || (resultMapping.getNestedResultMapId() != null && resultMapping.getResultSet() == null)；
     >         final String column = resultMapping.getColumn()；
     >         if (column != null) {
     >             //将内部标签属性为column的添加早已映射列
     >           resultMap.mappedColumns.add(column.toUpperCase(Locale.ENGLISH))；
     >         } else if (resultMapping.isCompositeResult()) {
     >           for (ResultMapping compositeResultMapping : resultMapping.getComposites()) {
     >             final String compositeColumn = compositeResultMapping.getColumn()；
     >             if (compositeColumn != null) {
     >               resultMap.mappedColumns.add(compositeColumn.toUpperCase(Locale.ENGLISH))；
     >             }
     >           }
     >         }
     >         if (resultMapping.getFlags().contains(ResultFlag.CONSTRUCTOR)) {
     >           resultMap.constructorResultMappings.add(resultMapping)；
     >         } else {
     >           resultMap.propertyResultMappings.add(resultMapping)；
     >         }
     >         if (resultMapping.getFlags().contains(ResultFlag.ID)) {
     >           resultMap.idResultMappings.add(resultMapping)；
     >         }
     >       }
     >       if (resultMap.idResultMappings.isEmpty()) {
     >         resultMap.idResultMappings.addAll(resultMap.resultMappings)；
     >       }
     >       // lock down collections
     >       resultMap.resultMappings = Collections.unmodifiableList(resultMap.resultMappings)；
     >       resultMap.idResultMappings = Collections.unmodifiableList(resultMap.idResultMappings)；
     >       resultMap.constructorResultMappings = Collections.unmodifiableList(resultMap.constructorResultMappings)；
     >       resultMap.propertyResultMappings = Collections.unmodifiableList(resultMap.propertyResultMappings)；
     >       resultMap.mappedColumns = Collections.unmodifiableSet(resultMap.mappedColumns)；
     >       return resultMap；
     >     }
     > }
     > ```

   - 装饰器模式（Decorator pattern）：通过传入的委派对象，去改变本身对象的责任与行为

     > 参考执行器CachingExecutor、以及缓存实现都用了装饰器模式，CachingExecutor就是一个装饰对象代码如下，通过构造函数传入委派对象Executor ：
     >
     > ```java
     > public class CachingExecutor implements Executor {
     >     private static final Logger log = LoggerFactory.getLogger(CachingExecutor.class)；
     > 
     >   private Executor delegate；
     >   private TransactionalCacheManager tcm = new TransactionalCacheManager()；
     > 
     >   public CachingExecutor(Executor delegate) {
     >     this.delegate = delegate；
     >     delegate.setExecutorWrapper(this)；
     >   }
     > ......
     > }
     > ```

   - 动态代理模式（Proxy pattern）：由JDK的Proxy对象生成代理对象，运行期间动态执行目标方法

     > 参考MapperProxyFactory（相当于客户端）、MapperProxy（代理对象）、SqlSession（委派执行对象），MapperProxy执行源码：
     >
     > ```java
     > @Override
     > public Object invoke(Object proxy， Method method， Object[] args) throws Throwable {
     >     if (Object.class.equals(method.getDeclaringClass())) {
     >       try {
     >         return method.invoke(this， args)；
     >       } catch (Throwable t) {
     >         throw ExceptionUtil.unwrapThrowable(t)；
     >       }
     >     }
     >     //缓存真实调用方法，args为真实方法的参数
     >     final MapperMethod mapperMethod = cachedMapperMethod(method)；
     >     log.debug("执行Mapper类中的方法")；
     >     return mapperMethod.execute(sqlSession， args)；
     > }
     > ```
     >
     > 此外mybatis的logging包下面的ConnectionLogger、PreparedStatementLogger、ResultSetLogger等都是动态代理对象

4. **其他设计模式**

   - 工厂模式：将每个对象，交给了各自工厂去创建。有猫工厂，狗工厂，作用就是建立对象

   - 装饰者模式（增强HttpServletRequest对象、IO流）：继承、装饰者模式、动态代理
     - 增强的内容是不能修改的

     - 被增强的对象可以是任意的

       ```java
       class 咖啡类 {}
       class 有糖咖啡 extends 咖啡类 {}
       class 加奶咖啡 extends 咖啡类 {}
       class 加盐咖啡 extends 咖啡类 {}
       咖啡 a = new 加糖()；
       咖啡 b = new 加盐(a)；//对a进行装饰，就是给a加盐
       咖啡 c = new 加奶(b)；
       ```

   - 组合模式：集合

   - 桥接模式：JDBC编程

4. **设计模式的的六大原则及其含义**

   - 开闭原则（Open Close Principle）

     定义：一个软件实体如类、模块和函数应该对扩展开放，对修改关闭。

     用抽象构建框架，用实现扩展细节

     优点：提高软件系统的可复用性及可维护性

     > 公司的弹性制就是开闭原则，对八小时工作制度的修改是关闭的，什么时候来什么时候走是开放的。早点来的可以早点走，晚点来的晚点走。总之要满足八小时制
     >
     > 实际项目中不能用 System.out.println ("")，因为里面是有锁

     核心思想：就是面向接口编程，而不是面向具体的类。发生变化时可以创建抽象方法来处理新的变化

   - 里氏代换原则（Liskov Substitution Principle）

     里氏代还原则是面向对象设计的基本原则之一。原则：基类存在的的地方，子类一定存在。
     
   - 依赖倒转原则（Dependence Inversion Principle）

     这个原则是开闭原则的基础，针对接口编程，依赖于抽象而不依赖于具体。
     
   - 接口隔离原则（Interface Segregation Principle）

     使用多个隔离的接口，降低类之间的耦合度。便于升级和维护，降低依赖，降低耦合。
     
   - 迪米特法则，又称最少知道原则（Demeter Principle）

     一个实体应当尽量少地与其他实体之间发生相互作用，使得系统功能模块相对独立。
     
   - 合成复用原则（Composite Reuse Principle）

     尽量使用合成、聚合的方式，而不是用继承。

### SpringCloudAlibaba 微服务、分布式系统

**什么是微服务**

微服务就是跑在自己的进程里的服务，每个服务为独立的业务，可以独立部署且分布式的管理

**为什么使用微服务？：**因为它们是一个个独立的小系统，可以独立部署并对外提供服务，并提高服务的可重用性。比如双 11 来了我们可以给订单服务的节点部署的多一些，用户管理的节点部署的少一些，方便系统扩展和维护

1. **谈谈分布式事务（分布式事务的控制）**

   - 事务的ACID特性

     原子性、一致性、隔离性、持久性
   - 分布式事务的产生的原因

     数据库分库分表、应用服务化
   - 分布式事务的应用场景

     支付、电商平台下单
   - 常见的分布式事务解决方案

     消息事务 + 最终一致性，所谓的消息事务就是基于消息中间件的两阶段提交，本质上是对消息中间件的一种特殊利用，它是将本地事务和发消息放在了一个分布式事务里，保证要么本地操作成功成功并且对外发消息成功，要么两者都失败1、A 系统向消息中间件发送一条预备消息 2、消息中间件保存预备消息并返回成功 3、A 执行本地事务 4、A 发送提交消息给消息中间件

2. **谈谈分布式锁（分布式锁如何设计）**

   [分布式锁](#RedLock)

3. **什么是微服务？为什么使用微服务（微服务优缺点）？**

   简而言之，微服务架构风格是一种将单个应用程序开发为一组微服务的方法，每个微服务都运行在自己的进程中并采用轻量级机制（通常是 HTTP 资源 API）进行通信。这些服务是围绕业务能力构建的，并且可以通过全自动部署机制独立部署。并且这些服务可以用不同的编程语言编写并使用不同的数据存储技术。

   优点：

   - 强大的模块边界：微服务加强了模块化结构，比如我负责下单、支付模块，同事负责商品模块，模块之间就有明确的分界。
   - 独立部署：简单的服务更容易单独治理并独立部署，因此这样不容易导致整个系统故障。（比如我们系统中的投票系统，投票服务故障以后并不影响其他商品下单支付服务的运行）
   - 技术多样性：借助微服务，您可以混合使用多种语言、开发框架和数据存储技术。（这样就更加灵活了嘛，如果有个单独的服务用 Python + 其他数据库更容易开发，就可以把任务分给熟悉 Python 的人完成）

   > 微服务一种架构模式或者说是一种架构风格，它提倡应用程序单一化，每个服务运行在其独立的进程中，服务间采用轻量级通信机制（通常是基于 http 的 restful api）就是将传统的单一应用，根据业务拆分成一个一个的服务，彻底去耦合，一个服务只做一件事，能够单独启动或销毁，拥有独立的数据库
   >
   > 优点：每个服务足够内聚、足够单一，代码更容易聚焦到指定的业务，使得微服务能够使用不同语言开发并且只用关心业务逻辑代码，前后端分离，不会和 html、css 或其它界面组件混合。每个微服务可以有单独数据库，也可以有统一数据
   >
   > 缺点：开发人员要处理分布式系统的复杂性。随着服务的增加，运维的压力也会增大，系统部署依赖程度高服务间通信也会有成本，系统需要集成性能的监控

5. **什么是服务熔断？什么是服务降级**

   服务熔断是应对雪崩效应的一种微服务保护机制，hystrix 会监控服务间调用情况，如果服务出现异常则会走 fallback 备选处理，熔断机制注解是 @HystrixCommand 服务降级处理是在客户端处理的，与服务端没有关系，统一在客户端接口中声明 @FeignClient-->fallbackFactory 属性指定服务出错的处理，这样就能将服务端容错出现彻底解耦出来

5. **CAP（请说下 Redis/eureka 和 zookeeper 两个的区别）**

   CAP 理论：CAP 理论核心是一个分布式系统不可能同时很好满足一致性、可用性、分区容错性这三个需求

   Redis 保证的是 AP：Redis 集群部署时，set 一条数据，首先会回复 OK，再把数据异步复制给从节点。它保证的是高可用，但牺牲了数据一致性

   > Eureka 保证的是 AP：Eureka 在设计时就先保证了可用性，节点间都是平等的，节点挂掉不会影响其它节点的工作，Eureka 的客户端在向某个 Eureka 注册服务时如果失败，则会自动切换至其它节点

   Zookeeper 保证的是 CP：它是把主节点的数据同步给 slave 节点，保证所有的都同步了才返回 OK。这样 Zookeeper 的高可用和并发性就下降了

   > 当 master 节点失去联系时，剩余节点会重新进行 leader 的选举，但是 leader 的选举时间过长（一般在 30s~120s）且选举期间整个集群是不可用的，这就会导致选举期间注册服务瘫痪

   总结：Eureka/Redis 可以很好的应对因网络故障导致的节点丢失这种情况，而 zk 则会使整个注册服务瘫痪

7. **分布式 session 如何设计**

   [见 Java WEB](#session)

### Redis 缓存相关

1. <span id='CPU'>**Redis 都有哪些数据类型？分别在哪些场景下使用比较合适？**</span>

   > [Command reference – Redis](https://redis.io/commands)

   i. String：做基础的 k/v 缓存

   使用场景：

   - 商品编号、订单号可用采用 INCR 命令生成，每次 +1
   - 微信公众号：是否喜欢作者、统计阅读数（只要点击了 restful 地址，就是用 INCR key 命令使数字 +1）
     <img src="https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/image-20210726153732341.png" alt="image-20210726153732341" style="zoom:25%;" />

   ii. List：有序列表（LRANGE 命令可以做分页）

   场景：微信订阅的微信公众号平台发布的文章

   ![image-20210726152607272](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/image-20210726152607272.png)

   ```bash
   127.0.0.1:6379> LPUSH likeauthor:uid_xxx article1 article2 # List 里放入两篇文章
   (integer) 2
   127.0.0.1:6379> LRANGE likeauthor:uid_xxx 0 -1 # 展示所有微信公众号更新的文章
   1) "article2"
   2) "article1"
   127.0.0.1:6379>
   ```

   <img src="https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/image-20210726153805138.png" alt="image-20210726153805138" style="zoom: 25%;" />

   

   iii. Set：基于 Redis 进行全局的 Set 去重

   场景：特别适合做社交类网站

   - 微信抽奖（200 人参与抽一本书，一人中奖，用 Redis 来做就特简单）

     ![image-20210726155741396](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/image-20210726155741396.png)

     ```bash
     127.0.0.1:6379> SADD Lottery u1 u2 u3 # 三名用户参与抽奖，用于显示用户头像
     (integer) 3
     127.0.0.1:6379> SCARD Lottery # 统计参与人数，共有 2189 人参与
     (integer) 3
     127.0.0.1:6379> SRANDMEMBER Lottery 1 # 随机抽奖 1 人，元素不删除（只抽一次）
     1) "u3"
     127.0.0.1:6379> SMEMBERS Lottery # 查询 Set 集合
     1) "u2"
     2) "u3"
     3) "u1"
     127.0.0.1:6379> SPOP Lottery 1 # 随机抽奖 1 人，元素删除（抽除第一名外还要抽第二三名，抽了奖的就不许再抽了）
     1) "u1"
     127.0.0.1:6379> SMEMBERS Lottery
     1) "u2"
     2) "u3"
     ```

   - 微信朋友圈

     <img src="https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/image-20210726161547686.png" alt="image-20210726161547686" style="zoom: 25%;" />

     ```bash
     127.0.0.1:6379> SADD pub:msgID u1 u2 # 新增点赞
     (integer) 2
     127.0.0.1:6379> SREM pub:msgID u1 # 取消点赞
     (integer) 1
     127.0.0.1:6379> SMEMBERS pub:msgID # 展示所有点赞过的用户
     1) "u2"
     127.0.0.1:6379> SCARD pub:msgID # 统计点赞人数
     (integer) 1
     127.0.0.1:6379> SISMEMBER pub:msgID u1 # 判断某个朋友是否点赞过
     (integer) 0
     ```

   - 微博好友的关注社交关系

     - 共同关注：我到华为余承东的微博页面（我没有关注余总的情况下），马上就能获得我和余总共同关注的人 —— 局座张召忠

       ```bash
       127.0.0.1:6379> SADD intersection1 u1 u2 u3 # 我关注的人
       (integer) 3
       127.0.0.1:6379> SADD intersection2 u2 u3 u4 # 余总关注的人
       (integer) 3
       127.0.0.1:6379> SINTER intersection1 intersection2 # 我们共同关注的人
       1) "u2"
       2) "u3"
       ```

       <img src="https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/image-20210726165040926.png" alt="image-20210726165040926" style="zoom: 50%;" />

     - 我关注的人也关注他（大家爱好相同） ：我关注了华为余总和局座，余总也关注了局座，那么我们的爱好相同

       ```bash
       ...
       127.0.0.1:6379> SISMEMBER intersection1 u3 # 我是否关注了局座
       (integer) 1
       127.0.0.1:6379> SISMEMBER intersection2 u3 # 余总是否关注了局座
       (integer) 1
       ```

       <img src="https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/image-20210726163614597.png" alt="image-20210726163614597" style="zoom:50%;" />

   - QQ 可能认识的人

     ```bash
     127.0.0.1:6379> SADD diff1 u1 u2 u3 # 我关注的人
     (integer) 3
     127.0.0.1:6379> SADD diff2 u2 u3 u4 # 朋友关注的人
     (integer) 3
     127.0.0.1:6379> SINTER diff1 diff2 # 我们的共同好友
     1) "u2"
     2) "u3"
     127.0.0.1:6379> SDIFF diff1 diff2 # 推荐给朋友的可能认识的人
     1) "u1"
     127.0.0.1:6379> SDIFF diff2 diff1 # 推荐给我的可能认识的人
     1) "u4"
     ```

   iv. Sorted set：排序的set，去重、排序。我们项目里面有用它来做了课程排行榜

   - 根据销量对商品进行排序显示

     ```bash
     127.0.0.1:6379> ZADD goods:sellsort 100 goods1 200 goods2 # 添加销量为 100 的商品 goods1 和销量为 200 的 goods2
     (integer) 2
     127.0.0.1:6379> ZRANGE goods:sellsort 0 9 withscores # 展示销量前 10 的商品
     1) "goods1"
     2) "100"
     3) "goods2"
     4) "200"
     127.0.0.1:6379> ZINCRBY goods:sellsort 400 goods1 # goods1 商品销量增加 400
     "500"
     127.0.0.1:6379> ZRANGE goods:sellsort 0 99 withscores
     1) "goods2"
     2) "200"
     3) "goods1"
     4) "500"
     ```

     

   - 抖音排行榜

     ![image-20210726183247371](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/image-20210726183247371.png)

     ```bash
     127.0.0.1:6379> ZADD hot:20200726 1163 曹缘陈艾森跳水男子10米跳台银牌 1041 中方向美方提出两份清单 913 台风烟花 # 添加三条热榜数据
     (integer) 3
     127.0.0.1:6379> ZINCRBY hot:20200724 200 中方向美方提出两份清单 # 增加点击的量
     "1241"
     127.0.0.1:6379> ZREVRANGE hot:20200724 0 9 WITHSCORES # 查询前 10 热榜数据
     1) "\xe4\xb8\xad\xe6\x96\xb9\xe5\x90\x91\xe7\xbe\x8e\xe6\x96\xb9\xe6\x8f\x90\xe5\x87\xba\xe4\xb8\xa4\xe4\xbb\xbd\xe6\xb8\x85\xe5\x8d\x95"
     2) "1241"
     3) "\xe6\x9b\xb9\xe7\xbc\x98\xe9\x99\x88\xe8\x89\xbe\xe6\xa3\xae\xe8\xb7\xb3\xe6\xb0\xb4\xe7\x94\xb7\xe5\xad\x9010\xe7\xb1\xb3\xe8\xb7\xb3\xe5\x8f\xb0\xe9\x93\xb6\xe7\x89\x8c"
     4) "1163"
     5) "\xe5\x8f\xb0\xe9\xa3\x8e\xe7\x83\x9f\xe8\x8a\xb1"
     6) "913"
     ```

     > 排行榜：将每个用户以及其对应的什么分数写入进去，zadd board score username，接着zrevrange board 0 99，就可以获取排名前100的用户；zrank board username，可以看到用户在排行榜里的排名

   v. Hash：主要是用来存放一些对象，后续操作时可以直接修改这个对象中的某个字段的值

   > 对应 Java 中的数据结构 Map<String,Map<Object,Object>>

   ```bash
   127.0.0.1:6379> hset person id 1
   (integer) 1
   127.0.0.1:6379> hset person name lin
   (integer) 1
   127.0.0.1:6379> hget person id
   "1"
   127.0.0.1:6379> hget person name
   "lin"
   127.0.0.1:6379> hgetall person
   1) "id"
   2) "1"
   3) "name"
   4) "lin"
   127.0.0.1:6379> hlen person
   (integer) 2
   127.0.0.1:6379> hdel person name
   (integer) 1
   127.0.0.1:6379> hgetall person
   1) "id"
   2) "1"
   ```

   ==场景：购物车（中小厂的话 hash 就能做个购物车）==

   新增商品 → hset shopcar:uid1 pid1 1
   新增商品 → hset shopcar:uid1 pid2 1
   增加商品数量 → hincrby shopcar:uid1
   商品总数 → hlen shopcar:uid1
   全部选择（拿到整个商品列表的详细信息） →hgetall shopcar:uid1

   ```bash
   127.0.0.1:6379> hset shopcar:uid1 pid1 1
   (integer) 1
   127.0.0.1:6379> hset shopcar:uid1 pid2 1
   (integer) 1
   127.0.0.1:6379> hincrby shopcar:uid1 pid1 3
   (integer) 4
   127.0.0.1:6379> hlen shopcar:uid1
   (integer) 2
   127.0.0.1:6379> hgetall shopcar:uid1
   1) "pid1"
   2) "4"
   3) "pid2"
   4) "1"
   ```

   ![image-20210726150555575](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/image-20210726150555575.png)

2. **Redis 处理拿来做缓存，你还见过基于 Redis 的什么用法？**

   i. [Redis 五大基本数据](#CPU)

   ii. 分布式锁
   
3. <span id='RedLock'>**知道分布式锁吗？有哪些实现方案？**</span>

   概念：单机版下是在 JVM 的层面加锁（synchronized、ReentrantLock），而分布式锁是采用分布式架构拆分单体服务为微服务，拆分后避免各个微服务之间冲突和数据故障而加入的一种锁

   > 分布式锁：控制在分布式架构里面的多个模块（多进程）访问资源的一个优先级
   >
   > 分布式架构下，不同服务器的锁管不到其它服务器的锁，所以就得用分布式锁

   实现方案：1. 基于 Zookeeper 2. 利用数据库 3. 我常用的是基于 Redis。Redis 官方推荐的一种思想是用 RedLock，具体到 Java 是使用 Redisson 来实现类似于单机的 ReentrantLock 的 lock/unlock

   > 了解即可
   >
   > i. 基于 Redis：利用 setnx（set not exist）命令，就是只有在某个 key 不存在情况才能 set 成功并返回 0 和 1，如果 set 成功的话返回一个 0，只要这个值存在就是谁先往 Redis 里面设置值谁就获得这个锁，后续 setnx 失败的就没有这个资格，就要等其它锁释放以后才能进行访问，这样就达到了多个进程并发去 set 同一个 key，只有一个进程能 set 成功
   >
   > ii. 基于 Zookeeper：往 / Locks 目录下写入临时有序节点，再根据最小的节点去判断你是否有权限，如果你是 Locks 下的最小节点，那你就可以获得这个锁。Zookeeper 的优势就是有个 Watch 机制，如果你这个节点失效以后它会自动被删除，删除以后它会 Watch 会监听到下一个节点去重新获取节点获取个锁操作
   >
   > iii. 利用数据库的唯一约束，创建一个 Lock 表里面有 id 和 method_name 字段，在 method_name 里面加唯一约束，你多个数据往数据库里面插入时只有一个节点能成功，只要插入成功的这个节点返回的结果是 1 的话，那么就意味着这个节点可以获得锁，其他就失败，因为访问的方法做了唯一约束嘛。那这个时候获得锁的就能对文件进行个读写，其他没有获得的就继续等待这个锁释放，这个锁释放就是通过删除这条记录，删除完后续节点就又能插入数据了

4. **谈谈你对 Redis 分布式锁的理解。Redis 做分布式锁有哪些需要注意的问题？（Redis 删 Key 的时候有哪些问题？）**

   [手写分布式锁的坑，看代码 Controller 的 Git History：interview/boot-redis01 at master · liuilin/interview](https://github.com/liuilin/interview/tree/master/boot-redis01)

   ==延伸问题：==

   - Redis 处理拿来做缓存，你还见过基于 Redis 的什么用法？

     i. [Redis 五大基本数据](#CPU)

     ii. 分布式锁

   - 如果 Redis 是单点部署，会带来什么问题？

     单点部署用 Jmeter 压测会出现商品超卖问题

   - Redis 做分布式锁时有哪些需要注意的问题？

     见下面 9 条

   - [集群模式下，比如主从有没有什么问题呢？](#redisLockLose)

     会造成异步复制锁丢失

     基本：一主二从，无人值守的话加个哨兵

     一般：配置 Redis 集群，三主三从。这种情况下 Redis 分布式锁有什么问题

   - 那你简单介绍一下 RedLock 把。你简历上写了 Redisson，你谈谈呢？

     一步步讲解自己手写的分布式锁有哪些坑，最终选用 Redisson 实现

   - [Redis 分布式锁如何续期？](#redisXuqi)

   

   ---

   

   i. 比如有 100 个商品，在分布式多线程竞争的情况下就会导致超卖（线程安全）问题

   解决：加单机版锁：[synchronized 和 ReentrantLock（可重入锁）的区别](#lockDiff)

   ii. 分布式部署后用 JMeter 压测，发现单机版锁还是会出现超卖问题

   解决：使用 Redis 分布式锁 setNX

   利用 setnx（set if not exists）命令，就是只有在某个 key 不存在情况才能 set 成功并返回 1，说明加锁成功，否则返回 0

   ```java
   ...
   String value = UUID.randomUUID() + Thread.currentThread().getName();
   // 加锁，放入要加锁的 key、value
   Boolean flag = stringRedisTemplate.opsForValue().setIfAbsent(REDIS_LOCK, value);
   ...
   // 释放锁
   stringRedisTemplate.delete("redis_lock");
   ```

   <img src="https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/image-20210727111107856.png" alt="image-20210727111107856" style="zoom:25%;" />

   iii. 假如释放锁之前代码出异常了，可能导致无法释放锁

   解决：必须把删除 Key 操作放在 finally 代码块中执行来释放锁

   ```java
   } finally {
       stringRedisTemplate.delete("redis_lock");
   }
   ```

   iv. 服务器宕机了：在运行到中间步骤时，部署了微服务的服务器挂了，代码层面呢根本没有走到 finally 这块，也就没法释放锁，这个 key 没有被删除，就算服务器重新启动后也没有办法获得这把锁

   解决：对加锁的 Key 加个过期时间

   ```java
   Boolean flag = stringRedisTemplate.opsForValue().setIfAbsent(REDIS_LOCK, value);
   stringRedisTemplate.expire(REDIS_LOCK, Duration.ofSeconds(10));
   ```

   v. 上面的程序设置 Key 和加过期时间分开了是非原子操作，还是会出现并发问题

   解决：必须合并成一行来具备原子性

   ```java
   Boolean flag = stringRedisTemplate.opsForValue().setIfAbsent(REDIS_LOCK, value, 10, TimeUnit.SECONDS);
   ```

   vi. 删除了别人的锁：A 先加锁，B 进来发现 Test 锁存在，加锁失败，进程 A 执行到在中间业务时（T4 时间线）它调用的服务挂了，最终导致 A 执行时间超过了 30s，Redis 就自动释放过期 Key。B 进来之后发现没有没有锁之后就加了一个 B 的锁，往下执行业务到一半时，A 突然那边执行完了后，往下执行把 B 的锁删除了。当 B 走到最后要删除 Key 时，发现自己的锁没了

   ![Redis 删错锁](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/Redis%20%E5%88%A0%E9%94%99%E9%94%81.png)

   解决：加个判断，如果 value（随机数 + 线程名）与 Redis 中设置 Key 对应的值相同才删除锁

   ```java
   String value = UUID.randomUUID() + Thread.currentThread().getName();
   ...
   if (value.equalsIgnoreCase(stringRedisTemplate.opsForValue().get(REDIS_LOCK))) {
       stringRedisTemplate.delete(REDIS_LOCK);
   }
   ```

   vii. finally 代码块的 if 判断和删除锁操作不具备原子性

   解决：

   - 使用官方推荐的 Lua 脚本

     ```java
     Jedis jedis = RedisUtil.getJedis();
     String script = "if redis.call(\"get\",KEYS[1]) == ARGV[1] then\n"
             + "    return redis.call(\"del\",KEYS[1])\n"
             + "else\n"
             + "    return 0\n"
             + "end";
     try {
         Object obj = jedis.eval(script, Collections.singletonList(REDIS_LOCK), Collections.singletonList(value));
         if ("1".equals(obj.toString())) {
             System.out.println("---delete REDIS_LOCK success");
         } else {
             System.out.println("---delete REDIS_LOCK error");
         }
     } finally {
         if (null != jedis) {
             jedis.close();
         }
     }
     ```

   - 使用 Redis 事务

     ```java
     // 乐观锁（失败就重试）
     while (true) {
         // 加事务
         stringRedisTemplate.watch(REDIS_LOCK);
         if (value.equalsIgnoreCase(stringRedisTemplate.opsForValue().get(REDIS_LOCK))) {
             stringRedisTemplate.setEnableTransactionSupport(true);
             stringRedisTemplate.multi();
             stringRedisTemplate.delete(REDIS_LOCK);
             List<Object> list = stringRedisTemplate.exec();
             // 如果为 null，则是删除失败，返回重新执行
             if (CollectionUtils.isEmpty(list)) {
                 continue;
             }
             // 如果删除成功，释放监视器
             stringRedisTemplate.unwatch();
             break;
         }
     }
     ```
     
     - Redis的事务是通过 MULTI，EXEC，DISCARD 和这四个命令来完成。
     - Redis的单个命令都是原子性的，所以这里确保事务性的对象是命令集合。
     - Redis将命令集合序列化并确保处于一事务的命令集合连续且不被打断的执行。
     - Redis不支持回滚的操作。
     
     > Watch 类似于乐观锁，别人改动了我就修改失败，然后在 While 里重新尝试，直到我修改数据成功为止

   viii. 要确保锁的过期时间大于执行时间的问题：后台写个程序来扫描即将过期的 Key，要过期了就延长 10s，隔一段时间就来检查，来确保锁的过期时间大于执行时间，保障程序的健壮性。

   问题1.<span id='redisXuqi'> ==Redis 分布式锁如何续期？== </span>这个问题我们自己很难解决，写了上生产环境后还可能不能用

   问题2. <span id='redisLockLose'>==Redis 异步复制造成锁丢失==</span>（CAP + 集群对比 Zookeeper）现在 Redis 是单机版，但一般情况下我们会配一主两从或三主三从，它什么都好就是容易在集群环境下出现 CAP 的一些小故障，主机 master 加锁后会里面同步给 slave 从机，而 Redis 的特性是 AP（分区容错 + 高可用），没有保证数据一致性，导致的问题就是 master 刚刚获得了锁就返回了 OK，但是 master 还没同步回 slave 的时候 master 就宕机了，假如我们有高可用的哨兵或自动重启的无人值守，master 降级为 slave，其他 slave 上位成新的 master。由于 master 宕机的时候还没有同步锁给 slave，slave 上位后发现没有锁。这就 Redis 异步复制就会造成锁丢失

   <img src="https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/image-20210727134029862.png" alt="image-20210727134029862" style="zoom: 33%;" />

   > 类似于汽车加油，发现你快要没有油了，马上给你加一点
   >
   > AP（分区容错 + 高可用）：保证了高可用就牺牲了数据一致性
   >
   > CP（数据一致性 + ）：Zookeeper 主节点挂了以后有选举算法，它加锁以后不会着急回复，它会让从节点数据都和主节点一致了之后才会回复我这边加锁成功了
   >
   > 理论 Zookeeper 更好，但是一致性保证了，高性能、并发性就下降了，所以实际我还是会选择 Redis，实在出现锁丢失在考虑修复数据

   ```java
   RLock rLock = redisson.getLock(REDIS_LOCK);
   rLock.lock();
   ...
   } finally {
       // 增强程序健壮性
       // 避免超高并发量的情况下出现错误 attempt to unlock lock, not locked by current thread node id（当前线程和解锁的那个线程不是同一个）
       if (rLock.isLocked() && rLock.isHeldByCurrentThread()) {
           rLock.unlock();
       }
   }
   ```

5. **Redis 缓存过期淘汰策略**

   - **生产上你们的 Redis 内存设置多少？**

     一般推荐 Redis 内存设置为最大物理内存的 3/4，也就是 0.75

   - **如何配置、修改 Redis 的内存大小**

     - 修改配置文件 redis.conf

       ```bash
       # maxmemory <bytes>
       ```

     - 用 config 命令来配置

       ```bash
       127.0.0.1:6379> config set maxmemory 1
       OK
       127.0.0.1:6379> info memory
       ```

     - 如果内存满了你怎么办

       ```bash
       127.0.0.1:6379> config set maxmemory 1 # 设置为 0 时，代表没有内存限制
       OK
       127.0.0.1:6379> set k1 lin
       (error) OOM command not allowed when used memory > 'maxmemory'.
       ```

       会报出一个 OOM，内存溢出异常

       设置了 maxmemory 的选项，假如 Redis 内存使用达到上限从而报出 OOM 内存溢出异常。原因是因为不断地写数据但没有给它们加上过期时间会导致数据写满，为了避免类似情况，就会使用到内存淘汰策略。但是在谈它之前呢需要先连接到 Redis 清理内存的方式

   - **Redis 过期删除策略（清理内存的方式）？定期删除和惰性删除了解过吗？**

     Redis 有三种键的过期删除策略：

     - 定时删除

       MAXMEMORY POLICY：缓存过期淘汰策略

       ```bash
       127.0.0.1:6379> set k1 v1 ex 10 # key 会在 10s 后定时删除
       OK
       ```

       Redis 不可能时时刻刻遍历所有被设置了生存时间的 key，来检测数据是否已经到达过期时间，然后对它进行删除。

       优点：立即删除能保证内存中数据的最大新鲜度，因为键过期后数据会里面移除

       缺点：但是立即删除对 CPU 是最不友好的。因为删除操作会占用 CPU 的时间，如果 CPU 正在处理大量并发请求，此时还要挂着一个时间事件去查键是否已过期，这就会给 CPU 造成额外的压力与性能开销，同时也会影响数据的读取操作

       **总结：** 数据新鲜度非常好，但是对 CPU 不友好，用处理器性能换取存储空间（拿时间换空间）

     - 惰性删除

       数据到达过期时间时先不做处理。等下次再访问该数据时会进行查看，如果 key 未过期则返回数据。如果已过期就删除过期 key 并返回不存在

       优点：对 CPU 友好，不用花费太多时间去进行过期检查。

       缺点：对内存不友好，如果一个键已经过期，但是一直没有被访问到，那么键就会一直存在内存中，而这种数据多了以后就会造成内存泄漏。

       **总结：** 对内存不友好，用存储空间换取处理器性能（拿空间换时间）

     - 定期删除

       定期删除策略是前两种策略的折中方案，它会每隔一段时间执行一次删除过期键操作，并通过限制删除操作执行的时长和频率来减少删除操作对 CPU 时间的影响。周期性轮询 Redis 库中的时效性数据，并采用随机抽取的策略，利用过期数据占比来控制删除频率

       缺点：难以确定删除操作的时长和频率。如果删除操作执行得太频繁，或者执行的时间太长，就会变得和定时删除一样，会对 CPU 不友好。如果删除操作执行得太少，又会和惰性删除一样了，会出现浪费内存的情况。所以需要合理地设置删除操作的时长和频率。
       
       总结：周期性抽查存储空间（随机抽查，重点抽查)

     Redis 默认采用的是惰性删除 + 定期删除组合，默认每个 100ms 进行一次随机检查，看是否有过期 key，有过期 key 就删除。但也同样存在漏洞：

     如果定期删除时，从来没有被抽查到；同时惰性删除时，没有再次被访问到呢

     这样一来还是会导致大量过期的 key 堆积在内存中，导致 Redis 内存空间紧张或耗尽。此时就需要一种兜底方案（内存淘汰策略）来解决这种问题

   - **Redis 内存（缓存）淘汰策略**

     当现有内存大于 maxmemory 时，便会触发 redis 主动淘汰内存方式，通过设置 maxmemory-policy

     ```bash
     # 利用 LRU 算法移除设置了过期时间 key
     # volatile-lru -> Evict using approximated LRU, only keys with an expire set. 
     # 利用 LRU 算法移除任何最近最少使用的 key（不管是否设置了过期时间的 key）
     # allkeys-lru -> Evict any key using approximated LRU.
     # volatile-lfu -> Evict using approximate LFU, only keys with an expire set
     # allkeys-lfu -> Evict any key using approximated LFU.
     # volatile-random -> Remove random key having an expire set.
     # allkeys-random -> Remove random key, any key.
     # 移除即将过期的 key
     # volatile-ttl -> Remove the key with the nearest expire time（ minor TTL）
     # noeviction -> don' t evict anything, just return an error on write operations.
     ```

     总结：

     2 个维度：

     - volatile（易挥发的）：过期键中筛选
     - allkeys：所有键中筛选

     4 个方面：

     - LRU：LRU means Least Recently Used（最近最少使用）
     - LFU：LFU means Least Frequently Used（最不常用）
     - random：随机删除
     - ttl：移除即将过期 key

     > 目前跟着市场上走使用 allkeys-lru。千万不要使用 random 的，随机删除很恐怖，会删错；不要使用 noeviction 不驱逐，会导致 Redis 内存满了报 OOM
     >

     怎么配置内存淘汰策略：

     - 修改配置文件 redis.conf

       ```bash
       maxmemorey-policy allkeys-lru
       ```

     - 命令行配置

       ```bash
       127.0.0.1:6379> config set maxmemory-policy allkeys-lru
       OK
       127.0.0.1:6379> config get maxmemory-policy
       1) "maxmemory-policy"
       2) "allkeys-lru"
       ```

    - **Redis 的 LRU 了解过吗？可否手写一个 LRU 算法**

      Redis 的缓存空间是有限的，那就需要一种机制去清理一些相对不怎么使用的数据

      - 是什么：LRU 是 Least Recently Used 的缩写，即最近最少使用缓存机制。是一种常用的页面置换算法，选择最近最久未使用的数据给淘汰掉

        > 手机后台案例：不经常使用的应用在最左边，常用的任务在右边，假如后台任务只能有三个，新任务进来后，最左边的任务就会被干掉
        >
        > <img src="https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/image-20210727224931109.png" alt="image-20210727224931109" style="zoom:25%;" />
        
         - 算法来源：[146. LRU 缓存机制 - 力扣（LeetCode）](https://leetcode-cn.com/problems/lru-cache/)

         - 设计思想：LRU 算法核心是哈希链表，本质是 HashMap + DoubleLinkedList（哈希表 + 双向链表），时间复杂度是 O(1) 

           > 1. 所谓缓存，必须要有读 + 写两个操作，按照命中率的思路考虑，写操作 + 读操作时间复杂度都需要为 O (1) 
           >
           > 2. 特性要求分析
           >    
           >    i. 必须有顺序之分，以区分最近使用的和很久没用到的数据排序。
           >    
           > ii. 写和读操作一次搞定
           >    
           >    iii. 如果容量 (坑位) 满了要删除最不长用的数据，每次新访问还要把新的数据插入到队头 (按照业务你自己设定左右那一边是队头) 
           >    
           >    <font color='red'> 查找快，插入快，删除快，且还需要先后排序 —— 什么样的数据结构满足呢？</font>
           >
           > 你是否可以在 O (1) 时间复杂度内完成这两种操作？
           >
      > 如果一次就可以找到，你觉得什么数据结构最合适？
           >
      > Hash 能够一次就找到对应的值
      
      get 和 put key流程：
      
      ![LRU cache 的 get 操作流程动图演示](http://rjnbyrj6g.hn-bkt.clouddn.com/interview/LRU%20cache%20%E7%9A%84%20get%20%E6%93%8D%E4%BD%9C%E6%B5%81%E7%A8%8B%E5%8A%A8%E5%9B%BE%E6%BC%94%E7%A4%BA.png)
      
         - 手写 LRU
        
           ![LRU principle](http://rjnbyrj6g.hn-bkt.clouddn.com/interview/LRU%20principle.png)
           
           方案一：使用自带 JDK 库快速完成 LRU 算法
           
           ```java
           public class LruCache<K, V> extends LinkedHashMap<K, V> {
           
               // 缓存坑位
               private Integer capacity;
           
               /**
                * the ordering mode -
                * <tt>true</tt> for access-order
                * <tt>false</tt> for insertion-order
                *
                * @param capacity 坑位
                */
               public LruCache(int capacity) {
                   super(capacity, 0.75F, true);
                   this.capacity = capacity;
               }
           
               public static void main(String[] args) {
                   LruCache lruCache = new LruCache(3);
                   lruCache.put(1, 11);
                   lruCache.put(2, 22);
                   lruCache.put(3, 33);
                   System.out.println(lruCache.keySet());
                   lruCache.put(4, 44);
                   System.out.println(lruCache.keySet());
                   lruCache.put(3, 33);
                   System.out.println(lruCache.keySet());
                   lruCache.put(3, 33);
                   System.out.println(lruCache.keySet());
                   lruCache.put(5, 55);
                   System.out.println(lruCache.keySet());
               }
           
               @Override
               protected boolean removeEldestEntry(Entry<K, V> eldest) {
                   return super.size() > capacity;
               }
           }
           /** access-order = true
            * [1, 2, 3]
            * [2, 3, 4]
            * [2, 4, 3]
            * [2, 4, 3]
            * [4, 3, 5]
            */
           
           /** access-order = false
            * [1, 2, 3]
            * [2, 3, 4]
            * [2, 3, 4]
            * [2, 3, 4]
            * [3, 4, 5]
            */
           ```
           
           方案二：不使用 JDK，自己使用数据结构完成
           
           ```java
           /**
            * 手写 LRU
            *
            * @author liuqiang
            * @since 2021-07-28
            */
           public class LruCacheWithHand {
           
               // Map 负责查找，构建一个虚拟的双向链表，里面装的是一个个 Node 节点作为数据载体
               Map<Integer, Node<Integer, Integer>> map;
               DoubleLinkedList<Integer, Integer> doubleLinkedList;
               private Integer cacheSize;
           
               /**
                * 初始化好 LRU 雏形，HashMap + DoubleLinkedList
                */
               public LruCacheWithHand(Integer cacheSize) {
                   this.cacheSize = cacheSize; // 坑位
                   map = new HashMap<>(); // 用于查找 Node
                   doubleLinkedList = new DoubleLinkedList<>();
               }
           
               public static void main(String[] args) {
                   LruCacheWithHand lruCache = new LruCacheWithHand(3);
                   lruCache.put(1, 11);
                   lruCache.put(2, 22);
                   lruCache.put(3, 33);
                   System.out.println(lruCache.map.keySet());
                   lruCache.put(4, 44);
                   System.out.println(lruCache.map.keySet());
                   lruCache.put(3, 33);
                   System.out.println(lruCache.map.keySet());
                   lruCache.put(3, 33);
                   System.out.println(lruCache.map.keySet());
                   lruCache.put(5, 55);
                   System.out.println(lruCache.map.keySet());
               }
           
               public int get(int key) {
                   if (!map.containsKey(key)) {
                       return -1;
                   }
                   Node<Integer, Integer> node = map.get(key);
                   doubleLinkedList.removeNode(node);
                   doubleLinkedList.addNode(node);
                   return node.value;
               }
           
               // saveOrUpdate
               public void put(int key, int value) {
                   if (map.containsKey(key)) { // update
                       // put(1,2) put(1,3)
                       Node<Integer, Integer> node = map.get(key);
                       node.value = value;
                       doubleLinkedList.removeNode(node);
                       doubleLinkedList.addNode(node);
                   } else {
                       if (map.size() == cacheSize) { // 坑位满了，删除最后一个 Node
                           Node<Integer, Integer> lastNode = doubleLinkedList.getLast();
                           map.remove(lastNode.key);
                           doubleLinkedList.removeNode(lastNode);
                       }
                   }
                   // 新增 Node 节点
                   Node<Integer, Integer> newNode = new Node<>(key, value);
                   map.put(key, newNode);
                   doubleLinkedList.addNode(newNode);
               }
           
               /**
                * 1. 构造一个 Node 节点作为数据载体
                *
                * @see java.util.HashMap 数组 + 单向链表
                */
               class Node<K, V> {
           
                   K key;
                   V value;
                   Node<K, V> next;
                   Node<K, V> prev;
           
                   public Node() {
                       this.next = this.prev = null;
                   }
           
                   public Node(K key, V value) {
                       this.key = key;
                       this.value = value;
                       this.next = this.prev = null;
                   }
               }
           
               // 虚拟一个双向链表，里面放 Node 节点
               class DoubleLinkedList<K, V> {
           
                   Node<K, V> head;
                   Node<K, V> tail;
           
                   // 2.1 构建双向链表
                   public DoubleLinkedList() {
                       head = new Node<>();
                       tail = new Node<>();
                       head.next = tail;
                       tail.prev = head;
                   }
           
                   // 2.2 添加节点
                   public void addNode(Node<K, V> node) {
                       node.next = head.next;
                       node.prev = head;
                       head.next.prev = node;
                       head.next = node;
                   }
           
                   // 2.3 删除节点
                   public void removeNode(Node<K, V> node) {
                       node.next.prev = node.prev;
                       node.prev.next = node.next;
                       node.prev = null;
                       node.next = null;
                   }
           
                   // 2.4 获取最后一个节点
                   public Node<K, V> getLast() {
                       return tail.prev;
                   }
               }
           }
           ```
   
6. **在项目中缓存是如何使用的？缓存如果使用不当会造成什么后果？**

   ![01_缓存是如何实现高性能的](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/01_缓存是如何实现高性能的.png)

   用缓存，主要是俩用途，高性能和高并发

   课程首页访问量非常大，我们把开始走数据库改为走缓存，之前走数据库需要 600ms 的，现在直接走缓存 2ms 搞定了，性能提升 300 倍，保证了系统的高性能。特别是高 QPS 的系统，每次都去查询数据库的话，对于数据库来说就是个灾难。但是呢用上缓存就会出现各种问题

7. **Redis线程模型**

   ![01_redis单线程模型](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/01_redis单线程模型.png)

   Redis是单线程的，客户端发送请求到socket，通过IO复用模型把消息压入队列，文件事件处理器会调用之前关联好的事件处理器来处理这个事件并返回OK

8. **为啥Redis单线程模型也能效率这么高**

   - 因为它的核心是基于 ==非阻塞的 IO 多路复用机制==，就是多个客户端发过来的请求被它监听到了以后，它不处理而是直接压到队列里面去了是不阻塞的。所以它效率非常高，甚至监听几千个客户端都可以。
- 再有就是文件事件处理器是基于纯内存来操作的，以几微秒的速度处理请求
  
- 单线程反而避免了多线程的频繁上下文切换问题
  
9. **内存淘汰机制都有哪些（Redis缓存算法）**

   > 如果Redis的内存占用过多的时候，此时会进行内存淘汰

   allkeys-lru：当内存不足以容纳新写入数据时，会在键空间中移除最近最少使用的key

   > FIFO算法：First in First out，先进先出。原则：一个数据最先进入缓存中，则应该最早淘汰掉。也就是说，当缓存满的时候，应当把最先进入缓存的数据给淘汰掉
   >
   > LFU算法Least Frequently Used，最不经常使用算法
   >
   > LRU算法：Least Recently Used，近期最少使用算法。
   >
   > 对于缓存系统常见的缓存满了和数据丢失问题，需要根据具体业务分析，通常我们采用LRU策略处理溢出

11. **如何保证 Redis 的高并发和高可用？**

    高并发：主从架构，一主多从，一般来说，很多项目其实就足够了，单主用来写入数据，单机几万 QPS，多从用来查询数据，多个从实例可以提供每秒 10 万的 QPS

    高可用：如果你做主从架构部署，其实就是加上哨兵就可以了，就可以实现，任何一个实例宕机，自动会进行主备切换

    > 这个在 master node 故障时，自动检测，并且将某个 salve node 自动切换为 master node 的过程，叫做主备切换。这个过程，实现了 Redis 的主从架构下的高可用性

12. **Redis的主从复制原理能介绍一下么？（主从架构的核心原理）**

    ![Redis主从复制的原理](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/redis主从复制的原理.png)

    当启动一个slave node的时候，它会发送一个PSYNC命令给master node，如果这是slave node重新连接master node，那么它只会复制部分缺少的数据给slave；否则如果是slave node第一次连接master node，那么会触发一次full resynchronization全量复制。开始full resynchronization的时候，master会启动一个后台线程，开始生成一份RDB快照文件，同时还会将从客户端收到的所有写命令缓存在内存中。RDB文件生成完毕之后，master会将这个RDB发送给slave，slave会先写入本地磁盘，然后再从本地磁盘加载到内存中。然后master会将内存中缓存的写命令发送给slave，slave也会同步这些数据。

13. **Redis的哨兵原理能介绍一下么？**

    哨兵（sentinel）是Redis集群架构中非常重要的组件，负责一些集群监控、消息通知、故障转移、配置中心等

    > 经典的3节点哨兵集群 Configuration： quorum = 2，majority = 2
    >
    > 如果M1所在机器宕机了，那么三个哨兵还剩下2个，S2和S3可以一致认为master宕机，然后选举出一个来执行故障转移
    >
    > 同时3个哨兵的majority是2，所以还剩下的2个哨兵运行着，就可以允许执行故障转移

14. **RDB和AOF两种持久化机制的介绍（Redis挂了怎么办）**

    RDB(Redis DataBase)：对Redis中的数据执行周期性的持久化

    AOF(Append Only File)：对每条写入命令作为日志，以append-only的模式写入一个日志文件中（现代操作系统中，写文件不是直接写磁盘，会先写os cache ，然后到一定时间再从os cache到disk file。每隔1秒调用一次操作系统 fsync操作强制将os cache中的数据刷入磁盘文件中）

    > 通过RDB或AOF，都可以将Redis内存中的数据给持久化到磁盘上面来，然后可以将这些数据备份到别的地方去，比如说阿里云，云服务
    >
    > 如果Redis挂了，服务器上的内存和磁盘上的数据都丢了，可以从云服务上拷贝回来之前的数据，放到指定的目录中，然后重新启动Redis，Redis就会自动根据持久化数据文件中的数据，去恢复内存中的数据，继续对外提供服务

    优缺点及选择：综合使用AOF和RDB两种持久化机制，用AOF来保证数据不丢失，作为数据恢复的第一选择；用RDB来做不同程度的冷备，在AOF文件都丢失或损坏不可用的时候，还可以使用RDB来进行快速从云端恢复数据

15. **Redis 集群模式的工作原理能说一下么？**

    3.0 之后用的是 Cluster ，它采用的是虚拟槽分区，一共有 16384 个 slot。它根据 key 使用 CRC16 算法再 %16384 来计算出它落在哪个桶上。所有的 Redis 节点彼此互联，通过集群中超过半数的节点检测来判断节点是否 fail 了

16. **如何处理Redis雪崩？**

    ![01_缓存雪崩现象](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/01_缓存雪崩现象.png)

    缓存雪崩：某一时刻大规模的缓存失效，比如你的缓存服务宕机了，会有大量的请求进来直接打到DB上。结果就是DB抗不住就挂掉，像这样的连锁反应就是缓存雪崩

    事前：采用Redis cluster来保证Redis高可用，避免全盘崩溃

    事中：在系统内先使用本地ehcache缓存 + hystrix限流&降级，避免MySQL被打死

    > 使用 ehcache 本地缓存的目的也是考虑在 Redis Cluster 完全不可用的时候，ehcache 本地缓存还能够支撑一阵。
    >
    > 使用 Hystrix进行限流 & 降级 ，比如一秒来了5000个请求，我们可以设置假设只能有一秒2000个请求能通过这个组件，那么其他剩余的 3000 请求就会走限流逻辑。然后去调用我们自己开发的降级组件（降级），比如设置的一些默认值呀之类的。以此来保护最后的 MySQL 不会被大量的请求给打死。

    事后：开启Redis持久化机制，快速恢复缓存数据

    > 一旦重启，就能从磁盘上自动加载数据恢复内存中的数据

    ![02_如何解决缓存雪崩](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/02_如何解决缓存雪崩.png)

    

17. **如何处理Redis缓存穿透？**
    ![03_缓存穿透现象以及解决方案](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/03_缓存穿透现象以及解决方案.png)

    什么是缓存穿透：正常情况下，我们去查询数据都是存在。那么请求查询一条压根儿数据库中根本就不存在的数据，也就是缓存和数据库都查询不到这条数据，但是请求每次都会打到数据库上面去

    如果有黑客对你的系统进行攻击，拿一个不存在的id去查询数据，并产生大量的请求到数据库去查询。可能就会导致你的数据库压力过大而宕掉

    - 那么我们就可以为这些key对应的值设置为null丢到缓存里面去并设置过期时间。那后面再出现查询这个key请求的时候呢就会走缓存直接返回null
    - 在缓存之前在加一层布隆过滤器(BloomFilter)，它就是一个bitmap嘛，在查询的时候先去BloomFilter去查询key是否存在，如果不存在就直接返回，存在再去查缓存

    > 如何选择
    >
    > 针对key异常多、请求重复率比较低的数据，我们就没有必要进行缓存，使用第二种方案直接过滤掉；而对于空数据的key有限的，重复率比较高的，我们则可以采用第一种方式进行缓存

18. **如何解决缓存击穿**

    缓存击穿：在平常高并发的系统中，大量的请求同时查询一个key时，此时这个key又刚好失效了，就会导致大量的请求都打到数据库上

    why：会造成某一时刻数据库请求量过大，可能搞死数据库

    how：由于是多个线程同时去查询这条数据嘛，那么我们可以在第一个查询数据的请求上使用一个互斥锁来锁住它。其他的线程走到这一步拿不到锁就一直等着，等第一个线程查询到数据做好缓存。然后后面的线程进来发现已经有缓存了，就会直接走缓存

19. **解决热点数据集中失效问题**

    我们在设置缓存的时候，一般会给缓存设置一个失效时间，过了这个时间，缓存就失效了。对于一些热点的数据来说，当缓存失效以后会存在大量的请求过来，然后打到数据库去，从而可能导致数据库崩溃的情况。

    - 设置不同的失效时间：为了避免这些热点的数据集中失效，那么我们在设置缓存过期时间的时候，我们让他们失效的时间错开。比如在一个基础的时间上加上或者减去一个范围内的随机值。

    - 互斥锁：结合上面的击穿的情况，在第一个请求去查询数据库的时候对他加一个互斥锁，其余的查询请求都会被阻塞住，直到锁被释放，从而保护数据库。但是也是由于它会阻塞其他的线程，此时系统吞吐量会下降。需要结合实际的业务去考虑是否要这么做

20. **高并发场景下的缓存+数据库双写不一致问题分析与解决方案设计**

    简单的缓存不一致：先删除缓存，再修改数据库

    高并发场景：![复杂的数据库+缓存双写一致保障方案](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/复杂的数据库+缓存双写一致保障方案.png)

    更新数据的时候，根据数据的唯一标识，将操作路由之后，发送到一个jvm内部的队列中读取数据的时候，如果发现数据不在缓存中，那么将重新读取数据+更新缓存的操作，根据唯一标识路由之后，也发送同一个jvm内部的队列中一个队列对应一个工作线程每个工作线程串行拿到对应的操作，然后一条一条的执行这样的话，一个数据变更的操作，先执行，删除缓存，然后再去更新数据库，但是还没完成更新此时如果一个读请求过来，读到了空的缓存，那么可以先将缓存更新的请求发送到队列中，此时会在队列中积压，然后同步等待缓存更新完成。读操作呢可以做个排重去优化

    > 按1：99的比例计算读和写的请求，每秒5万的读QPS，可能只有500次更新操作如果一秒有500的写QPS，那么要测算好，可能写操作影响的数据有500条，这500条数据在缓存中失效后，可能导致多少读请求，发送读请求到库存服务来，要求更新缓存一般来说，1：1，1：2，1：3，每秒钟有1000个读请求，会hang在库存服务上，每个读请求最多hang多少时间，200ms就会返回在同一时间最多hang住的可能也就是单机200个读请求，同时hang住单机hang200个读请求，还是ok的1：20，每秒更新500条数据，这500秒数据对应的读请求，会有20 * 500 = 1万1万个读请求全部hang在库存服务上，就死定了

21. **Redis并发竞争问题以及解决方案？分布式锁？**

    分布式锁用的是 zookeeper（redis），确保同一时间，只能有一个系统实例在操作某个key，别人都不允许读和写每次要写之前，先判断一下当前这个value的时间戳是否比缓存里的value的时间戳要更新，如果更新，那么可以写如果更旧，就不能用旧的数据覆盖新的数据

    ![01_Redis并发竞争问题以及解决方案](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/01_redis并发竞争问题以及解决方案.png)

    > 悲观锁：认为所有的操作都是不安全，需要加锁强制一个一个执行
    >
    > 乐观锁：认为所有操作都是安全，不需要加锁，但是提交需要验证条件提交数据时要比对当前记录的版本号和提交数据的版本号，一致则成功，否则失败

22. **生产环境中的 Redis 是怎么部署的？**

    Redis cluster，10 台机器，5 台机器部署了 Redis 主实例，另外 5 台机器部署了 Redis 的从实例，每个主实例挂了一个从实例，5 个节点对外提供读写服务，每个节点的读写高峰 qps 可能可以达到每秒 5 万，5 台机器最多是 25 万读写请求 /s。

    机器是什么配置？32G 内存 + 8 核 CPU+1T 磁盘，但是分配给 Redis 进程的是 10g 内存，一般线上生产环境，Redis 的内存尽量不要超过 10g，超过 10g 可能会有问题。

    5 台机器对外提供读写，一共有 50g 内存。

    因为每个主实例都挂了一个从实例，所以是高可用的，任何一个主实例宕机，都会自动故障迁移，Redis 从实例会自动变成主实例继续提供读写服务

    你往内存里写的是什么数据？每条数据的大小是多少？商品数据，每条数据是 10kb。100 条数据是 1mb，10 万条数据是 1g。常驻内存的是 200 万条商品数据，占用内存是 20g，仅仅不到总内存的 50%。

    目前高峰期每秒就是 3500 左右的请求量

    比如我们吧，大型的公司，其实基础架构的 team，会负责缓存集群的运维

### 框架相关 Spring、SpringMVC 等等

1. **SpringMVC执行流程（请求原理）**

   ![SpringMVC处理执行流程](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/10173293-005aa35c51e174dd.webp)

   1. 用户发送请求到核心控制器（DispatcherServlet）
   2. 核心控制器根据请求路径通过处理器映射器找到对应的方法（也就是对应的RequestMapping下的方法）
   3. 处理器适配器执行找到的方法，处理业务之后返回视图mv（ModelAndView）
   4. 通过视图解析器处理返回的视图mv，之后返回真正的视图对象View
   5. 最后对视图页面进行渲染（也就是转发或重定向），渲染后响应给用户

2. **WebFlux**

   非阻塞 + 异步

   异步：客户端发送请求到用户注册（花了 3s）+发送短信（花了 2s），但我不一定需要发送完短信之后才收到服务端请求。

   我可以把发送短信过程异步化交给一个线程去处理，这样总共就只需要 3s

   多任务并行：采用 `Mono.zip()` 压到一起同时去处理

3. **Spring 如何解决循环依赖问题**

   [面试作答：Spring如何解决循环依赖问题的，看看普通人和高手是如何回答的？_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1FZ4y197qE/?spm_id_from=333.788)

4. **谈谈IOC、AOP（Spring）**

   IOC：控制反转(Inversion of Control)，以前需要我们自己new对象变成Spring帮我们new，并且用容器管理

   > 将对象交给IOC容器管理，你只需要在Spring配置文件中配置对应的Bean以及设置相关的属性，让Spring容器来生成类的实例对象以及管理对象。在Spring容器启动的时候，Spring会把你在配置文件中配置的Bean都初始化好，然后在你需要调用的时候，就把它已经初始化好的那些Bean，分配给你需要调用这些Bean的类（BeanFactory里面getBean获取Bean）
   >
   > 依赖注入：把底层类作为参数传递给上层类，实现上层对下层的控制
   >
   > 推荐阅读：
   >
   > https://www.zhihu.com/question/23277575/answer/169698662
   >
   > https://martinfowler.com/articles/injection.html
   >
   > IOC源码：https://Javadoop.com/post/Spring-ioc
   >
   > 

   AOP：面向切面编程(Aspect-Oriented Programming)，简单来说就是可以在一段程序之前或之后做一些事，我们一般是将系统中非核心的业务提取出来，进行单独处理。比如事务、日志和安全等。

   - AOP的实现是JdkProxy和Cglib，具体哪种方式生成是由AopProxyFactory根据AdvisedSupport对象的配置来决定
   - JDK动态代理通过反射来接收被代理的类，并且要求被代理的类实现InvocationHandler接口
   - 如果目标类没有实现接口，那么AOP就会选择使用Cglib来动态代理目标类，Cglib全称是Code Generation Library，是一个代码生成类库，可以在运行时动态的生成某个类的子类，它是通过继承的方式做的动态代理，所以如果某个类被标记为final时，那么它是无法使用Cglib来实现动态代理的

     > 动态代理：就是说AOP框架不会去修改字节码，而是在内存中临时为方法生成一个AOP对象，这个AOP对象包含了目标对象的全部方法，并且在特定的切点做了增强处理，并回调原对象的方法

     Spring里的代理模式的实现：真实实现类的逻辑包含在了getBean方法里，getBean方法返回的实际上是Proxy的实例，而这个代理类我们自己没写方法。它实例是Spring采用JDK Proxy或CGLIB动态生成的，getBean方法用于查找或实例化容器中的Bean，这也是为什么Spring AOP只能作用于Spring容器中Bean的原因，对于不是IOC容器管理的对象呢，Spring是无能为力的

   - https://www.jianshu.com/p/fe8d1e8bd63e

   - http://www.cnblogs.com/puyangsky/p/6218925.html

   - https://juejin.im/post/5a55af9e518825734d14813f 

     > Aspect ：通用功能的代码实现（写的日志代码类）
     >
     > Target ：被织入Aspect的对象（写的HelloController）
     >
     > Join Point ：可以作为切入点的机会，所有方法都可以作为切入点（hello方法）
     >
     > Pointcut ： Aspect实际被应用在的Join Point ，支持正则
     >
     > Advice ：（上面Aspect）类里的方法以及这个方法如何织入到目标方法的方式
     >
     > Weaving：Aop的实现过程（即将切面运用到实际对象，从而创建出一个新的代理对象的过程）

     Advice的种类：

     > 前置通知（before）
     >
     > 后置通知（AfterReturning）
     >
     > 异常通知（AfterThrowing）
     >
     > 最终通知（After）
     >
     > 环绕通知（Around）

5. **Spring中使用了哪些设计模式**

   - 简单工厂模式：是由一个工厂类根据你传入的参数，动态决定应该创建哪一个类，在 BeanFactory 以及 ApplicationContext 创建中都有用到

     > Spring中的BeanFactory就是简单工厂模式的体现，根据传入一个唯一的标识来获得Bean对象，不过是否在传入参数后创建还是传入参数前创建这个要根据具体情况来定

   - 代理模式：通过代理对象访问目标对象。好处是可以在目标对象实现的基础上增强额外的功能

     > 在 AOP 中有体现，见上 AOP 内容
     >
     > 代理模式：接口 + 真实实现类 + 代理类，而真实实现类和代理类都是要实现接口的，实例化的时候需要代理类，所以 Spring AOP 要做的是生成一个代理类来替换掉真实实现类来对外提供服务

   - 观察者模式：定义对象间的一种一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新

     大白话：其实就像发布订阅模式，发布者发布信息，订阅者这边获取信息

     > Spring 中 Observer 模式常用的地方是 listener 的实现。如 ApplicationListener

   - 单例模式：一个类只有一个对象实例

     > 1.私有构造方法；2.在本类的成员位置，创建出自己类；3.对象提供公共方法，返回创建的对象

     懒汉式 DCL 双重校验锁（Double Check Lock 双端检锁机制 - 线程安全、高效）

     ```java
     public class Singleton {
             private static volatile Singleton instance = null；
     
             private Singleton() {}
     
             public static Singleton getInstance() {
                 //对获取实例的方法进行同步
                 if (instance == null) {
                     synchronized (Singleton.class) {
                         if (instance == null) instance = new Singleton()；
                     }
                 }
                 return instance；
             }
     }
     ```
     
     > 不要在方法是用 synchronized，这是重锁把整个方法锁了。而用同步代码块在加锁之前和之后进行判断。在初始化时加上 volatile 可以防止高并发情况下的指令重排
     
   - 模板方法模式：用来解决代码重复的问题

   - 前端控制器模式：Spring 提供了 DispatcherServlet 来对请求进行分发

   - 依赖注入模式：贯穿于 BeanFactory / ApplicationContext 接口的核心理念

   > 责任链模式
   >
   > 装饰者模式

6. **Spring Bean的作用域**

   - singleton ： Spring的默认作用域，每个容器中只有一个Bean的实例
   - prototype ：针对每个getBean请求，容器都会创建一个Bean实例

   - request ：会为每个Http请求创建一个Bean实例，在请求完成以后Bean会失效并被垃圾回收器回收
   - session ：会为每个session创建一个Bean实例，在session过期后，Bean会随之失效
   - globalsession ：会为每个全局Http session创建一个Bean实例，该作用域仅对Portlet有效

7. **解释Spring框架中bean的生命周期。**

   ![1553956304046](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/1553956304046.png)

   Spring Bean 生命周期比较复杂，可以分为创建和销毁两个过程。首先，创建 Bean 会经过一系列的步骤，主要包括：

   - 实例化 Bean 对象以及设置 Bean 属性
   - 如果我们通过各种 Aware 接口声明了依赖关系，则会注入 Bean 对容器基础设施层面的依
     赖。具体包括 BeanNameAware、BeanFactoryAware 和 ApplicationContextAware，分
     别会注入 Bean ID、Bean Factory 或者 ApplicationContext
   - 调用 BeanPostProcessor 的前置初始化方法 postProcessBeforeInitialization。（在Spring完成实例化之后呢，对Spring容器实例化的bean添加一些自定义处理的逻辑）
   - 如果实现了 InitializingBean 接口，则会调用 afterPropertiesSet 方法。（做一些属性被设置之后的自定义的事情）
   - 之后会调用 Bean 自身定义的 init 方法（去做一些初始化相关的工作）
   - 调用 BeanPostProcessor 的后置初始化方法 postProcessAfterInitialization。（去做一些bean实例初始化之后的自定义工作
   - Bean初始化完成

   当Bean不在被用到的时候，便会来到清理的阶段。销毁过程：

   - 若实现了DisposableBean接口，则会调用destroy方法
   - 若配置了destry-method属性，则会调用其配置的销毁方法

   > Aware，是感应、感知的意思。当bean实现了对应的Aware接口时，BeanFactory会在生产bean时根据它所实现的Aware接口，给bean注入对应的属性，从而让bean获取外界的信息

8. **Spring事务中的隔离级别**

   读未提交（read uncommitted）：最低级别，可能会导入脏读

   读已提交（read committed）：可以避免脏读，只能查询到已经提交的数据。且具有良好的性能，但是不能避免不可重复读和幻读

   可重复读（repeatable）：解决了不可重复读，可能会出现幻读

   串行化（serializable）：通过加锁，使同一时间只能执行一个事务，不出现上述问题，但是可能会导致大量的超时现象和锁竞争

   > 另外，MySQL 中默认的隔离级别是可重复读。Oracle 中默认的事务隔离级别是读已提交。

9. **Spring 事务中的事务传播行为**

   支持当前事务的情况： 

   - TransactionDefinition.PROPAGATION_REQUIRED： 如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务
   - TransactionDefinition.PROPAGATION_SUPPORTS： 如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行
   - TransactionDefinition.PROPAGATION_MANDATORY： 如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。（mandatory：强制性）

   不支持当前事务的情况： 

   - TransactionDefinition.PROPAGATION_REQUIRES_NEW： 创建一个新的事务，如果当前存在事务，则把当前事务挂起
   - TransactionDefinition.PROPAGATION_NOT_SUPPORTED： 以非事务方式运行，如果当前存在事务，则把当前事务挂起
   - TransactionDefinition.PROPAGATION_NEVER： 以非事务方式运行，如果当前存在事务，则抛出异常

   其他情况：

   - TransactionDefinition.PROPAGATION_NESTED： 如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价TransactionDefinition.PROPAGATION_REQUIRED

10. **Spring 如何管理 Bean 的**

   Spring 在初始化的时候将配置文件中 Bean 以及相对应关系的配置都加入到 ApplicationContext通过一系列的转换将这些 Bean 实例化，Bean 被它进行了管理，所以 ApplicationContext 就扮演了一个容器的角色

11. **@Transactional 注解在什么情况下会失效**

    同一个类中， 一个未标注 @Transactional 的方法去调用标有 @Transactional 的方法， 事务会失效在非 public 方法上标注 @Transactional， 事务无效

12. **自定义注解的实现原理**

    注解本质是一个继承了 Annotation 的特殊接口，其具体实现类是 Java 运行时生成的动态代理类。而我们通过反射获取注解时，返回的是 Java 运行时生成的动态代理对象 $Proxy1。通过代理对象调用自定义注解（接口）的方法，会最终调用 AnnotationInvocationHandler 的 invoke 方法。该方法会从 memberValues 这个 Map 中索引出对应的值。而 memberValues 的来源是 Java 常量池

13. **动态代理的两种方式，以及区别**

    动态代理分为两种一种是 JDK 反射机制提供的代理、另一种是 CGLIB 代理。区别在 JDK 代理，必须提供接口、CGLIB 则不需要提供接口。在 Mybatis 里两种动态代理技术都已经使用了，在 Mybatis 中通常在延迟加载的时候才会用到 CGLIB 动态代理

14. **Mybatis 中的 ${} 和 #{} 的区别 **

    i. #{} 是预编译处理可以防止 SQL 注入

    ii. ${} 对传递进来的参数原样拼接在 SQL 中

15. **Mybatis 中的一级缓存和二级缓存 **

    我们知道，频繁的数据库操作是非常耗费性能的，尤其是对于一些相同的查询语句，完全可以把查询结果存储起来，下次查询同样的内容的时候直接从内存中获取数据即可，这样在某些场景下可以大大提升查询效率。

    一级缓存是 Sqlsession 级别的缓存。当我们执行查询时查询的结果会同时存入到 Sqlsession 提供一个 Map 区域中。对于相同的查询，会从缓存中返回结果而不是查询数据库

    二级缓存是 Mapper 级别的缓存，定义在 Mapper 文件的 `<cache>` 标签中并需要开启此缓存，多个 Mapper 文件可以共用一个缓存，依赖 `<cache-ref>` 标签配置

    > 二级缓存的使用步骤：
    >
    > 第一步：让 Mybatis 框架支持二级缓存 (在 SqIMapConfig.xml 中配置)
    >
    > 第二步：让当前的映射文件支持二级缓存 (在 1UserDao.xml 中配置)
    >
    > 第三步：让当前的操作支持二级缓存 (在 select 标签中配置)

16. **当实体类中的属性名和表中的字段名不一样 ，怎么办 ？**

    通过 `<resultMap>` 来映射字段名和实体类属性名的一一对应的关系

17. **Dao 接口的工作原理是什么？Dao 接口里的方法，参数不同时，方法能重载吗？**

    Dao 接口，就是人们常说的 Mapper 接口，接口的全限名，就是映射文件中的 namespace 的值，接口的方法名，就是映射文件中 MappedStatement 的 id 值，接口方法内的参数，就是传递给 sql 的参数。
    Mapper 接口是没有实现类的，当调用接口方法时，接口全限名 + 方法名拼接字符串作为 key 值，可唯一定位一个 MappedStatement

    Dao 接口里的方法，是不能重载的，因为是全限名 + 方法名的保存和寻找策略。

    Dao 接口的工作原理是 JDK 动态代理，Mybatis 运行时会使用 JDK 动态代理为 Dao 接口生成代理 proxy 对象，代理对象 proxy 会拦截接口方法，转而执行 MappedStatement 所代表的 sql，然后将 sql 执行结果返回。

18. **Mybatis 是否支持延迟加载？如果支持，它的实现原理是什么？**

    Mybatis 仅支持 association 关联对象和 collection 关联集合对象的延迟加载，association 指的就是一对一，collection 指的就是一对多查询。在 Mybatis 配置文件中，可以配置是否启用延迟加载 lazyLoadingEnabled=true|false。

    它的原理是，使用 CGLIB 创建目标对象的代理对象，当调用目标方法时，进入拦截器方法，比如调用 a.getB ().getName ()，拦截器 invoke () 方法发现 a.getB () 是 null 值，那么就会单独发送事先保存好的查询关联 B 对象的 sql，把 B 查询上来，然后调用 a.setB (b)，于是 a 的对象 b 属性就有值了，接着完成 a.getB ().getName () 方法的调用。这就是延迟加载的基本原理。

    当然了，不光是 Mybatis，几乎所有的包括 Hibernate，支持延迟加载的原理都是一样的。

19. **Mybatis 都有哪些 Executor 执行器？它们之间的区别是什么？**

    SimpleExecutor：每执行一次 update 或 select，就开启一个 Statement 对象，用完立刻关闭 Statement 对象

    ReuseExecutor：执行 update 或 select，以 sql 作为 key 查找 Statement 对象，存在就使用，不存在就创建，用完后，不关闭 Statement 对象，而是放置于 Map<String， Statement> 内，供下一次使用。简言之，就是重复使用 Statement 对象

    BatchExecutor：执行 update（没有 select，JDBC 批处理不支持 select），将所有 sql 都添加到批处理中（addBatch ()），等待统一执行（executeBatch ()），它缓存了多个 Statement 对象，每个 Statement 对象都是 addBatch () 完毕后，等待逐一执行 executeBatch () 批处理。与 JDBC 批处理相同

### MQ | 消息队列

[RabbitMQ 可靠性、重复消费、顺序性、消息积压解决方案 - 掘金](https://juejin.cn/post/6977981645475282958)

[尚硅谷RabbitMQ教程丨快速掌握MQ消息中间件_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1cb4y1o7zz?p=84&spm_id_from=pageDriver&vd_source=4844de7cb051be29fbaf4555af0bbd8b)

![RabbitMQ](http://rjnbyrj6g.hn-bkt.clouddn.com/interview/1*HoAG-7IhLaXShPJG-g9kvA.png)

[RabbitMQ, What is that?. RabbitMQ is one part of Message Broker… | by Ahmad Kamil Almasyhur | Medium](https://medium.com/@kamilmasyhur/rabbitmq-what-is-that-10c74ac7620a#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6ImJhMDc5YjQyMDI2NDFlNTRhYmNlZDhmYjEzNTRjZTAzOTE5ZmIyOTQiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJuYmYiOjE2NjQ0MzU5MDAsImF1ZCI6IjIxNjI5NjAzNTgzNC1rMWs2cWUwNjBzMnRwMmEyamFtNGxqZGNtczAwc3R0Zy5hcHBzLmdvb2dsZXVzZXJjb250ZW50LmNvbSIsInN1YiI6IjEwOTI1NTI3Mjg3NDYzMzY4NTAyMiIsImVtYWlsIjoibGl1MTE0Mjg2NTM5NkBnbWFpbC5jb20iLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwiYXpwIjoiMjE2Mjk2MDM1ODM0LWsxazZxZTA2MHMydHAyYTJqYW00bGpkY21zMDBzdHRnLmFwcHMuZ29vZ2xldXNlcmNvbnRlbnQuY29tIiwibmFtZSI6IkRhbmllbCBMaXUiLCJwaWN0dXJlIjoiaHR0cHM6Ly9saDMuZ29vZ2xldXNlcmNvbnRlbnQuY29tL2EtL0FDTlBFdThyc1BoOThMRjVDb1BYRmtaak44V2FWLTNfRTVpR1lIaklWTS04QlE9czk2LWMiLCJnaXZlbl9uYW1lIjoiRGFuaWVsIiwiZmFtaWx5X25hbWUiOiJMaXUiLCJpYXQiOjE2NjQ0MzYyMDAsImV4cCI6MTY2NDQzOTgwMCwianRpIjoiNWRjYzFlNGRjNTQ4YjVkMWJkZTMxOWM0NTRlNmIzZmY1NzViNTgxMiJ9.iS-nFDj3l23wlcX1iy6aYaii1hbHJ_fZoqyBzZUXHlDutu6Tpsvxf0CsX0DWYeg0VUj-4vXmcYxdQoTI_Qw_dL5rU0y8D1ycjehKrBxvnTPzIQGRsQEtQUHXIVdWaSoGpybHM99LNiSJgxuHcfg5fKByWnqGxBdxw4ZIVbwoP0xslLdFSH7XrPQW8NZkIOpz9EVWM4VSjX7HaSV-EeqqiJP9ZuXzQ7aV5bgZzfgIrC1xBbR-VRZ5DScQBiLftbEMz17qSgcJZhl5j4NCRaMfomyHHB7wNByI7Y5ztxif8hWi8HDq5RqEOqbpiPR0KX5pwHHChT1dl65EAaoX-X5RWA)

[mq消息积压中，突然mq挂了，或者mysql挂了，或者两个都挂了怎么处理？](https://www.zhihu.com/question/475985829/answer/2680703908)

RabbitMQ 消费端使用 @RabbitMQListener 来监听消费消息

```properties
# 消费者数量，定义为 1 就是串行的。也可以定义为多个消费者来加快出队的速度
spring.rabbitmq.listener.simple.concurrency= 10
spring.rabbitmq.listener.simple.max-concurrency= 10
# 一次从队列中取一个消息，如果消息堆积比较严重的话可以设置为多个来加快消费。但是如果有数据长时间不消费会导致数据一直卡在这里。秒杀这里不能长时间不处理，所以设置为 1
spring.rabbitmq.listener.simple.prefetch= 1
# 消费者自动启动
spring.rabbitmq.listener.simple.auto-startup=true
# 消费者消费失败后重新入队，相当于重试
spring.rabbitmq.listener.simple.default-requeue-rejected= true
# 重试相关配置
spring.rabbitmq.template.retry.enabled=true
# 初始多少秒后重试第一次
spring.rabbitmq.template.retry.initial-interval=1000
# 最大重试次数
spring.rabbitmq.template.retry.max-attempts=3
# 间隔多少秒，不是等分的，如果 multiplier 为 2，相当于每次 interval*multiplier
spring.rabbitmq.template.retry.max-interval=10000
spring.rabbitmq.template.retry.multiplier=1.0
```

1. **为什么使用消息队列啊**

   > 其实就是问问你消息队列都有哪些使用场景，然后你项目里具体是什么场景，说说你在这个场景里用消息队列是什么
   >
   > 面试官问你这个问题，期望的一个回答是说，你们公司有个什么业务场景，这个业务场景有个什么技术挑战，如果不用 MQ 可能会很麻烦，但是你现在用了 MQ 之后带给了你很多的好处

   因为消息队列的优点可以解耦、削峰、异步。在我们有个订单系统，订单系统会每次下一个新的订单的时候，就会发送时一条消息到 MQ 里面去，后台有个库存系统负责获取了消息然后更新库存。

   我们订单系统里面下单之后会用 MQ 的延时队列在半小时后查看订单是否支付，如果没有支付就把订单状态改为取消支付

   发邮件、发送注册短信都需要连接外部的服务器，需要额外等待一段时间。此时就可以用消息队列来做异步处理，实现快速响应

   - 解耦
     
     在我们做的小程序商城中，如果订单系统耦合着调用支付系统、库存系统、物流系统。那么任何一个子系统出了故障，都会造成下单操作异常。那么引入消息队列之后就解决这个问题。比如物流系统因为发生故障需要几分钟来修复。在这几分钟的时间里，物流系统需要处理的数据就会缓存在消息队列中，并不会影响用户的正常下单。当物流系统恢复后继续处理订单信息就可以了，下单用户对物流系统的故障是无感知的，从而提高了系统的可用性
     
     ![MQ_01_MQ在解耦场景中的使用](http://rjnbyrj6g.hn-bkt.clouddn.com/interview/MQ_01_MQ%E5%9C%A8%E8%A7%A3%E8%80%A6%E5%9C%BA%E6%99%AF%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8.jpg)

   - 削峰

     大量用户（100万用户）在高峰期同时在进行操作，每秒 5000 个请求。（当然我们系统还没这么大的量），每秒 5000 个请求写入 MQ，系统 A 每秒钟最多只能处理 2000 个请求，因为 MySQL 每秒钟最多处理 2000 个请求，系统 A 从 MQ 中慢慢拉取请求，每秒钟就拉取 2000 个请求，不要超过自己每秒能处理的最大请求数量就 OK 了，哪怕是高峰期，系统 A 也不会挂掉。MQ 每秒 5000 个请求进来，结果就 2000 个请求出去，导致在中午高峰期（1 个小时），可能有几十万甚至几百万的请求积压在 MQ 中。但是这短暂的积压是 OK 的，因为高峰期过了之后，每秒钟就只有几十个请求进 MQ 了。但是系统 A 还是会按照每秒 2000 个请求的速度来把积压的消息处理掉

     ![MQ_03_使用MQ来进行消峰的场景](http://rjnbyrj6g.hn-bkt.clouddn.com/interview/MQ_03_%E4%BD%BF%E7%94%A8MQ%E6%9D%A5%E8%BF%9B%E8%A1%8C%E6%B6%88%E5%B3%B0%E7%9A%84%E5%9C%BA%E6%99%AF.jpg)

   - 异步

     可以提高系统响应速度。用户发起一个请求，系统 A 接到后需要在自己本地执行 SQL 耗时 20ms，然后需要分别调用 BCD 三个系统进行数据库操作分别耗时 300ms。这样下来一个请求总耗时接近 1s，这种响应速度对于互联网用户来说是不可接收的，单个请求至少都要控制在 200ms 以内才能留住客户
     
     而在用了消息队列以后，系统 A 操作完以后连续发送 3 条消息到队列中耗时 5ms，然后就立马返回了。而其他系统就到 MQ 中去消费对应的数据来执行本地操作，这样总耗时才 25ms
     
     ![MQ_02_使用MQ进行异步化之后的接口性能优化](http://rjnbyrj6g.hn-bkt.clouddn.com/interview/MQ_02_%E4%BD%BF%E7%94%A8MQ%E8%BF%9B%E8%A1%8C%E5%BC%82%E6%AD%A5%E5%8C%96%E4%B9%8B%E5%90%8E%E7%9A%84%E6%8E%A5%E5%8F%A3%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96.jpg)

2. **消息队列有什么优点和缺点**

   优点：解耦、削峰、异步。（一致性、点对点）

   缺点： （参考异步的那张图）

   - 系统可用性降低

     因为本身 MQ 就有可能挂掉，这样直接导致不可用了

   - 系统复杂性提高

     - 重复消费（幂等）：系统 A 给 MQ 发送一条消息，结果 MQ 给系统 B 重复发送了 2 条消息，导致消息被重复消费，往数据库插入了 2 条同样的数据
     - 消息丢失：系统 A 给 MQ 发送一条消息，结果 MQ 在内部给搞丢了
     - 顺序性：本来系统 A 分别给 BCD 系统按顺序发了三条系统，结果 MQ 发送时顺序变为了 DCB
     - 消息积压：如果系统 B 挂了，结果导致大量的消息积压在 MQ 中无法被消费，积压几个小时，磁盘都满了

   - 一致性问题

     用户给系统 A 发送请求，本来这个请求应该是系统 ABCD 都执行成功才能返回的结果，系统 ABC 执行都成功了，但系统 D 执行执行失败了。就导致整个请求给用户返回的是成功，但后台逻辑没完全执行成功。导致数据一致性问题

3. **RabbitMQ、RocketMQ、Kafka 都有什么优缺点？为什么选用 RabbitMQ（技术选型）**

   [Kafka、RabbitMQ、RocketMQ 之间的区别是什么 ? - 四猿外的回答 - 知乎](https://www.zhihu.com/question/275090117/answer/2298925188)

   [RabbitMQ在国内为什么没有那么流行？ - kimmking的回答 - 知乎](https://www.zhihu.com/question/449611434/answer/1824707689)

   RabbitMQ：

   - 优点：1. 吞吐量到万级，功能比较完备 2. 开源了一套完善的管理界面，用起来很舒服。搭起来之后直接能在后台看到积压的消息、每秒承载请求数量的变化等等 3. 社区活跃高，版本发布的很勤，在快速迭代快速修复 Bug
   - 缺点：Erlang 语言难度较大。基本只能依赖于开源社区的快速维护和修复bug，不利于做二次开发和维护

   RocketMQ：

   - 优点：1. 单机吞吐量能达到 10w，分布式的扩展起来容易些 2. 还可以支撑大规模的 topic 数量，支持 MQ 复杂业务场景 3. Java 语言开发我们就可以自己阅读源码来定制公司自己的 MQ
   - 缺点：还有就是阿里出台的技术，你得做好这个技术万一被抛弃的风险

   Kafka：

   - Kafka采用拉取（Pull）方式消费消息，吞吐量相对更高，适用于海量数据收集与传递场景，例如日志采集和集中分析。

     | 特性                                   | RabbitMQ                                                     | RocketMQ                                         | Kafka                                                        |
     | -------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------ | ------------------------------------------------------------ |
     | 单机吞吐量（QPS 一秒能处理多少个请求） | 几万，做到了开箱即用，基本无需配置                           | 10万级                                           | 10万级，为了更好的吞吐量，很大程度上会增加了自己的复杂度。配置复杂、维护复杂 |
     | 延迟队列、死信队列                     | 支持                                                         | 支持                                             | 不支持（非常麻烦）                                           |
     |                                        | 并发强，性能好，延时低，开源了一套完善的管理界面             |                                                  |                                                              |
     | 消息消费顺序                           | 无法保证消费顺序（如果有消息执行失败，会重新入队）           | 保证分区按顺序消费                               | 保证分区按顺序消费                                           |
     | 消息匹配                               | 容易（touting_key 或自定义消息头，再通过一些特殊的 Exchange） |                                                  | 很麻烦                                                       |
     | 消息保持（消息回溯）                   | 消息被人取出来就被删除了                                     |                                                  | 消息会被持久化一个专门的日志文件里。不会因为被消费了就被删除。 |
     | 消息错误处理                           | 消息消费出问题允许跳过                                       |                                                  | 消息消费出问题不允许跳过                                     |
     | 语言                                   | Erlang                                                       | Java                                             | Scala                                                        |
     | 消息堆积                               |                                                              | 支持10亿级别的消息堆积，不会因为堆积导致性能下降 |                                                              |

   延时队列：（在电商业务里，有个需求：下单之后，如果用户在 15 分钟内未支付，则自动取消订单。）在单一服务的系统，可以起个定时任务就搞定了。但是，在微服务架构下，这样做就不行了。因为很多个服务都关心是否支付这件事，如果每种服务，都自己实现一套定时任务的逻辑，既重复，又难以维护。

   在这种情况下，我们往往会做一层抽象：把要执行的任务封装成消息。当时间到了，直接扔到消息队列里，消息的订阅者们获取到消息后，直接执行即可。希望把消息延迟一定时间再处理的，被称为延迟队列。对于订单取消的这种业务，我们就会在创建订单的时候，同时扔一个包含了执行任务信息的消息到延迟队列，指定15分钟后，让订阅这个队列的各个消费者，可以收到这个消息。随后，各个消费者所在的系统就可以去执行相关的扫描订单的任务了。

   > RabbitMQ 的消息自带手表，消息中有个 TTL 字段，可以设置消息在 RabbitMQ 中的存放的时间，超时了会被移送到一个叫死信队列的地方。
   >
   > 所以，延迟队列 RabbitMQ 最简单的实现方式就是设置 TTL，然后一个消费者去监听死信队列。当消息超时了，监听死信队列的消费者就收到消息了。
   >
   > 不过，这样做有个大问题：假设，我们先往队列放入一条过期时间是 10 秒的 A 消息，再放入一条过期时间是 5 秒的 B 消息。 那么问题来了，B 消息会先于 A 消息进入死信队列吗？
   >
   > 答案是否定的。B 消息会优先遵守队列的先进先出规则，在 A 消息过期后，和其一起进入死信队列被消费者消费。
   >
   > 在 RabbitMQ 的 3.5.8 版本以后，官方推荐的 rabbitmq delayed message exchange 插件可以解决这个问题。
   >
   > - 用了这个插件，我们在发送消息的时候，把消息发往一个特殊的 Exchange。
   > - 同时，在消息头里指定要延迟的时间。
   > - 收到消息的 Exchange 并不会立即把消息放到队列里，而是在消息延迟时间到达后，才会把消息放入。

   消息保持：事件溯源有个最经典的场景，就是事件的重放。简单来讲就是把系统中某段时间发生的事件依次取出来再处理。而且，根据业务场景不同，这些事件重放很可能不是一次，更可能是重复 N 次。假设，我们现在需要一批在线事件重放，去排查一些问题。

   > 吞吐量不具体，还得根据你的服务器配置相关。1w、5w 还是 8w，跟服务器配置 CPU、内存、磁盘有很大的关系
   >
   > 延时低：指消息到 MQ 再发出来的时间很低

   总结：不过现在确实越来越多的公司，会去用RocketMQ，确实很不错，但是我提醒一下自己想好社区万一突然黄掉的风险，对自己公司技术实力有绝对自信的，我推荐用RocketMQ，否则回去老老实实用RabbitMQ吧，人是活跃开源社区，绝对不会黄所以中小型公司，技术实力较为一般，技术挑战不是特别高，用RabbitMQ是不错的选择

   大型公司，基础架构研发实力较强，如果官方放弃维护了，公司内部能拉出一组的人去研究源码并定制修改，那用RocketMQ是很好的选择

   如果是大数据领域的实时计算、日志采集等场景，不管公司规模大小。用Kafka是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范

4. **如何保证消息队列的高可用**

   RabbitMQ 不是分布式的，RabbitMQ 有三种模式：单机模式，普通集群模式，镜像集群模式

   - 单机模式

     单台机器并发能力不够；而且无法保证高可用，如果遇到内存崩溃、机器断电就玩完了

   - 集群模式

     > 图解参考镜像队列

     RabbitMQ 集群允许消费者和生产者在 RabbitMQ 单个节点崩惯的情况下继续运行， 它可以通过添加更多的节点来线性地扩展消息通信的吞吐量。当失去一个 RabbitMQ 节点时， 客户端能够重新连接到集群中的任何其他节点并继续生产或者消费。此时集群中的所有节点都会备份所有的元数据信息，但是主节点上的所有队列中的消息也会丢失，所以无法保证高可用

   - 镜像队列

     ![MQ_04_RabbitMQ的镜像队列模式](http://rjnbyrj6g.hn-bkt.clouddn.com/interview/MQ_04_RabbitMQ%E7%9A%84%E9%95%9C%E5%83%8F%E9%98%9F%E5%88%97%E6%A8%A1%E5%BC%8F.jpg)

     引入镜像队列（Mirror Queue）机制，可以将队列（queue）镜像到集群中的其他 Broker 节点之上，然后每次写消息到队列时，都会自动同步消息到多个实例的队列里面。如果集群中的一个节点失效了，队列能自动地切换到镜像中的另一个节点上以保证服务的可用性。通常都是一个（master）主节点和若干个（slave）从节点

     > 缺点是：1. 性能开销也太大了，消息同步所有机器，导致网络带宽压力和消耗很重 2. 这个根本没有扩展性可言了，如果某个 queue 负载很重，选择加机器，但新增的机器也包含了这个 queue 的所有数据，并没有办法线性扩展

     那么怎么开启这个镜像队列呢？

     > 在 RabbitMQ 管理控制台的 Admin - Policies 里面添加一个镜像队列策略，指定的时候可以要求数据同步到所有节点的，但一般都同步到一个节点就好了，这样避免浪费资源。（比如三个节点，主节点有 100w 数据，同步到另外两个节点的话，一共再同步 200w 数据）

5. **如何保证消息不被重复消费啊（如何保证消息消费时的幂等性）**

   **场景：**

   ![Kafka原理图解](https://res.cloudinary.com/hevo/images/f_auto,q_auto/v1642485693/hevo-learn/Kafka-Clusters-Kafka-Clusters-Architecture-Diagram/Kafka-Clusters-Kafka-Clusters-Architecture-Diagram.png?_i=AA)

   Kafka 针对 partition 有个 offset（偏移量）的概念，就是每个消息写进去都会记录所在的 offset，然后消费者消费了数据之后，每隔一段时间会把 offset 提交到 zookeeper，代表我已经消费到这里了。但假如生产环境正在重启的时候 offset 数据还没来得及提交，重启过后消费组又会去重新拉取之前的消息，结果就导致重复消费了

   > Kafka 分布式的分区越多（一个分区就是单机），支持的并发量（吞吐量）越高。单机的话差不多是 10w 级，消费者吞吐量为 40 多万。单分区 3 个副本就降为了 3w 左右吞吐量，因为要同步到副本
   >
   > 一个 topic 是一类消息，比如支付完之后会发送支付消息；商品上架发送的商品消息
   >
   > Kafka 的 Broker 是无状态的，用 ZooKeeper 来保存 Kafka 集群状态

   ==**怎么保证消息队列消费的幂等性：**==

   > 拿着数据要插入操作时，先根据主键去查一下，如果已经有数据了那就 update 一下

   - 用 Redis 天然的 Set 来进行去重
   - 让生产者发送每条数据的时候，里面加一个全局唯一的 id，类似订单 id 之类的东西，在准备消费的时候先根据这个 id 去 Redis 里查一下之前消费过没。如果没有消费过就进行业务处理然后这个 id 写到 Redis。如果已经消费过不做处理避免重复消费

   > 一般来说消息重复消费都是在短暂的一瞬间消费多次，我们可以使用 redis 将消费过的消息唯一标识存储起来，然后在消费端业务执行之前判断 redis 中是否已经存在这个标识。举个例子，订单支付成功后，要通知积分系统增加积分。这里可以用订单号 + 积分 id 做唯一标识。业务开始先判断 redis 是否已经存在这个标识，如果已经存在代表处理过了就不处理业务。不存在就把唯一标识就放进 redis 并设置过期时间，然后执行业务

6. **如何保证消息的可靠性传输（如何处理消息丢失的问题）**

   > 如果说你这个是用来传递非常核心的消息，比如说计费，扣费的一些消息，因为我以前设计和研发过一个公司非常核心的广告平台，计费系统，计费系统是很重的一个业务，操作是很耗时的。所以说广告系统整体的架构里面，实际上是将计费做成异步化的，然后中间就是加了一个MQ。
   >
   > 我们当时为了确保说这个MQ传递过程中绝对不会把计费消息给弄丢，花了很多的精力。广告主投放了一个广告，明明说好了，用户点击一次扣费1块钱。结果要是用户动不动点击了一次，扣费的时候搞的消息丢了，我们公司就会不断的少几块钱，几块钱，积少成多，这个就对公司是一个很大的损失。

   消息丢失可能出现的情况：

   - **生产者弄丢消息**

     问题：生产者将数据发送到 RabbitMQ 的时候，数据在网络传输中丢了；或者消息到了 RabbitMQ 但内部出错消息没保存下来

     方案一&#x274E;：事务机制

     事务机制是同步的，生产者发送一个消息会同步阻塞，等待成功还是失败。这样会严重导致降低 RabbitMQ 的消息吞吐量

     > 在通过 channel.txSelect 方法开启事务之后再发消息给 RabbitMQ，此时如果事务提交成功，那么消息一定到达了 RabbitMQ 中，但如果在事务提交执行之前由于 RabbitMQ 异常崩溃或者其他原因抛出异常，这个时候我们可以进行捕获，然后通过执行 channel.txRollback 方法来实现事务回滚

     ```java
     channel.txSelect();
     channel.basicPublish(EXCHANGE_NAME,ROUTING_KEY,
     	MessageProperties,PERSISTENT_TEXT_PLAIN,
     	"transaction messages".getBytes());
     channel.txCommit();
     // channel.txSelect;
     // try{
     //     // 发送消息
     // } catch (Exception ex){
     //     channel.txRollback;
     //     // 再次重试发送该消息
     // }
     // channel.txCommit;
     ```

     方案二&#x2705;：发送方确认机制（publisher comfirm）

     生产者开启 confirm 模式后，每次写的消息都会分配一个唯一 ID，消息被投递到所有匹配的队列之后，RabbitMQ 就会发送一个 Ack 消息给生产者，来让生产者知道消息已经送达

     > Comfirm 方式有普通、批量、异步三种方式。推荐异步 comfirm 方式，吞吐量能达到万级；事务机制只能达到 2000 左右
     >
     > 实际上据我了解一些企业并不会在这两个监听里面去做重发，为什么呢？成本太高了......首先 RabbitMQ 本身丢失的可能性就非常低，其次如果这里需要落库再用定时任务扫描重发还要开发一堆代码，分布式定时任务......再其次定时任务扫描肯定会增加消息延迟，不是很有必要。真实业务场景是记录一下日志就行了，方便问题回溯，顺便发个邮件给相关人员，如果真的极其罕见的是生产者弄丢消息，那么开发往数据库补数据就行了。

   - **RabbitMQ 弄丢消息**

     问题：RabbitMQ 接收到消息之后暂存在内存中，但消费者还没来得及消费就挂了的话会导致内存中数据丢失。所以需要用持久化机制来避免数据丢失

     方案：持久化机制

     i. 创建队列时设置为持久化，这样就可以保证 RabbitMQ 持久化 queue 的元数据到磁盘上。但是它不会持久化 queue 里的消息；ii. 发送消息的时候将消息的 deliveryMode 设置为 2，就是将消息设置为持久化的，此时 RabbitMQ 就会将消息持久化到磁盘上去。只有这两个配置要同时配置上，才能保证 RabbitMQ 挂了重启之后也能从磁盘上恢复 queue 里面的消息

     > 不开启持久化的情况下 RabbitMQ 重启之后所有队列和消息都会消失，所以我们创建队列时设置持久化，发送消息时再设置消息的持久化即可（设置 deliveryMode 为 2 就行了）。一般来说在实际业务中持久化是必须开的。

   - **消费者弄丢消息**

     问题：消费者接收到消息还没来得及处理就挂了，RabbitMQ 重启后会从之后的消息开始读取，前面的消息就丢了

     RabbitMQ 提供了消费者应答（ack）机制，默认情况下是自动应答，只要消息推送到消费者就会autoAck，然后从 RabbitMQ 队列中删除消息。但要是消费端执行业务代码报错了没执行成功、宕机了。消费端就返回 ack 消息给 RabbitMQ，然后把这条消息删除了，这样相当于消息变相的丢失了。

     方案：手动应答。关闭自动应答改为启用手动应答之后我们在消费端成功处理完业务之后。调用 API 手动 ack 确认，然后才让 RabbitMQ 从队列删除这条消息

     > kafka
     >
     > 唯一可能导致消费者弄丢数据的情况，就是说，你那个消费到了这个消息，然后消费者那边自动提交了 offset，让 kafka 以为你已经消费好了这个消息，其实你刚准备处理这个消息，你还没处理，你自己就挂了，此时这条消息就丢咯。
     >
     > 生产环境 kafka 的 leader 机器宕机了，将 follower 切换为 leader 之后，就会发现说这个数据就丢了所以此时一般是要求起码设置如下 4 个参数：
     >
     > 给这个 topic 设置 replication.factor 参数：这个值必须大于 1，要求每个 partition 必须有至少 2 个副本
     >
     > 在 kafka 服务端设置 min.insync.replicas 参数：这个值必须大于 1，这个是要求一个 leader 至少感知到有至少一个 follower 还跟自己保持联系，没掉队，这样才能确保 leader 挂了还有一个 follower 吧
     >
     > 在 producer 端设置 acks=all：这个是要求每条数据，必须是写入所有 replica 之后，才能认为是写成功了在 producer 端设置 retries=MAx（很大很大很大的一个值，无限次重试的意思）：这个是要求一旦写入失败，就无限重试，卡在这里了

7. **如何保证消息的顺序性**

   > 我们以前做过一个mysql binlog同步的系统，压力还是非常大的，日同步数据要达到上亿。mysql->mysql，常见的一点在于说大数据team，就需要同步一个mysql库过来，对公司的业务系统的数据做各种复杂的操作。
   >
   > 你在mysql里增删改一条数据，对应出来了增删改3条binlog，接着这三条binlog发送到MQ里面，到消费出来依次执行，必须要保证顺序来执行。不然本来是：增加、修改、删除；执行顺序变为了删除、修改、增加，就会导致数据不一致。

   有些业务场景会需要让消息顺序消费，比如使用 canal 订阅 MySQL 的 binary 日志来更新 Redis，通常我们会把 canal 订阅到的数据变化发送到消息队列。

   ![消息顺序性业务](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/07bf9772d6594514aebe58a789fa3341~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.awebp)

   其实队列本身是有顺序的（先进先出），但生产环境服务实例一般都是集群，当消费者是多个实例时，队列中的消息会分发到（所有？|对应）实例进行消费（一个消息只会有一个消费者消费到），这样就不能保证消息消费的顺序，因为不能确保哪台机器执行消费端业务代码的速度更快

   所以对于需要保证顺序消费的业务，我们可以只部署一个消费者实例，然后设置 RabbitMQ 每次只推送一个消息，处理完成业务之后手动 ack 应答再消费下一个消息，这样就能确保消息顺序性了

   ```yaml
   spring:
     rabbitmq:
       listener:
         simple:
           prefetch: 1 # 每次只推送一个消息
           acknowledge-mode: manual # 消费端默认的自动提交改为手动提交
   ```

   > 拆分多个 queue，每个 queue 一个消费者，然后把我们要保证顺序的数据只放到一个 queue 里面，这样数据就会只被一个消费者消费到，然后顺序的灌到数据库里面去

8. **有几百万消息持续积压几小时，如何解决消息队列的延时以及过期失效问题？消息队列满了以后该怎么处理？说说怎么解决**

   > 你看这问法，其实本质针对的场景，都是说，可能你的消费端出了问题，不消费了，或者消费的极其极其慢。接着就坑爹了，可能你的消息队列集群的磁盘都快写满了，都没人消费，这个时候怎么办？或者是整个这就积压了几个小时，你这个时候怎么办？或者是你积压的时间太长了，导致比如 RabbitMQ 设置了消息过期时间后就没了怎么办？

   所谓消息积压一般是由于消费端消费的速度远小于生产者发消息的速度（或者消费端直接挂了），导致大量消息在 RabbitMQ 的队列中无法消费。

   之前我们遇到过这种生产环境的线上故障，那会是消费端出故障了，导致百万条（几千万）消息在 MQ 中积压了五六个小时，从下午 4 点多持续到 10 点多

   处理方案：多部署几台消费者实例进行快速消费

   > Kafka：先将现有消费者停掉改了代码，然后新建一个topic，加为原来的10倍的partition，消费者改下代码，改成消费到数据重新给他写到新导出来有30个partition的这么个topic里面去，这时我们可以向公司申请一批机器来部署30个消费者，这30个消费者去消费30个partition，这样就可以以原来10倍的速度去消费，再落库这样子。消费完后可以把机器下掉，再恢复成原来的样子
   >
   > 像原来3个消费者需要1小时才能搞定的，现在10分钟就搞定了

   **消息队列磁盘满了怎么处理？**

   消息积压在 MQ 中长时间没有处理掉，导致磁盘快写满了。此时可以临时写个程序来消费，消费一个丢弃一个都不要了，来快速消费掉所有消息。然后等高峰期过了，再把丢失的那批数据一点点查出来重新发给 MQ 来把白天的数据补回来

10. **RabbitMQ 有哪几种工作模式**

    - 点对点模式：默认的
    - Work queues（工作队列模式）：一个生产者，多个消费者，每个消费者获取到的消息唯一
    - Publish/Subscribe（发布订阅模式）：一个生产者发送的消息会被多个消费者获取
    - Routing（路由模式）&#x2705;：生产者发送消息到交换机时要指定 RoutingKey，消费者将队列绑定到交换机时也需要指定 RoutingKey
    - Topics（主题模式）：像那种 like 模糊查询。将路由键和某模式进行匹配，此时队列需要绑定在一个模式上，“#” 匹配一个词或多个词，“*” 只匹配一个词
    - RPC 模式：
    - Publisher Confirms（发送发确认模式）：官方给定的不确定算不算
    
    > RabbitMQ 常用的交换器类型有 fanout（广播）、 direct（直连）、 topic、 headers（headers 键值对全匹配）这四种
    
10. **如果让你写一个消息队列，该如何进行架构设计啊？说一下你的思路（弱）**

    可以参照 Kafka，RocketMQ 也是参照的 Kafka 做的分布式消息系统

    - 伸缩性（扩容）

      就是需要的时候能快速扩容来增加容量和吞吐量，可以参照 Kafka 的设计一个分布式的系统，（broker -> topic -> partition）每个 broker 是机器上的一个节点，一个 topic 可以拆成多个 partition，每个 partition 放一个机器存一部分数据。如果资源不够了就给 topic 增加 partition，然后做数据迁移增加机器就可以存放更多数据，提供更高的吞吐量了

    - 持久化（写入磁盘）

      要落磁盘才能保证进程如果挂了数据能够不丢失。写磁盘采用顺序写，能够减少磁盘随机读写的寻址开销提高读写性能

    - 高可用

      Kafka 的高可用就是分了多个副本，分为 leader、follower 两个角色，只有 leader 对外提供读写，follower 从 leader 去同步数据，leader 挂了的话就要在剩下的 follower 中重新选举出一个 leader 对外提供服务

    - 可靠性（传输）

      参考之前说的那个 Kafka 数据零丢失方案

### Elasticsearch

[Elasticsearch from the Bottom Up, Part 1 | Elastic Blog](https://www.elastic.co/blog/found-elasticsearch-from-the-bottom-up/)

[Elasticsearch from the Top Down | Elastic Blog](https://www.elastic.co/blog/found-elasticsearch-top-down)

[Elasticsearch 原理图解 - 知乎](https://zhuanlan.zhihu.com/p/62892586)

ES 非常优秀，可以承载几亿的数据量级，每秒几千的 QPS 都没问题。性能控制的好的话几百ms以内是完全没问题的

ELK 使用场景：ELK 系统，也就是日志分析系统。其中 E 就是 Elasticsearch，L 是 Logstash，是一个日志收集系统，K 是 Kibana，是一个数据可视化平台

分析日志的用处可大了，你想，假如一个分布式系统有 1000 台机器，系统出现故障时，我要看下日志，还得一台一台登录上去查看，是不是非常麻烦？

但是如果日志接入了 ELK 系统就不一样。比如系统运行过程中，突然出现了异常，在日志中就能及时反馈，日志进入 ELK 系统中，我们直接在 Kibana 就能看到日志情况。如果再接入一些实时计算模块，还能做实时报警功能。

1. **倒排索引是什么**
   

[倒排索引为什么叫倒排索引？](https://www.zhihu.com/question/23202010/answer/23901671)

其实我觉得它叫 ”反向索引“ 更容易让人理解，一般我们的数据库都是以文档 ID 作为索引，文档内容作为记录；而 Inverted index 是指将单词或记录作为索引，把文档 ID 作为记录，这样可以更方便的通过单词或记录查找到其所在的文档

业务场景：中文用的 ik 分词器（ik_max_word）

   ![ES 组成结构](https://pic1.zhimg.com/v2-1897d394e89794a9776d83e47ec3da98_b.jpg)

   keywords 类型不会进行分词而是直接建立反向索引，text 是所有内容在存入前会先进行分词再根据分词后的内容建立反向索引

   > 就是把你的数据内容先分词，每句话分成一个一个的关键词，然后记录好每个关键词对应出现在了哪些 ID 标识的数据里。那么你要搜索包含 “Java” 关键词的帖子，直接扫描这个倒排索引，在倒排索引里找到 “Java” 这个关键词对应的那些数据的 ID 就好了。然后你可以从其他地方根据这几个 ID 找到对应的数据就可以了。这种利用倒排索引查找数据的方式，也被称之为全文检索

2. **ES 的分布式架构原理能说一下么（ES 是如何实现分布式的） | 和 Kafka 类似**

   ![01_elasticsearch分布式架构原理](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/01_elasticsearch分布式架构原理.pn)

   ![Simple Cluster Topology](https://api.contentstack.io/v2/assets/575e4c999e7a83165490c360/download?uid=blt47da469cfb3097c3)

   > 要作为协调者必须要知道集群中其他节点的状态，节点的状态都会被复制到集群所有节点中。它有分片路由表（哪些节点托管哪些索引和分片）、每个节点的元数据（例如它在哪里运行以及节点具有哪些属性），和索引映射（可以包含重要的路由配置）和模板。复制到所有节点的集群状态是“映射”需要合理调整大小的重要原因
   >
   > 虽然说 Elasticsearch 是分布式的，但是对于我们开发者来说并未过多的参与其中，我们只需启动对应数量的 ES 实例(即节点)，并给它们分配相同的 `cluster.name` 让它们归属于同一个集群，创建索引的时候只需指定索引 *分片数* 和 *副本数* 即可，其他的都交给了 ES 内部自己去实现。

   // TODO 插一张es架构图

   ES 设计的理念就是分布式搜索引擎（不过底层还是基于 Lucene 的），核心思想就是在多台机器上启动多个 ES 进程来组成集群，从而达到高可用的目的

   一个索引可以分为多个 shards（分片），内部都有一个 primary shard 负责写入数据，会将数据同步到其他机器的 replica shard。所以如果某台机器宕机了也不影响数据读取（和 Kafka 很像，分布式原理都是互通的）

   由于 ES 集群是多节点的，它会自动选举一个节点作为 master node，这个 master node 其实就是干一些管理工作的，比如维护索引元数据、负责切换主从身份之类的。要是 master node 宕机了，那么会重新选举一个 master node。新的 master node 会识别出来原来的节点宕机了，会将宕机了的 primary shard 对应其他机器上的 replica shard （副本数据）提升为 primary shard。还有就是 ES 只能往 primary shard 里面去写，但可以从主从分片两个里面读数据

   > index：好比 MySQL 的一张表。如果一个 index 里面有多个 type 的话可以把 index 看做是一个类别的表
   >
   > 文档：相当于一条条记录
   >
   > type：没法跟 MySQL 里去对比，一个 index 里可以有多个 type，每个 type 的字段都是差不多的，但是有一些略微的差别。
   >
   > mapping 就代表了这个 type 的表结构定义，定义了这个 type 中每个字段的名称、类型和配置等等
   >
   > 实际上你往 index 里的一个 type 里面写的一条数据，叫做一条 document，一条 document 就代表了 mysql 中某个表里的一行给，每个 document 有多个 field，每个 field 就代表了这个 document 中的一个字段的值
   >
   > 好比说，有一个 index，是订单 index，里面专门是放订单数据的。就好比说你在 mysql 中建表，有些订单是实物商品的订单，就好比说一件衣服，一双鞋子；有些订单是虚拟商品的订单，就好比说游戏点卡，话费充值。就两种订单大部分字段是一样的，但是少部分字段可能有略微的一些差别。
   >
   > 所以就会在订单 index 里，建两个 type，一个是实物商品订单 type，一个是虚拟商品订单 type，这两个 type 大部分字段是一样的，少部分字段是不一样的。
   >
   > 不同的 type 字段要大部分保持一致，放在不同的 index 中，如果搞的字段都不一样会导致存储空间浪费、性能下降

3. **ES 写入数据的工作原理（ Elasticsearch principle）是什么啊（重点了解写入过程）**

   > 问这个，其实面试官就是要看看你了解不了解 es 的一些基本原理，因为用 es 无非就是写入数据，搜索数据。你要是不明白你发起一个写入和搜索请求的时候，es 在干什么，那你真的就是...
   >
   > 对 es 基本就是个黑盒，你还能干啥？你唯一能干的就是用 es 的 api 读写数据了... 要是出点什么问题，你啥都不知道，那还能指望你什么呢？是不是...

   ![ES 持久化模型](https://api.contentstack.io/v2/uploads/582483eeca8ca040424dd13f/download?uid=blt68fe526f76e11145)

   ![01_es读写底层原理剖析](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/01_es读写底层原理剖析.png)

   - **写数据过程**：写入一条数据时，客户端会发送请求到任意一个 node，被选中的成为了 coordinate node 协调节点，协调节点会对数据进行一个 hash，然后路由到对应的 primary shard 上并且同步到它的 replica shard 上，如果它俩都写完了，协调节点会返回写成功的响应给客户端

   - **读数据过程**：查询就是 GET 操作，比如 GET 某条数据写入了 document，这个 doc 会自动给你分配一个全局唯一的 doc id，同时也是根据 doc id 进行 hash 路由到对应的 primary shard 上面去。也可以手动指定 doc id（比如用订单 id，用户 id）。然后你可以通过 doc id 来查询，它会根据 doc id 进行 hash，找到当时把 doc id 分配到了哪个 shard 上去了（包括这个 primary shard 和 replica shard）。此时会使用 round-robin 随机轮询算法，在 primary shard 以及其所有 replica 中随机选择一个，让读请求负载均衡，读到数据的 node 返回 doc 给 coordinate node，再由它返回 doc 给客户端

   - **搜索数据过程**

     1. 客户端发送请求到一个 coordinate node
     2. 协调节点将搜索请求转发到所有的 shard 对应的 primary shard 或 replica shard
     3. query phase（短语搜索）：每个 shard 将自己的搜索结果（其实就是一些 doc id），返回给协调节点，由协调节点进行数据的合并、排序、分页等操作，产出最终结果
     4. fetch phase（短语获取）：接着由协调节点根据 doc id 去各个节点上拉取实际的 document 数据，最终返回给客户端

   - **写数据底层原理**

     1. 当数据写到 shard 时，会先写到内存 buffer 中，同时会将数据写入 translog 日志文件，此时在里面的数据客户端是搜索不到的

     2. 如果 buffer 快满了，或到达一定时间，就会将 buffer 里的数据 refresh 到一个新的 segment file（磁盘文件）中，每隔 1 秒钟进行一次。但刷进去时不是直接进磁盘文件的，而是先到达 os cache（操作系统缓存）。到达 os cache 时就可以被搜索到了

     3. 这就可以解释 es 为什么是准实时（NRT，near real-time）的？因为它默认每隔 1 秒 refresh 一次，写入的数据 1 秒之后才能被看到。也可以通过 es 的 restful api 或 Java api，手动执行 refresh 操作刷入数据到 os cache，让数据立马能被搜索到，此时 buffer 就会被清空

     4. 之后会重复之前的操作，新的数据不断进入 buffer 和 translog，buffer 倒是每次 refresh 完会清空，但 translog 会保留。它会变得越来越大，大到一定阈值时，就会触发 commit 操作

     5. commit 操作呢第一步还是会将 buffer 中现有数据 refresh 到 os cache 中去并清空 buffer，再将一个 commit point 写入磁盘文件，里面标识着这个 commit point 对应的所有 segment file

     6. 然后强行将 os cache 中目前所有的数据都 fsync 到磁盘文件中去

     7. 最后将现有的 translog 清空并重新启用一个 translog，此时 commit 操作完成（每隔 30 分钟或日志文件过大时触发）

        > translog 日志文件的作用是：一旦此时机器宕机，再次重启的时候，es 会自动读取 translog 日志文件中的数据，恢复到内存 buffer 和 os cache 中去。
        >
        > flush 对应的是 commit 全过程
        >
        > 不断写数据 - 每秒从 buffer 刷到 os cache - 每 30 分钟放一梯 os cache 的内容到 segment file

     8. translog 其实也是先写入 os cache 的，默认每隔 5 秒刷一次到磁盘中去，所以会有 5 秒数据丢失的可能

     9. 如果是删除操作，它是把删除数据写到.del 文件来标识数据被删除了

     10. 当 segment file 多到一定程度的时候，es 就会自动触发 merge 操作，将多个 segment file 给 merge 成一个 segment file，而在 merge 时会将之前表示为.del 的文件进行物理删除

         > es里的写流程，有4个底层的核心概念，refresh、flush、translog、merge

4. **ES 查询数据的工作原理是什么啊**

5. **ES 生产集群的部署架构是什么？每个索引的数据量大概有多少？每个索引大概有多少个分片？**

   > 这个问题，包括后面的 Redis 什么的，谈到 ES、Redis、MySQL 分库分表等等技术，面试必问！就是你生产环境咋部署的？说白了，这个问题没啥技术含量，就是看你有没有在真正的生产环境里干过这事儿！
   >
   > 其实这个问题没啥，如果你确实干过 ES，那你肯定了解你们生产 ES 集群的实际情况，部署了几台机器？有多少个索引？每个索引有多大数据量？每个索引给了多少个分片？

   - ES 生产集群我们部署了 5 台机器，每台机器是 6 核 64G 的，集群总内存是 320G
   - 我们 ES 集群的日增量数据大概是 2000 万条，每天日增量数据大概是 500MB，每月增量数据大概是 6 亿，15G。目前系统已经运行了几个月，现在 ES 集群里数据总量大概是 100G 左右。
   - 目前线上有 5 个索引（这个结合你们自己业务来，看看自己有哪些数据可以放 ES 的），每个索引的数据量大概是 20G，所以这个数据量之内，我们每个索引分配的是 8 个 shard，比默认的 5 个 shard 多了 3 个 shard

### 容器化 | Docker | K8s

1. 讲讲你了解的 Docker

   [K8S原理架构与实战（基础篇） - 知乎](https://zhuanlan.zhihu.com/p/382229383)

### 秒杀 | Java秒杀系统方案优化 高性能高并发实战（慕课）

每个核跑一个线程还是多个：

你们遇到消息积压的问题没有呢：阿里云 - 磁盘告警，测试环境出现数据太大

ES 怎么搜索的（小程序搜索框）：商品标题、分类、厂家等等，where like 导致索引失效

解耦、异步之后需要再继续展开讲吗

es问下我；抢购业务怎么问；redis使用池归还没有；秒杀数据库设计；分布式ID用的什么；秒杀单独领出来一个服务对吧；

**微信支付一般问的问题**

事务的传播（一个@transaction方法里面嵌套@transaction）；redis服务器搭的多少（哨兵）

@cacheable

MySQL：MySQL 的主从模式，master 主才能进行读写操作，而 slave 节点只能进行读操作



[hfbin/Seckill: ☁️基于 SpringBoot+Mybatis+Redis+RabbitMQ 秒杀系统 ☁️](https://github.com/hfbin/Seckill)

service 里面调用别的服务就一定是调用别人的 xxxService 而不是调用 dao。自己的 service 倒是可以调用自己的 xxxDao，这样更清晰而且别人可能在 service 层做了缓存操作，切记！

50000 的并发，QPS 是 1267

更新用户对象时要新建一个对象来更新，只更新需要更新的数据。如果整个对象都更新的话会导致产生的 SQL 和 binlog 越多，为了求效率需哪个字段才更新哪个字段不要全量更新

一个 service 调用另一个 service 一定要进行判空，尤其是调别人接口。比如 orderService 里面根据 ID 查询订单、goodsService 根据商品 ID 查询商品、积分 service 等

> 要先更新数据库，再更新缓存。不然如果先删除缓存，然后做了一次读操作把旧数据写入缓存，又做了一次更新操作，此时缓存里面就是旧的数据了，数据不一致了
>
> [高性能网站设计之缓存更新的套路_代码技巧的博客-CSDN博客](https://blog.csdn.net/tTU1EvLDeLFq5btqiK/article/details/78693323)

1. 业务

   先判断库存是否充足，库存不足直接返回失败。然后主要是减库存->下订单->写入秒杀订单（设计个秒杀订单表是为了后面还有其他活动的话不会搞乱原本的订单表）。要保证这三步操作的事务（原子性）

2. 用 Jmeter 进行压测

   压测 10000 个线程，用 top 命令查看服务器资源情况，发现瓶颈在于数据库，load average 到达了 3 以上说明有大量的进程都在等待。最终吞吐量为接近 200 每秒

   四核八线程服务器跑 Redis 差不多 8w 的 QPS

   系统的瓶颈在数据库，因为数据库的 IO 比内存读写慢得多。而 Java 程序也容易横向扩展，无非就是加机器。但数据库不行，如果一开始不是分库分表的结构那么改动就特别大。所以高并发就是要让数据库抗住压力，然后程序扩展起来就非常容易了

3. 优化

   - 页面缓存：把 HTML 缓存到 Redis 里面，过期时间设为 60s 的话对用户来说感知不是特别大。就只缓存不怎么变动的前两页数据

   - 对象级缓存：把用户等重要且使用频率高的对象缓存起来，对象更新的时候再去更新缓存

     > 经过上面两种情况优化后负载 load average 从 15 降到了 5 左右，因为 MySQL 的负载暂时没有了，60s 内都是走缓存，减少了对数据库的访问。此时 QPS 从之前的 1267 涨到了 2884，翻了一倍多

   - 浏览器缓存：商品详情页面通过 Spring 的 yml 配置让请求缓存在浏览器中并返回 304

   - 静态资源优化：从用户发送请求开始就进行优化，

     - 用 Webpack 将散落的模块打包到一起，这样就解决了浏览器频繁请求模块文件的问题，还减少了流量
     - 走 CDN 优化
     - Nginx 做一层缓存

   - 接口优化：思路就是减少数据库访问

     - 系统初始化，把商品库存数量加载到 Redis

     - 收到请求，Redis 预减库存，库存不足，直接返回，否则进入 MQ

       > 这里还有一个小优化，就是利用 `内存标记` 来减少 Redis 访问（因为 Redis 访问也需要一定的开销），初始化的时候用 ConcurrentHashMap 来标记商品为 false 代表还没被秒杀完，在用 Redis 做预减库存且库存小于 0 时，把 Map 标记的 false 改为 true，那么在下次内存标记判断时为 true 就直接返回秒杀失败！就不会再走后面的 ”预减库存“、”创建订单“ 等等一系列操作了（假如秒杀 10 个商品，那么就算是 100000 个请求对数据库都是完全没有压力的）

     - 请求入队，立即返回排队中

     - 请求出队，生成订单，减少库存

     - 客户端轮询，是否秒杀成功

   - 安全优化

     - 秒杀接口地址隐藏：在秒杀开始之前是拿不到我们的秒杀地址的，怕被拿到地址之后来刷

     - 数学公式验证：避免机器人以及进行一个削峰的作用。具体实现就是用 BufferImage 来画图，生成一个随机的验证码画在图片上，然后把验证码写到 Redis 中。

     - 接口限流（接口防刷）：一个用户最多访问我们系统 10 次。如果自己手动实现又是计时器又要计算访问次数，一分钟后还要重新计算，想想还是有点复杂。但如果用 Redis 就很简单了，直接设置 1 分钟有效期，访问了一次（带用户 ID 的请求路径）就 +1 超过了限制的次数就抛错。技术就是这样这层窗户纸捅破了就很简单

       > 和业务无关的代码，可以用拦截器来进行实现，做到不同的接口可配置化。因为我这个接口可能想 5s 最多访问 5 次，那个接口想要 10s 最多访问 10 次。
       >
       > 方案：
       >
       > i. 可以自定义注解+拦截器，定义 @AccessLimit(seconds=5, maxCount=5, needLogin=true) 来实现
       >
       > ii. 使用 Gateway 网关的限流（令牌桶算法）
       >
       > iii. （只能用于单机）Guava 的 RateLimiter
       >
       > iv. （用于分布式系统）Redis 限流方案

4. 可能出现的问题及解决方案

   - 订单超卖问题（库存变为负数）：`update ... stock - 1` 改为 `update ... stock - 1 where stock > 0` ，改为只有大于 0 的时候才更新库存，更新操作会锁表，所以不用担心两个线程会操作出负数的情况

   - 同一个用户刷单问题（重复购买）：我们可以把订单附属的另一张秒杀表里面的 userId、orderId 组成唯一索引，这样就不能插入同一条数据，会报错并回滚

     ```java
     @Transactional
     public OrderInfo createOrder(MiaoshaUser user, GoodsVo goods) {
         OrderInfo orderInfo = new OrderInfo();
         orderInfo.setCreateDate(new Date());
         orderInfo.setDeliveryAddrId(0L);
         orderInfo.setGoodsCount(1);
         orderInfo.setGoodsId(goods.getId());
         orderInfo.setGoodsName(goods.getGoodsName());
         orderInfo.setGoodsPrice(goods.getMiaoshaPrice());
         orderInfo.setOrderChannel(1);
         orderInfo.setStatus(0);
         orderInfo.setUserId(user.getId());
         long orderId = orderDao.insert(orderInfo);
         MiaoshaOrder miaoshaOrder = new MiaoshaOrder();
         miaoshaOrder.setGoodsId(goods.getId());
         miaoshaOrder.setOrderId(orderId);
         miaoshaOrder.setUserId(user.getId());
         orderDao.insertMiaoshaOrder(miaoshaOrder);
         redisService.set(OrderKey.getMiaoshaOrderByUidGid, ""+user.getId()+"_"+goods.getId(), miaoshaOrder);
         return orderInfo;
     }
     ```

   - 引入 Redis、MQ 等等势必会增加系统的复杂性，可能突然不小心把缓存给清除了。所以应该有个管理后台（总后台）来能提供实时查看 Redis 数据以及一键清除缓存等功能

5. 通用缓存 Key 封装

   模板模式：接口（定义一些契约） -> 抽象类（实现一些共通的实现） -> 实现类（一些子类特有的实现）

### 项目 | 面试

生产环境是 2 台 4 核 16g，4620 元 / 台 / 年；Redis 和 MQ 用的是容器（镜像）服务；MongoDB 是 1 核 2g，10g 磁盘，一年 2466。测试环境就是自己搞的服务器，上面搭的虚拟机，跑的 Docker 服务

[时隔一年半，我，一个卑微的前端菜鸡，又来写面经了 - 掘金](https://juejin.cn/post/7036581158670303240)

[面试过四十几家互联网大厂，总结出的“项目介绍”技巧，看完别说不知道怎么介绍项目了_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1ud4y1D7rt/?spm_id_from=pageDriver&vd_source=4844de7cb051be29fbaf4555af0bbd8b)

怎么讲好自己的模块：是什么 what，先将文档上的需求分析 why，怎么实现的 how

自我介绍：面试官您好！我叫刘强。做开发两年多了。熟悉市场上比较主流的框架像 SpringMVC、WebFlux、Spring、Mybatis，以及 SpringBoot 这些，数据库话比较了解像 MySQL 这样的关系型数据库以及 Redis、MongoDB 这样的非关系型数据库。平时呢喜欢上一些像掘金、Stack Overflow、GitHub 等技术性网站。然后我来介绍一下最近做的项目吧

[什么是数字化和数字化转型？ - zhaimicc的回答 - 知乎](https://www.zhihu.com/question/343916263/answer/2588574213)

> 平级面试：讲好技术栈和技术细节，体现表达能力。要在描述你做的事情之前交代背景，体现出比较清晰的条理与逻辑啊
>
> 组长面试：
>
> 1.能否快速理解需求
> 2.思考问题的严谨程度（比如研发，在设计一个功能之前，有没有考虑到足够多的边界情况）
> 3.体现对需求的思考过程（描述项目时，有没有体现出接到一个需求时的思考过程）

您好，我叫刘强，从 18 年毕业到现在差不多工作 4 年了，最早是在大学项目里面接触了后端开发这个行业，17 年在北京的一家公司实习，感觉技术氛围很好毕业后就留下来并转正了。19 年我想要回成都发展，在这边入职了食安云，主要负责线上 bug 定位修复以及解决大表索引引发的一些问题。去年是入职了现在的广月科技，我在里面主要负责小程序电商的订单、支付业务，以及海康智能硬件的接入。平时的话呢喜欢上一些像掘金、Stack Overflow、GitHub 等技术性网站来填充知识。然后我来介绍一下最近做的项目吧

我们最近做的是一个物业自治 Saas 服务平台，，用户在商城交易，我主要负责的小程序电商、服务管控、智能硬件接入等功能。系统采用的是前后端分离的架构，前端采用的 React，后端用的 SpringBoot + SpringCloudAlibaba 来构建的微服务，数据库即用到了 MySQL，也用到了 MongoDB

万家美慧架构：

![万家美慧架构图](http://rjnbyrj6g.hn-bkt.clouddn.com/interview/%E4%B8%87%E5%AE%B6%E7%BE%8E%E6%85%A7%E6%9E%B6%E6%9E%84%E5%9B%BE.jpg)

> 面试不会问你哪个 API 怎么用，只会问你原理。把原理清晰的讲出来即可。因为不可能有面试官会让你把 MQ 的集群、高可用步骤说一遍
>
> 2、对大部分技术点，不会动手去落地实践，比如说 RabbitMQ 的高可用性，我们就是剖析 MQ 高可用部署的一个原理，现场大量手工绘图讲清楚如何回答这个东西，确保你被问到的时候，能说清楚整个高可用部署架构以及现场动手画图。但是我不会带着你去安装虚拟机，然后安装 RabbitMQ，然后再部署高可用架构，然后再进行高可用容灾演练。因为我们的定位是短期内快随突击面试题，转为面试准备，不是动手项目实战课程。
>
> 3、对很多技术，我们的定位就是面试被问到的时候，你可以回答出其基本原理，我们不可能对有些技术点刨根问底，深挖到源码级别，那是需要大量时间去讲解的，不是所谓的两三周短平快可以突击的完的。所以确实有可能对于一些要求很高的公司，是可能存在一个问题，人家问你一个问题，你答出来了，接看深挖，你答不出来了，那么此时是没办法的，你要想对各种技术都有深入底层的功底，那不是两三周的事儿，而是两三年的苦功夫才能得到的。

1. **项目介绍**

   [优秀简历法则：从star法则到start法则（简历系列2/3） - 知乎](https://zhuanlan.zhihu.com/p/67775969)

   [🔥正确介绍自己的项目经验 再也不为面试发愁了 - 掘金](https://juejin.cn/post/7017732278509453348)

   > - 服务管控模块：完成巡更、巡检、绿化、保洁业务的开发
   > - 群众诉求模块：小区业主通过小程序把诉求提交到街道办，物业监管平台的政府职能部门进行处理或者转办
   > - 智能硬件模块：海康系统的搭建及智能物联硬件设备的接入
   > - 数据分析模块：对管辖区域各个业务板块的数据进行统计

   我在广月科技做的是一个物业自治 Saas 服务平台，客户群体是 C 端用户，用户在商城交易，我主要负责的小程序电商、服务管控、智能硬件接入等功能。系统采用的是前后端分离的架构，前端采用的 React，后端用的 SpringBoot + SpringCloudAlibaba 来构建的微服务，数据库即用到了 MySQL，也用到了 MongoDB

   > 项目背景+分析过程+困难原因（时间紧迫？第三方难以配合？历史遗留问题很多？）+调研过程+方案制定+选择原因+协调资源+项目复盘（是否有更好的实现方法）
   >
   > 面试官在意的是你思考问题的方式和表达问题的能力
   >
   > 有做好本职工作，有对现状的改造，有推动学习，有自我学习，有经验总结
   >
   > 项目编写：OKR工作法，Objectives and Key Results即目标与关键成果法。
   >
   > 以及 [SMART](https://wiki.mbalib.com/wiki/SMART原则) 原则（按 star 法则来阐述，不仅仅是项目，你就已经打败 80% 的候选人了，在此基础上再蕴含你的思考，打败 90%。）
   >
   > 1.目标必须是具体的（Specific）
   > 2.目标必须是可以衡量的（Measurable）
   > 3.目标必须是可以达到的（Attainable）
   > 4.目标必须和其他目标具有相关性（Relevant）
   > 5.目标必须具有明确的截止期限（Time-based）
   >
   > 1.关键职责具体化，体现主导的工作
   > 2.工作成果可衡量（有数据支撑更好）
   > 3.项目的收获（学会了什么，发现了有哪些不足）
   > 4.思考过程（将需求转化为代码设计再转化为代码实现的过程）

   做这个项目背景就是传统物业公司来管理小区会有诸多的缺点，比如：

   - 服务过程缺乏监督

     > 很多传统物业在分配巡更、巡检任务的时候还是用的是纸质方式，这样导致无法得知服务人员是否偷懒，是否执行了相应的任务等，而我们就可以帮他们实现电子化，所有的任务都有线上记录，是否进行了点位打卡等操作
   - 物业公司不作为

     > 像我哥哥他们小区就是这样，就在龙泉那边。有一次就是电动车不管外来的还是小区业主的，都停在要出去的必经之路，导致堵得水泄不通勉强只能有一人通过，过了一段时间后物业的人也没处理。然后越来越多的业主开始找物业闹，最后物业才迫于压力才进行了整改
   - 物业资金使用不公开、不透明

     > 我哥他们小区，物业公司的电梯广告、地上停车等等的收支明细完全不透明，只公布说公司花销大连年都在亏钱，最离谱的是还把旁边小区亏损账单划到本小区来。引的业主各种不满开始找社区闹，想要通过业主大会换掉物业公司或着进行物业自治

   我们公司也是基于这些行业的种种痛点，于是着手开始研发万家美慧。万家美慧它是一个围绕着 “红色 - 阳光 - 智慧” 为核心的家园管理互联网平台，红色指党建引领（监管），阳光指公开透明，智慧指用一些软硬件（硬件采用人脸门禁、高空抛物、消防通道占用。软件采用我们平台）来让生活更智慧

   它有别于以往的传统物业平台，它是基于互联网、物联网以及大数据等一系列技术打造的物业自治 [SaaS](https://www.zhihu.com/question/35087138) 服务平台，从而实现家园更美好、 生活更智慧、 社会更和谐这一愿景。（万家美慧所有的数据都基于小区维度来进行隔离的）

   平台总的来说只有小程序和 WEB 端，前期为了业务的野蛮生长，所以选择了做更易于推广的小程序，所以没有做 APP。而小程序分为了家园端（万家美慧小程序）和服务人员小程序

   - 万家美慧小程序（家园端）：就是给那些物业管理人员和行业监管人员使用的，主要分为了几大板块

     - 家园

       业主可以进行物业缴费、查看小区收支等等

      - 商城

        小区业主和业委会可以根据身份在商城各自购买小区需要的服务商品（比如保安服务、保洁服务）、设备设施（包括像摄像头，人脸门禁这类硬件设备，因为我们老板以前是海康四川分公司的负责人，所以一直和海康都有紧密的合作）等

      - 行业管理（相当于物业监管）

        政府职能部门、街道办事处及社区居委会人员可以在行业管理模块查看辖区内小区的基础信息、防疫管理、重大事件上报等
        
      - 个人中心

   - 服务人员小程序：服务人员端是给那些巡更、巡检、绿化、保洁的人用的

      - 首页

        查看自己的待办任务，在物业管理平台的服务管控那边进行指派人员后，服务人员会在小程序这边接收到任务并去打卡执行
        
      - 物业管理

        提供给物业管理人员使用的，可以进行防疫上报、重大事件上报、查看小区购买商品的订单等等

   - WEB 端管理后台分了三个入口分别是物业管理平台、物业监管平台、服务商管理平台。这三个入口外加一个万家美慧总后台

     - 物业管理平台：主要实现小区的物业管理以及收支管理。主要模块有住户管理、房屋管理、物业收费、业主大会、服务管控、报事报修等等

       > 服务管控和服务人员那边是通的：先初始化好路线、点位什么的，然后新增一条计划，根据计划选择的每天、每周等日期按照频次来自动生成 n 个任务，再对任务进行指派到具体干活的人。指派过后呢对应负责人的小程序上就能接收到任务，完成后任务状态更新为已完成并生成任务报告

     - 物业监管平台：主要实现政府职能部分、街道办和社区居委会人员对小区日常事务的监督管理。主要模块有我的主页（相当于数据看板，根据行政区划查看辖区基本情况）、疫情防控模块（管辖范围内的疫情防控数据）、群众诉求模块（查看提交过来的诉求数据）等

     - 服务商管理平台：主要实现的是对入驻到我们平台的服务商日常的管理。主要包括了店铺管理、商品管理以及订单管理等等

     - 万家美慧总后台：主要管理着我们整个系统后台所需要的东西。包括了套餐管理（不同角色配置不同权限套餐）、电商管理（一些广告、商品分类）、微信支付服务商管理（用于微信支付所需要配置的分账商户，包括分账比列等等）

     > 印象最深的这个汇学网它是一个在线学习平台，项目采用前后端分离的技术架构，前端采用的是 Vue.js，后端采用 SpringBoot 和 SpringCloudAlibaba 来构建的微服务，用户可以通过 PC、手机等客户端来访问系统进行在线学习，那我主要负责的是页面管理模块、课程搜索模块、微信支付模块
     >
     > 业务流程：
     >
     > 用户可以通过 pc、手机等客户端访问系统进行在线学习
     >
     > 系统应用 CDN 技术，对一些图片、CSS、视频等资源从 CDN 调度访问
     >
     > 所有的请求全部经过负载均衡器
     >
     > 对于 PC、H5 等客户端请求，首先请求 UI 层，渲染用户界面
     >
     > 客户端 UI 请求服务层获取进行具体的业务操作
     >
     > 服务层将数据持久化到数据库
     
     万家美慧推广了 100 多个小区，主要是泸州、内江那边政府合作落地的。用户量接近 30w，日活 1w 多。活动高峰期 QPS 达到 1000，平时 QPS 是 100 多。秒杀活动时 QPS 5000
     
     > MySQL：QPS 如何计算的，100w 用户高峰期 QPS 达到每秒 5000 个请求。但 MySQL 一般只能抗每秒 2000 个请求，否则就挂了
     >
     > MongoDB：可以看到在 400 个并发情况下，插入 100 万记录，QPS 在 28000 左右。
     
     并发估算
     
     > 一般应用来说，并发估算公式如下：
     >
     > qps = 5 * 日pv / 86400
     >
     > 5是通用峰值倍数，如果你有高峰值特性自行调整，比如秒杀功能。
     >
     > mysql通常在实体机的读写综合qps在几千左右，具体你可以自己压测一下，跟机器配置有关。
     >
     > 所以首先你需要通过收集日志得到日pv，然后通过估算得到当前qps，再根据未来一段时间的用户量和场景看看够不够。
     >
     > 比如你每天数据库访问次数是100万，可估算峰值qps为60左右，比如数据库qps可达3000，那么可估算你的服务器还能撑同等场景50倍增长。
     >
     > 以上方法都是通用方法，杠精退散。
     >
     > 
     >
     > 脱离具体方法来说，题主这种小型站点，我的建议是不要听某些人说的用什么牛逼方案，用最成熟的通用方案就对了，有问题方便解决，以后也便于别人接盘。
     >
     > 切忌过度优化，互联网老兵给你的成熟建议。
     >
     > 
     >
     > 作者：大雄
     > 链接：https://www.zhihu.com/question/315282781/answer/666558937
     > 来源：知乎
     > 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

2. **项目架构介绍**

   安易物联做大表优化，安易物联是之前公司开发的一个单体智慧电动车停车系统。

3. **你们系统有多大 QPS？系统到底怎么抗住高并发的？**

   > 作者：旧港94
   > 链接：https://zhuanlan.zhihu.com/p/61123722
   > 来源：知乎
   > 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
   >
   > 
   >
   > （1）尴尬的面试现场1
   > 于是面试官和候选人可能会展开如下一系列的问题：
   > **面试官**
   > 你说你们系统用了Redis，那你说说你们项目目前有多少用户？
   > **候选人**  
   > 这个。。。。好像大概就几百或者几千？（或者有的人是小互联网公司的，可能会说，大概有个百万级的用户量）
   >  **面试官**
   > 好吧，那你说一下你们系统每天日活是多少？
   > （**解释一下日活**：如果一个公司的产品有100万注册用户，但肯定不是每天每个人都会来用你的系统的啊！就好像注册了一个APP，可能半年才会用一次！这个日活，就是每天到底多少人来用）
   > **候选人**  
   > 啊？每天多少人来用？我这个还真的没统计过啊。。大概可能有几千或者几万个人？
   > **面试官**
   > 行吧，那你们每天几千或者几万个人来用，那每天的请求量有多大？
   > **候选人**  
   > 这个我还真的没统计过啊，不好意思啊！
   >
   > **面试官**
   > 纳尼？那你知道你们的系统高峰期QPS有多大码？（QPS，Query Per Second，也就就是每秒有多少请求）
   > **候选人**  
   > （心里感觉快哭了，因为觉得这个面试要黄，为啥什么技术都没问题，直接来这些啊）我真的不知道啊。。。
   > **面试官**
   > 那你什么都不知道，你说说你们系统为什么要用Redis缓存？还要搭建一个3台机器组成的Redis Cluster，这原因何在？
   > 如果不用Redis，直接就用MySQL来抗能不能抗的住？
   > **候选人**  
   > 我们当时用Redis。。。。咦？到底为啥要用啊？我好像也忘了，就感觉可以把这个技术用一下，毕竟我们花了时间来调研，所以觉得可以用就用一下。
   > **面试官**
   > 你这是典型的为了用而用啊！那你简历上说，你熟悉高并发相关技术，包括Redis、RocketMQ，等等，那你到底做过高并发系统没有啊？
   > **候选人**  
   > 好吧，我错了，我确实不知道什么是高并发，我就是学了这些技术然后就写在简历上了。
   > **面试官**
   > **（心里活动）**咋回事，搞半天是没相关经验的，都是自己学了一下啊，好吧，那来压一压薪资，看来不能当资深码农来对待了 ，就当做普通的来面一下好了
   >
   > 于是两个人进入了一系列的技术问答，但是面试官心里有数，这个候选人最多就是给一个普通工程师的职位，因为其实他并没有过技术在项目如何落地的一些经验。
   >
   > （2）尴尬的面试现场2
   > 这个候选人痛定思痛，回来改了一下简历，说自己负责的系统，日活用户几十万人，高峰期QPS可以达到5000/s+。
   >
   > 然后心想，这回不会像上次一样，把这个事儿给聊黄了吧。到了面试现场坐下来开始了跟面试官下面的对话：
   >
   > **面试官**
   > 你们用户量多大？日活多少？每天请求量多大？高峰期QPS多高？
   > **候选人**  
   > （胸有成竹，嘴角挂着迷之微笑。。。）注册用户100万，每天日活几十万用户，每天请求量有几千万，高峰期QPS最大有5000/s+。
   > **面试官**
   > 奥，那你们线上的部署架构是怎么做的，给我画图看看？
   > 然后你们一共有哪些服务，每个服务部署了多少台机器？每台机器是什么配置？CPU和内存有多大？你的机器部署是怎么抗住每秒5000请求的？
   > **候选人**  
   > （心理活动）公司实际没啥请求量，每个服务就部署一台机器，连配置自己都没关注过，天知道每秒5000请求需要多少机器可以抗住啊。。。
   > **面试官**
   > 咦？你怎么支支吾吾的，自己项目线上部署情况都不清楚？
   > 那如果你们从网关入口层进来的请求是5000/s的话，那你能画图说一下你负责的每个服务的接口的QPS是多少？
   > 然后你们的各个中间件，MySQL、Redis、ES、RocketMQ，他们各自的QPS都是多少？以及他们各自都部署了多少机器，每个机器什么配置？
   > **候选人**  
   > 这个。。。。我从来没考虑过，我还真的不知道啊！
   > **面试官**
   > 那你简历说你系统可以抗5000/s的并发请求，你根本不知道系统架构是怎么抗住的啊！
   > **候选人**  
   > …………

4. **你在开发中遇到什么问题？是怎么解决的？（聊一个你觉得你做的项目中的难点）**

   例子：在处理订单时要用到定时任务，当时采用的是 Spring Task 来完成，由于一个订单服务会部署多个，多个订单服务同时去处理任务会造成任务被重复处理的情况，如何解决任务的重复处理

   解决：采用乐观锁解决，在任务表中设置一个 version 字段记录版本号，取出任务记录同时拿到任务的版本号，执行前对任务进行锁定，具体的做法是执行 update 根据当前版本号将版本号加 1，update 成功表示锁定任务成功，即可开始执行任务

   OutOfMemoryError（见 JVM 部分）、StackOverflowError（方法调用死循环）

5. **你们秒杀系统怎么做的**

   如何设计秒杀系统？ - 九章算法的回答 - 知乎 https://www.zhihu.com/question/54895548/answer/1352510403

   > 见秒杀目录

7. **微信支付**

8. **项目开发流程**

   [https://search.bilibili.com/all?keyword=%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B&from_source=nav_search_new](https://search.bilibili.com/all?keyword=项目开发流程&from_source=nav_search_new)

   1. 需求调研 - 需求文档
   2. 开发文档 - 包含业务和页面原型 --  页面代码  --- 交给美工做图，前端开发根据图纸完成页面
   3. (前后端分离的方式) 项目经理会根据业务，编写一个接口文档
   4. 安排任务 - excel 文档标注你要完成哪些功能，每一个功能所选要时间
   5. 熟悉公司的开发流程
      - 提前熟悉需求  -- 简单的功能做什么，数据表有哪些，细节处理
      - (刚刚进公司) 公司代码规范和开发流程
        - 提前熟悉项目 -- 上级都会给你一个项目
        - 需要问一下有没有需求文档或者接口或者功能说明文档
        - 该项目运行需要哪些环境 -- (需要发布 tomcat？ 需要安装哪些东西)
        - 如果能够运行 -- 试图自己运行起来，如果运行不起来，如果启动报错
          ，自己先百度是什么问题，如果不解决，那么可以问一下旁边的同事
        - 开发流程 (最好能够向上级申请一下希望能够找一个前辈帮自己快速熟悉公司的代码规范和开发流程，找一个同事帮梳理一遍某一个增删改查功能开发过程) 如果能够运行或者有接口文档 -> 获得请求的 url -> 全局搜索查找匹配该 url 的 controller-> 找到该 url 对应的请求方法 (分析调用了本地哪些 Java 文件，debug) 如果没有接口文档也不能运行，自己尝试把所有的包去看看，有没有自己熟悉，查看有没有 web，controller，service，mapper，reporties，dao，utils，filter (分析调用了本地哪些 Java 文件，debug)
      - 根据公司架构提供集成架构上进行业务功能开发
        - 从 git 或者 svn 将基础代码拉取下来
        - 按照公司开发规范编写代码 api 接口 (controller)  ---> 根据业务分析接口所需要的参数和返回值
          controller 实现类 service 具体业务 --> dao 业务的细节： 做一些数据验证，判断是否登陆才能操作之类
      - 每天下班的时候记得把完成功能 (可运行的代码) 提交 git

9. **项目人员**

   架构师 1 个；项目经理：1个；项目组长：1个；前端：4个；开发：4个；测试：2个；运维：1个

10. **接口定义规范（接口是怎么定义的？采用什么数据格式？如何实现？）**

   项目架构设立接口层，接口层使用 swagger 注解描述接口的内容，接口定义规范如下:

   - 接口定义

     使用 SpringMVC 定义接口，在类上定义了根路径，在每个方法上边我们用 GetMapping、PutMapping、PostMapping、DeleteMapping 等注解定义 URL

   - 数据格式

     - Get 请求时，采用 key/value 格式请求，SpringMVC 可采用基本数据类型接收，查询对象条件等呢可以使用自定义类型对象来接收

     - Post 请求时，可以提交 Form 表单数据（application/x-www-form-urlencoded）或 Json 数据 (ContentType=application/json) 以及文件等多部件类型数据（multipart/form-data），对于 Json 数据 SpringMVC 使用 @RequestBody 注解解析请求的 json 数据

   - 响应：统一响应 json 格式

     - 响应结果信息统一包括：是否成功、操作代码、提示信息以及自定义数据（比如查询结果用的就是 QueryResponseResult 来接收）

     - 响应结果格式呢统一为 json 数据

       > 服务端都是暴露的 rest 接口，统一用 json 展示数据

     实现：json 格式数据 SpringMVC 采用 FastJson 解析为对象。非 json 格式数据 SpringMVC 提供参数绑定的方法，将 key/value 或 Form-Data 数据转换为对象或基本数据类型的变量

11. **API 定义约束**

    Api 定义是使用 SpringMVC 来完成，由于接口后期将作为微服务远程调用使用，在定义接口时使用
    @PathVariable 和 @RequestParam 统一指定参数名称，必须在注解的括号中指明参数，如：@PathVariable ("id") @RequestParam（"id"） 

    > 为什么把 api 接口单独定义在一个服务中？
    >
    > 1）方便统一的管理，不然微服务特别多就会分散在各个位置
    >
    > 2）微服务与微服务之间得远程调用技术就是基于接口调用的，如果接口分散在不同的位置，服务 A 调用服务 B，那么 A 就依赖于 B，但现在统一定义在 api 服务中，那么所有服务依赖于 api 就可以轻易拿到任意微服务接口了
    >
    > 3）当有一天我们想要换用别的 web 框架了，那么 api 服务是不用动的，只用改实现类就可以了，增强了系统的可扩展性

12. **前后端开发时具体流程是什么？**

       > 前后端分离开发模式在互联网公司最常见，特别是一些大型的互联网公司，但是一些传统的软件开发企业仍然是采用传统开发模式，此问题被问及是考察你有没有真正体会前端开发的好处
       >
       > 好处：它是统一对外提供服务嘛，方便了系统的扩展性和可维护性，比如说现在要开发一个安卓客户端，那么现在只需要单独做个客户端就可以了，不管什么样的客户端来请求业务，服务层都统一提供服务。
       >
       > 还有就是说如果双十一要来了，下单的用户比较多，就可以把订单的节点服务部署的多一点，用户管理的部署少一些，这就是一些好处，方便系统的扩展性和可维护性
       >
       > 用户层：面向哪些用户、CDN 作为缓存、负载均衡把用户的请求分担到不同的节点、微服务层统一为前端提供服务

       - 前端与后端开发人员讨论确定接口接口讨论通过，形成接口文档。本项目专门设立一个 api 工程，在此工程定义接口，Spring Boot 集成 Swagger，生成 Swagger 接口，前后端开发人员通过 html 查看接口文档的内容
       - 前端与后端开发人员按照接口文档进行开发。开发过程中各自进行单元测试。前端人员怎么进行单元测试？前端人员可以通过一些工具生成一些模拟数据，比如：EasyMock

       > 项目是基于前后端分离的架构进行开发，前后端分离架构总体上包括前端和服务端，通常是多人协作并行开发，开发步骤如下：
       > 1、需求分析
       > 梳理用户的需求，分析业务流程
       > 2、接口定义
       > 根据需求分析定义接口
       > 3、服务端和前端并行开发
       > 依据接口进行服务端接口开发。
       > 前端开发用户操作界面，并请求服务端接口完成业务处理。
       > 4、前后端集成测试
       > 最终前端调用服务端接口完成业务。

11. **如何设计一个高并发系统？**

    ![01_高并发系统的架构组成](https://cdn.jsdelivr.net/gh/liuilin/interview@latest/interview_note/img/01_高并发系统的架构组成.png)

12. **前后端如何联调**

    就是我们把后端代码写好后，在云效跑个流水线，完事之后用 Kuboard 集成的 K8s 功能重新构建服务来注册到 Nacos 上。同时把本地打个 api 包，也在云效跑流水线，这个是架构师写的一个 maven 插件，这样前端用 npm 拉新的包就可以直接调用了。

13. **项目使用 Spring 了吗?用了它的哪些东西?**

    项目是基于 Spring 进行构建的：

    i. 所有的微服务开发采用 Spring Boot 开发

    ii. 数据层使用 Spring Data JPA，Spring Data MongoDB， Spring Data Redis；业务层使用 Spring 来控制本地事务，还使用了 Spring Task 任务调度框架 Spring AMQP 组件等；控制层使用 WebFlux

    iii. 微服务管理使用 SpringCloudAlibaba 的 Nacos 做注册中心，微服务之间调用使用架构师自己写的轻量级远程调用

    vi. 使用 Gateway 网关完成微服务安全验证

14. **什么是雪崩，如何解决？**

    微服务雪崩效应表现在服务与服务之间调用，当其中一个服务无法提供服务可能导致其它服务也挂掉

### 面试问题集锦

1. **CAP 定理**

2. **项目里面遇到过什么难点问题，怎么解决的?**

   

3. **项目中代码怎么优化？**

   - 字符串拼接的优化，对于日志字符串的拼接我们都是采用 StringBuffer（因为用到了多线程）来进行优化
   - 对于循环，我们是需要判断是否已经达到循环的目的，然后直接 break
   - 遵循《阿里巴巴开发手册》的开发规范，用插件来约束
   - SQL 优化

4. **工作中遇到过哪些异常？**

   Java.util.ConcurrentModificationExceptior

   [OutOfMemoryErrorWithNativeThread](#OutOfMemoryErrorWithNativeThread)

5. **你们项目中用到了多线程了吗**

   - ==消息堆积问题，以及如何处理==
   - 后台违约订单处理：利用定时任务扫描商品已经下架，但是没有支付的订单进行处理

6. **后台系统如何防止重复提交（微信支付）**

   可以使用 token 令牌机制，在进去表单页面的时候发送请求到后台。后台生成一个不重复的 ID 返回。并把 ID 存在 Redis 中。表单提交时带上这个 ID 去做判断就好了

   > 消息的不丢失。开启消息的持久化，包括生产者开启事务。交换机持久化，队列持久化。自动应答改为手动确认消费者端的自动消息确认，改为操作成功后手动提交

7. **对于 git，svn 代码冲突怎么处理？**

   多个人都在操作同一个文件，然后直接都在提交

   - 谁先提交，谁就不用管
   - 这个时候，就让别人再次更新他的代码，然后我先更新，更新之后再他的基础上进行修改
   - 直接采用 IDEA 工具来当前你要修改的文件和目前更新的文件有哪些地方不一样

   拉下来看，如果修改是同一个文件，并不是同一处，那么可以通过工具合并，再提交。

   如果修改的是同一处，那么我这边当前修改的内容比较少，我就让同事提交他的代码，然后我更新之后再修改，最后再提交

8. **关于项目上线的问题？ 比如：并发多少，集群环境，搭建了多少台服务器**

   集群环境是否负责： 生产环境我作为开发并没有涉及，主要负责的事开发环境和测试环境。

9. **当面试官问你还有什么问题问我时，回答模式？**

   人事：

   - 五险一金如何购买，买在哪里
   - 公司还有其他福利

   技术：

   - 技术框架
   - 开发模式，前后端分离吗
   - 业务方向
   - 加班如何

10. **SpringBoot 注解：**

   装配一个 Bean 时，会用到 @Configuration、@Bean 这两个注解。这种装配方法虽然简单粗暴，但是多了都用 @Bean 方式注入会很痛苦，那就可以使用 SpringBoot 的方式装配 Bean，使用的就是@Component（标明哪个类被扫描，在类里面使用 @Value 指定值）和 @ComponentScan，当然 @SpringBootApplication 里面已经包含了 @ComponentScan。

   @Autowired：根据属性类型找到对应的 Bean 进行注入

   @ComponentScan 组件扫描，可自动发现和装配一些 Bean

   @RestController 相当于 @Controller 和 @ResponseBody

   @PathVariable 获取参数

   @SpringBootApplication：包含了 @ComponentScan、@Configuration 和 @EnableAutoConfiguration 注解

   @Lazy：解决的是 Spring 循环依赖问题，懒加载

   > 其中 @ComponentScan 让 Spring Boot 扫描到 Configuration 类并把它加入到程序上下文

11. **开闭原则：对拓展开放、对修改关闭**

    里氏替换原则：任何子类都能替换父类

12. **如何用 MySQL 实现 cas 模式的数据安全**

    可以采用的乐观锁机制，在数据库里面加一个 version 的字段，判断当前的 version 和数据库里的是否一致，一致就可以进行修改，不一致就会不断重试

13. **IOC 的优点是什么？**

    IOC 或 依赖注入把应用的代码量降到最低。它使应用容易测试，单元测试不再需要单例和 JNDI 查找机制。最小的代价和最小的侵入性使松散耦合得以实现。IOC 容器支持加载服务时的饿汉式初始化和懒加载。

14. **@Qualifier** 

    注解当有多个相同类型的 bean 却只有一个需要自动装配时，将 @Qualifier 注解和 @Autowire 注解结合使用以消除这种混淆，指定需要装配的确切的 bean

15. **查看进程命令**

    ps 命令（Process Status）ps aux | grep Redis

16. **Servlet执行过程**

    当服务器端通过 HTTP 协议接收到客户请求后，会将其转化为 HttpServletRequest 对象传递给 Servlet。
    Servlet 通过这些类理解客户的请求，并将其处理后的内容通过 HttpServletResponse 回复到服务器端。
    Web 容器进行整理后用 HTTP 协议向客户端传送响应。

17. **Servlet生命周期**

    Servlet 实例装载有以下三种方式：

    当第一次调用 Servlet 时，就会创建一个 Servelt 实例，这个实例会长期驻留内存中。

    > 在 Web.xml 文件中的 <Servlet></Servlet> 之间添加如下代码：<loadon-startup>1</loadon-startup>，Servelt 容器启动时会自动装载这个 Servlet，数字越小表示优先级别越高。
    > Servlet 类文件被更新后，会重新装载 Servlet

    - init () 方法初始化阶段

      只调用一次 ，第一次创建 Servlet 时被调用。init () 方法简单地创建或加载一些数据，这些数据将被用于 Servlet 的整个生命周期

    - service () 方法处理客户端请求阶段

      处理客户端的请求，并把格式化的响应写回给客户端。每次服务器接收到一个 Servlet 请求时，服务器会产生一个新的线程并调用服务，它会根据来自客户端的请求类型来重载 doGet () 或 doPost () 

    - destroy () 方法终止阶段

      只会被调用一次，在 Servlet 生命周期结束时被调用。destroy () 方法可以让 Servlet 关闭数据库连接、停止后台线程、把 Cookie 列表或点击计数器写入到磁盘等

18. **拦截器和过滤器**

19. **各行业特点**

    传统行业：项目业务

    金融：事务的隔离级别、SSM

    互联网行业：技术新、微服务

20. **日志怎么贯穿你们 service 服务层了**

    使用 Zpkin 来查看你的服务调用链日志

21. **为什么用两个不同的数据库？**

    根据服务的特点来选择不同结构的数据库：

    - 比如有些前置服务使用 node 开发的，主要是展示类型的数据，类型很丰富，对事务要求不高，那我们就考虑使用 nosql 的 MongoDB
    - 如果是专门做搜索类型，考虑使用 ES
    - 像订单服务啊这种对事务要求高的，还是优先考虑用支持事务的关系型数据库

22. **Lock 锁抛出异常了怎么办 **

    Lock 必须要在 finally 代码块中 unlock() 手动释放锁，否则容易造成线程死锁

23. **用 JQuery 写一段伪代码，实现页面端通过 Ajax 调用服务端请求**

    ```javascript
    $.ajax({
        		type: "GET"，   						//请求方式
                url: "http://www.microsoft.com"，    //请求的url地址  
                dataType: "json"，   				//返回格式为json  
                async: true，	//请求是否异步，默认为异步，这也是ajax重要特性  
                data: {"id": "value"}，    			//参数值  
                success: function (req) {
                    //请求成功时处理  
                }，
                error: function () {
                    //请求出错处理  
                }
            });
    ```

24. **数据库视图与表的区别？**

    视图是按照条件查出来的结果集，只能进行 select 操作

    表是真实存在的，增删改查都可以

25. **分布式 ID 的特性**

    - 唯一性：确保生成的 ID 是全网唯一的。
    - 有序递增性：确保生成的 ID 是对于某个用户或者业务是按一定的数字有序递增的。
    - 高可用性：确保任何时候都能正确的生成 ID。
    - 带时间：ID 里面包含时间，一眼扫过去就知道是哪天交易的

26. **如何选择合适的分布式主键方案**

    - UUID

      > 算法的核心思想是结合机器的网卡、当地时间、一个随记数来生成 UUID

    - 数据库自增 ID

      > 使用数据库的 id 自增策略，如 MySQL 的 auto_increment。并且可以使用两台数据库分别设置不同步长，生成不重复 ID 的策略来实现高可用

    - Redis 生成 ID

      > Redis 的所有命令操作都是单线程的，本身提供像 incr 和 increby 这样的自增原子命令，所以能保证生成的 ID 肯定是唯一有序的
      >
      > 另外，比较适合使用 Redis 来生成每天从 0 开始的流水号。比如订单号 = 日期 + 当日自增长号。可以每天在 Redis 中生成一个 Key ，使用 incr 进行累加

    - 推特雪花算法（Twitter 的 snowflake 算法）

      > Twitter 利用 zookeeper 实现了一个全局 ID 生成的服务 Snowflake

27. **两个项目之间如何通信的**

    应用间通信： RPC（Dubbo）vs HTTP（Spring Cloud）

    Dubbo 本身定位就是一个 RPC 框架，它在服务治理集成上非常完善不仅提供了服务注册发现、负载均衡、路由等面向分布式基础的能力，还提供了面向测试的 mock、泛化调用等机制，同时提供了服务治理、监控等可视化平台。所以在 SpringCloud 没出来之前，Dubbo 在国内应用的相当之广泛

    Dubbo 的定位始终是 RPC 框架，而 SpringCloud 的目标是微服务下的一站式解决方案

    Spring Cloud 微服务架构下，微服务之间使用的是 HTTP restful 风格，restful 风格本身轻量、易用，适用性强。可以很容易的跨语言、跨平台或者与已有的系统交互

    Spring Cloud 中服务间有两种 restful 调用方式：1.RestTemplate 2.Feign

    两个 Java 项目，他们之间进行信息的通信
    前提：必须知道要通信的 Java 项目（接收请求方）的服务器的 IP 地址和访问路径。
    其实两个 Java 项目之间的通信还是使用 HTTP 的请求。主要有两种方式：
    :one: 使用 Apache 的 HttpClient 方式
    :two: 使用 JDK 自带的 Java.net 包下的 HttpURLConnection 方式

    HttpURLConnection 方式：HttpURLConnection 传递请求常用的有两种方式：POST 和 GET 方式。使用 setRequestMethod () 方法设置传递的方式

    直接通过远程过程调用来访问别的 service：REST

    > 优点：
    >
    > - 简单，常见
    > - 因为没有中间件代理，系统更简单
    >
    > 缺点：
    >
    > - 只支持请求 / 响应的模式，不支持别的，比如通知、请求 / 异步响应、发布 / 订阅、发布 / 异步响应
    > - 降低了可用性，因为客户端和服务端在请求过程中必须都是可用的

    使用 RabbitMQ 的异步消息来做服务间通信

    > 优点:
    >
    > - 把客户端和服务端解耦，更松耦合
    > - 提高可用性，因为消息中间件缓存了消息，直到消费者可以消费
    > - 支持很多通信机制比如通知、请求 / 异步响应、发布 / 订阅、发布 / 异步响应
    >
    > 缺点:
    >
    > - 消息中间件有额外的复杂性

28. **小伙子，你是我最近面试的程序员中基础知识最扎实的一个**

    没有没有，还是您引导的好，正好问到的是我之前复习到、有用到的，如果您再问深点估计我就挂了

### GitHub

GitHub快捷键：https://help.github.com/en/articles/using-keyboard-shortcuts

GitHub指南：https://sspai.com/post/46061

Git Flow工作流：https://github.com/xirong/my-git/blob/master/git-workflow-tutorial.md

https://www.ruanyifeng.com/blog/2015/12/git-workflow.html

[https://www.funtl.com/zh/git/GitFlow%E5%B7%A5%E4%BD%9C%E6%B5%81.html](https://www.funtl.com/zh/git/GitFlow工作流.html)

https://www.cnblogs.com/iBrand2018/p/8708740.html

BOSS 让做一个支付接口 / 秒杀

上 GitHub 借鉴优秀框架，阅读源码

1. Git：每个开发者可以有属于自己的整个工程的本地拷贝。隔离的环境让各个开发者的工作和项目的其他部分修改独立开来 —— 即自由地提交到自己的本地仓库，先完全忽略上游的开发，直到方便的时候再把修改反馈上去

2. in关键词限制搜索范围：xxx关键词 in:name，description，readme 项目名、描述、readme中包含关键字的

3. SpringBoot stars/forks:>=5000

   springboot forks: 100.. 200 stars:80.. 100

4. awesome Redis

5. 给别人指出关键代码的行号  地址+#L13（-L23）

6. 项目内源代码搜索：t

7. 搜索某地区内的大佬

   - 公式：
     - location：地区
     - language：语言
   - 地区成都的Java方向的用户：location:chengdu language:Java
   
   > 1.按照文件搜索
   > android in:file
   > 2.按照路径检索
   > andrioid in:path
   > 3.按照语言检索
   > android language:Java
   > 4.按照文件大小
   > android size:>100
   > 5.按照后缀名检索
   > android extention:css
   > 6.按照是否被fork过
   > android fork:true
   > 7.按照地域检索
   > android location:beijing

### 简历

[如何写一份有效的技术简历？ - 阮一峰的网络日志](https://www.ruanyifeng.com/blog/2020/01/technical-resume.html)

[技术简历 - 马士兵教育_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV13Y411G7XT/?vd_source=4844de7cb051be29fbaf4555af0bbd8b)

> 提前 3 个月准备

**基础**：熟悉 Java 基础、集合、多线程等；掌握常用设计模式

**JVM**：熟悉 JVM、JMM；熟悉常用垃圾回收器、垃圾回收算法以及 JVM 调优

**并发**：熟悉 Java 并发编程，对各种**锁机制**、线程池机制、AQS 等都有深入理解，并在项目中熟练使用

**框架**：熟悉 SpringMVC、Spring、MyBatis、SpringBoot 等核心框架知识；多年实战经验，可以快速根据需求完成项目构建，并且对**核心源码**也有深入研究

> Spring 核心源码：IOC、AOP、自动装配、启动流程；Bean 的生命周期；Spring 循环依赖
>
> SpringBoot：启动流程、自动装配
>
> MyBatis：如何 XML 和 SQL 映射

**微服务**：熟悉 SpringCloudAlibaba 技术体系，熟练掌握 Nacos、Gateway、OpenFeign、Seata、Sentinel 等常用组件，对服务划分、服务治理、旧服务改造都有深入研究

**数据库**：熟悉 MySQL 关系型数据库以及 SQL 调优，对事务、锁、索引等都有深入理解；熟悉 Redis、MongoDB 等非关系型数据库

> MVCC

**中间件**：熟悉 RabbitMQ、RocketMQ、Kafka 等消息中间件原理并根据合适业务场景进行技术选型

**搜索引擎**：熟悉使用 Elasticsearch 进行数据搜索，使用 ELK 搭建日志采集系统

**分布式**：熟悉分布式事务、分布式 ID、分布式锁、分布式链路追踪等技术方案

**Linux**：掌握运用 Linux 来对程序做故障排查以及性能评估；熟练使用 VIM；了解常用服务器和反向代理工具 Tomcat、Nginx

**开发工具**：熟练应用 IDEA、Git、Maven、Docker、K8s 等项目管理和项目构建工具；掌握 Jmeter 对项目接口进行压测

> 分布式链路追踪：在项目中加入 @Trace（AOP 技术），通过 SkyWalking 进行监控服务并将数据采集到 ES，最终再通过 SkyWalking UI 的可视化界面对最终结果进行查看
>
> 遇到面试官问到不会的技术，直接说这个技术我在项目中和我自己的技术储备中都没有接触过，确实是我的一个短板，您能跟我讲解一下这个技术在咱们项目中的应用场景吗
>
> 之前我在源码里面没有看到过这块，您能给我讲解一下这块是怎么实现的吗
>
> 不好意思面试官，我只是不清楚，您如果不方便说就不说了，我回家了再研究一下



项目

项目描述：

介绍项目是干啥的，写项目模块。项目价值，解决问题是什么，项目中设计到的数据量，项目的规模（如果数据量不好看可以说之前的项目存在什么问题，产生了什么样的后果，经过你的重构优化之后，达到了什么效率，现在是什么状态）

> 万家美慧是一个围绕着 “红色 - 阳光 - 智慧” 为核心物业自治 SaaS 服务平台，主要，解决了传统物业中服务缺乏监管、资金使用不公开不透明等问题。
>
> 美化自己的项目，1w 用户量直接说 30w，SSM 架构直接说分布式。就像相亲媒婆不会说你个穷逼挣得少，会说你扎实、老实，一个人在外面闯荡、独立
>
> 截至目前平台交易金额已接近1000万，日支付峰值40w+，日增 700 用户，月增 2w+ 用户，总用户数 30w+。订单数 60w+，月增 5w+，日增 1.5k
>
> 如何清楚易懂的解释“UV和PV＂的定义？ - Wise的回答 - 知乎 https://www.zhihu.com/question/20448467/answer/70592301

项目架构：

SpringBoot、SpringCloudAlibaba、WebFlux、RabbitMQ、MongoDB、MySQL、Redis、K8s  （引导面试官让你手绘架构图，架构思维）

职责描述：

1、分条描述 2、虚实结合 3、五条以上

```java
1. 万家美慧

万家美慧是一个围绕着 “红色 - 阳光 - 智慧” 为核心物业自治 SaaS 服务平台，主要有家园模块进行物业缴费、查看小区收支；商城模块进行商品及服务购买；行业管理模块来进行监管。解决了传统物业中服务缺乏监管、资金使用不公开不透明等问题。项目已推广了 100+ 小区，用户量达到 30w+

技术特点：

交易中心的下单链路基于分布式事务中间件 Seata，保障支付、库存、积分等核心业务的强一致性。支付网关回调基于高可靠性，高性能中间件 RabbitMQ 的可靠性消息保证后续物流、积分等业务的最终一致性
在核心的订单支付服务中引入 MQ 来进行解耦、削峰及异步；Redis 缓存来做库存预减；利用内存标记来减少 Redis 以及业务模块的访问，保证系统的高可用，优化上线后并发性能提升约 80%
基本分布式数据库 ElasticSearch 高性能的检索能力构建平台级搜索系统以及商品中心
通过秒杀地址隐藏、数学公司验证以及接口限流等方式来做安全优化，保证系统的平稳运行

主要职责：

参与核心模块产品需求评审，根据需求合理性、开发周期、开发资源以及项目排期定稿最终产品需求原型
参与系统高并发秒杀系统的架构设计与开发
负责订单支付中心核心业务的编码，对一些可扩展的代码通过设计模式进行改造
负责海康系统的搭建以及智能硬件的接入、服务管控模块、群众诉求模块以及数据统计模块的功能开发
优化线程使用不合理问题，导致项目会创建大量的线程。使用自定义线程池限制线程创建数，同时保证数据一致性

涉及的技术栈：SpringBoot、SpringCloudAlibaba、WebFlux、RabbitMQ、MongoDB、MySQL、Redis、K8s
```





自我评价
突出你的行业背景和技术经验夸自己，大胆夸自己
做过什么行业的项目，解决了什么问题，提出过什么技术解决方案，参与过多大数据量的项目，有什么样的架
构设计能力，接触过什么样的技术实现

> 5年Java开发经验，其中3年以上大型系统架构设计经验，以及3年以上的团队管理经验，具有分布式、高并发、高可用、大数据量的系统架构设计以及研发经验，目前正负责注册用户1.5亿，日活300万，日访问量1亿+的手机生活服务平台的架构设计与研发。同时拥有扎实的技术功底，对dubbo、spring cloud、spring、mybatis等开源框架均深度阅读过源码。



自我介绍（3-5 分钟，让面试官有时间去看你的简历）

面试官好，我叫连鹏举，来自河北邯郸，目前在北京XX公司担任java高级开发工程师的工作，已经在行业中做了十年了，在这十年中做了，舆情，金融，电信等相关行业，大大小小做了项目，用到很多的技术栈，我们最近的一个项目是XX舆情系统，里面涉及到XXXX技术架构，解决了XX问题，目前的用户已经有XX，当时做项目的时候最难的点在于情感分析，我们的系统架构是这样的，xxxx，将收集到数据要进行情感判断，返回给对应的用户响应，经过我们不断优化，从之前的1分钟，现在优化到了20s，主要的解决方案是XXX，这是最近的项目，面试官，您看您有什么想问的？

### 黑公司

- 法本
- 亿字节

JVM、分布式事务了解吗

1. Two-PC 实现方式：等两个都执行完才提交，假提交到事务管理器。有一个失败就回滚 （两阶段提交模式）

   Seta：实现了 Two-PC，把事务提交到事务管理器。写数据库表的方式（是真的提交了）。5条事务里面如果有一条就会逆向生成 SQL，加了一块钱就会去减一块钱，Seta 帮你做了

   假提交是等事务一起才提交，不适合高并发 - 会卡住

2. 消息队列，保证可靠性，提交了必须得消费掉，人工处理也得处理掉

异步（不拿结果的，下面的业务逻辑不用等那个线程）：存没存上照片都不管了，交给任务线程。

并行：要请求要去查用户的路由器信息，一个请求去查询它的套餐信息，这三个请求都比较耗时，如果都要花 3s，而使用多线程只用 3s

并行和异步回调

### 劳动仲裁

==[如何申请劳动仲裁？ - 做一个好法师的回答 - 知乎](https://www.zhihu.com/question/20066061/answer/1608506540)==

[成都市高新区劳动仲裁过程拖沓，如何加快案件开庭？ - 搬运工的文章 - 知乎](https://zhuanlan.zhihu.com/p/126666849)

[《中华人民共和国劳动合同法》](http://www.gov.cn/flfg/2007-06/29/content_669394.htm)

[Melody1024/labour_law: 劳动仲裁，违法辞退，违法解约，工资损失，竞业限制经济补偿金，996加班费。](https://github.com/Melody1024/labour_law)

看到钱之后再签 “协商解除劳动合同协议”，或者让公司补充一份资金资料

你相当于已经判我死刑了，然后告诉我让我自己选哪个死法。

**底线：这个月工资+假期换算为工资+赔偿1.5个月+断社保原因选择 “非本人意愿中断就业”（我好领失业金）**

**谈判（记得先录音、考勤记录）**

首先合法裁员就那么几类：1. 协商解除劳动合同 - 劳动法第三十六条 2. 过错解除劳动合同 - 劳动法第三十九条 3. 非过错解除劳动合同 - 劳动法第四十条 4. 经济性裁员 - 劳动法第四十一条

所以咱们协商解除劳动合同，公司就按照劳动法第四十条、第四十七条赔偿我 N+1 就可以了。如果公司这边要单方面解除我劳动合同的话，那就属于非法解除劳动合同了，后面的话我就会要求 2N 的赔偿（3 个月）。

我觉得我的要求并不过分，面试的时候跟我说的偶尔加班，结果从我入职开始就一直 996，经常不分日夜的干。有时候要赶个功能出来，像之前曹老板要智能物联拿到泸州去演示，每天在公司搞到晚上凌晨（周末在家边带娃边搞）。

balabala 一堆后，我的底线是 N 也就是 1.5 个月赔偿。如果不接受我的底线。那么我会和之前的同事合并劳动仲裁，到时候我们是不会接受庭外调解的了。直接按照劳动法的 N+1 赔偿 2.5 个月，到那时我还要按劳动法第三十一条提出加班补偿

**注意事项**

见下面 UP 主 ” 老蒋巨靠谱 “ 部分

一定要正常打上下班卡、打印考勤记录

> 视情况来看是否申请仲裁，如果公司即将一点钱都来不出来了，可预见性的公司过几个月有申请破产的话。那就建议不要仲裁，尽量协商能拿到一点是一点，不然就算 3 个月后仲裁成功了结果公司破产了，那就还是拿不到钱

**疑问（不确定是否是这样，需要询问律师）**

“协商解除劳动合同协议”中通常会写明，签署之后员工放弃后续诉讼权利，意味着你签完之后仲裁、去法院都不会赢。

如果你不签字，意味你需要通过仲裁，法院一审二审才能获得你的诉求。即使你仲裁赢了，公司一般都会选择在15天之内进行上诉，后面进行一审，二审的判决之后公司才会执行。

**赔偿金**

协商解除赔偿金：N+1，具体算法是 N* min (过去 12 个月平均工资，市平均工资 3) + 1 月工资，其中的 1 是没有提前通知一个月通知

违法解除赔偿金：2N，N 是入职那天到最后一天之间的年头，不足半年部分按半年算（即 0.5），超过半年不足一年按一年算（即 1）。比如 2019.1.13 入职，last day 是 2022.1.15 ，这期间就是 3 年零 2 天，N=3.5

**加班费**

劳动法 第三十一条

虽然国内普遍加班没有加班费，但是到了仲裁这一步，完全可以要加班费。提前准备8小时之后在工作的证据，注意不能仅是打卡下班的记录，而是需要证明你是在工作的。

日工资 = 月工资 / 21.75

小时工资 = 日工资 / 8

> **第三十一条**　用人单位应当严格执行劳动定额标准，不得强迫或者变相强迫劳动者加班。用人单位安排加班的，应当按照国家有关规定向劳动者支付加班费。

[【老蒋巨靠谱】【网易暴力裁员】面对流氓HR，职场新人该注意什么？_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1HJ411X75H?p=1&vd_source=4844de7cb051be29fbaf4555af0bbd8b)

1. 不要想着和 HR 交朋友（不要怜悯她们）

2. 要有很强的留档意识

   保存自己的工作成果，备份和同事、领导之间的聊天记录，保存历次报销相关的行政信息、考勤记录；全程录音（HR 也可能对你本身进行录音，对你的各种行为进行留档的，因此在一些特殊的问题上要谨言慎行，不要留下任何对自己不利的证据，比如说不要轻易承认自己工作中的失误，任何工作都是不完美的。当公司和你产生冲突的时候，HR 一种非常常见的策略就是提出你工作中的问题，这时候有的职场新人可能没有什么经验的。可能会说虽然我有工作失误，但是我们组的其他人也有我还算好的。或者会说一些比如说我加班这么多，我不可能保证所有事情都做得很完美，有失误很正常）

   > 我建议你不要这么做，因为你的这些话都有可能被录音，也都有可能成为他后续去判断你工作失误的依据。那么面对这样的问题，我的处理方式是只要HR说了这样的话，我就会说我不认可你所说的这些东西，如果你想和我继续讨论我的工作失误，那么请你先准备好关于我工作存在重大失误的相关的证据然后我们之后再谈。
   >
   > 那以我的经验的话，如果你在交锋之中每次都用这样的话给他，那么 HR 如果他只想找茬的话一般都会知难而退

3. 不要轻易在任何法律文书上签字

   因为哪怕这个文件不合情理，除非非常不合理，其他情况一旦在这个文书上签字，就视同于你同意了这个文件上说的所有内容，视同于你你同意他开的各种条件，在大多数情况中基本上你签字就没有回头路了，你想再去维权再去想推翻你这个东西就非常困难了

   > 还有 HR 为了胁迫你签署 “协商解除劳动合同协议”，常见套路是不给离职证明，但是本身这是违法的，如果不给劳动者离职证明，可以去劳动监察大队投诉，劳动监察大队是执法部门，会替你向公司索要离职证明，这个过程大概会在一个月之内。如果离职证明上有不利于劳动者的话，也是违法的，可以仲裁要求重新开具

4. 经典的一种劳资双方的博弈

   博弈的情况是出于各种各样的原因，公司想赶你走。那么从公司的角度，他的利益诉求是什么？他希望和你之间解除合同，但是他不想付出 N+1 的裁员成本

   从劳动者的角度，你的诉求是什么？你虽然希望拿到赔偿，但是不希望留下被辞退的记录。因为只要你在上一家公司是被辞退的，你在找下一份工作的时候，HR 一定会根据这个去大问特问，因为这是一个很敏感的情况。所以在这种比较常见的博弈场景之下，你和公司很有可能找到一种中间的妥协方案，这是符合你们双方的利益的

   > 那就是算作你主动提出离职，但是公司给你少于 N+1 的一定程度的经济补偿。假设你现在面临了这样的一种博弈场景的话，你一定要注意有一点，就是你作为劳动者在口头和 HR 之间达成了协议的时候。为了防止被 HR 套路，HR 有可能会说咱们聊的都没有问题咱们达成了一致。那么这样你先去主动提交离职申请，然后我这边再走后续的程序，但是如果是特别操蛋的 HR，一旦你提出了离职申请，他马上就会（在系统上）同意，他会翻脸不认人，不承认任何之前跟你达成有过你们口头上的任何协定，然后等于他就实现了一个很伟大的成就。不花一分钱把你赶出了公司，所以这种情况一定要小心！
   >
   > 就是你一定要顺序不能搞错，你一定要或者先看到钱，或者和他签一个合同一个补偿性的条款，然后你再提出离职申请

5. 在谈判前详细了解劳动法并且想好自己的底线和筹码

   最基本的是你要懂法，你不懂法就谈不上维护自己的合法权益。你至少应该做的是要去看2018年最新版的《中华人民共和国劳动法》
   那么另外一点是你应该研究一下你当地的劳动局发没发过什么补充性的条文。另外我觉得还可以去做的一件事情，是你去咨询你们当地的劳动仲裁机构或者劳动者保护机构的一些热线。

6. 理性维护自身合法利益

   如果你要想维护你的权益的话，需要考虑我应该在和资方的谈判中用什么样的手段争取到什么。甚至你可能需要换位思考，你站在 HR 的角度，什么样的条款有利于你们双方达成一致

7. 劳动法是倾向于劳动者一方的要有底气

   举个栗子：一个是一些不正规的小公司爱玩一个很常见的猫腻，那就是在入职之后，他会找各种各样的借口不给你劳动合同，比如说常见的说法是盖公章的那个负责人他不在公司。如果劳动者没有合同的时候你和公司发生了任何的纠纷时就会比较麻烦。但是就算没有合同，在法律上有一种说法是你和公司之间是存在劳动事实的。你如果能拿出来你和同事的工作沟通的记录、工作成果、工作素材、还有考勤记录等等。就构成了劳动事实

   再举个栗子：流氓 HR 的做法，HR 企图用劳动法第 3 章第 25 条，然后来威胁你让你自己滚蛋。流氓 HR 最常用的是第 2 和第 3 小条
   比如说你违反了公司的纪律、规定。比如说你迟到早退，或者说在某个项目里你没有按照这个规定流程去做给公司造成的损失等等，他会用这样的说法去恶心你去击垮你的心理防线，然后告诉你你只要不主动走我们就会依法开除你

   我也咨询过当时的 HR 朋友咨询过律师，而且我咨询过当地的劳动局，这样的威胁大家其实不用过分害怕的，因为在法律实践中真正认定劳动者存在严重的违反劳动纪律或公司规定的情况或造成重大的公司损失是一件很困难的事情，条件对于公司来说是很苛刻的。咱就拿违反公司规定来说，首先公司要做到的是什么，这个公司制定的规章制度必须它的内容本身是合理合法的，比如说公司如果出一些流氓的条款，说规定你连续三天迟到一分钟，你就会被开除，这个法律是不支持他的，这是无效的条文

   其次有一步就是公司需要证明规章制度，里面的内容是经过了民主程序的，就是经过了职工代表大会或者全体职工的，讨论得出的规章制度，这一点对公司的限制虽然在实践中没有，那么强，但是比如说公司如果为了整你突然，她在有一天对规章制度单方面进行了明显，不利于你的重大修改，然后没有经过全体员工的讨论，也有可能这是无效的，再接下来公司还需要证明他的这份规章制度制度向全体员工公示过或者告知过

   也就是说公司偷偷摸摸搞一个规定是没有，甚至它贴在公司的墙上也是没有用的，必须这事得让你知道什么叫让你知道或是公司群发了一个邮件并且要求每个人回复确认你收到了。再或者是他给你一个纸质的东西然后你在上面签字，只有这样的话才能在法律上是最有效的

   公司需要提交强有力的证据，证明你确实存在违规行为，而且必须重大的违规行为，刚才说的这些步骤，公司一步一步全都做到了。他在法律上才能够以违反公司规定这个理由不付出任何赔偿把你开除。在实际的司法实践中这是相当难做到的一件事情，如果你问心无愧，没有真的严重违规过，那么是完全不用虚的。

   ==不要害怕用人单位，要不卑不亢。要明白一般的公司 HR 是真的不愿意你把这个事闹到劳动仲裁机构的==

   > 第二十五条劳动者有下列情形之一的，用人单位可以解除劳动合同：（一）在试用期间被证明不符合录用条件的；（二）严重违反劳动纪律或者用人单位规章制度的；（三）严重失职，营私舞弊，对用人单位利益造成重大损害的；
   > （四）被依法追究刑事责任的。

   

[【大东】在试用期遭遇公司非法辞退后第90天，我的劳动仲裁终于成功了～_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1xL4y1B7wJ/?spm_id_from=333.788&vd_source=4844de7cb051be29fbaf4555af0bbd8b)

[【猪摸摸Li】它们当初最忌惮这个劳动仲裁视频！干货分享被违法解雇时的应对之策，里面的预感现在已经被应验！_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV19g411c7zy/?spm_id_from=333.788&vd_source=4844de7cb051be29fbaf4555af0bbd8b)

[最近公司在裁员，赔偿怎么谈？ - 知乎](https://www.zhihu.com/question/384593047/answer/2311804039)

[你经历过怎样的裁员套路？ - 知乎](https://www.zhihu.com/question/323406187/answer/2355145449)

[单位怕劳动仲裁吗？为什么？ - 知乎](https://www.zhihu.com/question/359601077/answer/2261310930)

试用期被解雇

用人单位在录用劳动者时应当向劳动者明确告知录用条件；用人单位在解除劳动合同时应当向劳动者说明理由及法律依据。

（辞退提前一个月告知？好找下份工作）

劳动法说：试用期以不符合录用条件解除劳动关系，需要书面证据证明。即，要证明我不适合我的工作，但显然公司无法澄清。作为程序员，我有各种胜任工作的业绩清单。

[对用人单位违规延长劳动者工作时间的处罚 - 四川政务服务网（成都市人力资源和社会保障局）](http://www.sczwfw.gov.cn/jiq/front/transition/ywTransToDetail?areaCode=510100000000&itemCode=B-000346-001-510100000000-000-11510100MB0Q417031-2-00&taskType=2&deptCode=28490)

```
0. 咨询种问题是问公司所在区，还是住在哪个区，还是都可以
1. 社保每个月缴纳日期是每月1号，还是看公司意愿呢？
2. 领失业保险需要，需要被公司开除的离职证明、并且社保断缴吗（假如不开怎么办）；四川政务网下面的失业保险金申领条件都需要满足吗。（去年9月到今年8月满12个月就算对吧）
（一）已参加失业保险，所在单位和本人已按照规定履行缴费义务满1年的； （二）非本人意愿中断就业的； （三）已按规定办理失业登记，并有求职要求的。 非因本人意愿中断就业是指下列人员： （一）终止劳动合同的；（二）被用人单位解除劳动合同的；（三）被用人单位开除、除名和辞退的；（四）根据《中华人民共和国劳动法》第三十二条第二、三项与用人单位解除劳动合同的；（五）法律、行政法规另有规定的。
3. 失业补助金是在领完失业保险金之后还没找到工作才申领的对吧，两者不能同时申请
4. 我听别人讲要人事她们在停缴社保时选什么辞退选项才算吗？还是说我这边不需要管这个，只要有解除劳动合同的证明就行了呢
```

> 企业为员工停保时选择“非本人意愿中断就业”，是否对企业有影响？
>
> 答：企业为离职员工办理停保手续时，需根据真实的离职原因选择“本人意愿”或者“非本人意愿”，“非本人意愿”停保是离职员工能否领取失业保险金的申领之一。在一个自然年度内，企业年领取失业保险金人数与年平均参保人数之比，为企业裁员率。当企业裁员率超过该年度城镇登记失业率（或者城镇登记调查失业率）的，企业将无法享受对应的企业稳岗补贴政策。